{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/jessie/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from bs4 import BeautifulSoup\n",
                "from tqdm import tqdm\n",
                "import csv\n",
                "import json\n",
                "import stanza\n",
                "import re\n",
                "import requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Current working directory:  /Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\n"
                    ]
                }
            ],
            "source": [
                "# Set working directory to project root\n",
                "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
                "os.chdir(project_root)\n",
                "print(\"Current working directory: \", os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_dir = 'data/h_variant'\n",
                "preprocessed_dir = 'data/preprocessed/cusanus_lemma'\n",
                "result_dir = 'results/preprocessing_result'\n",
                "os.makedirs(preprocessed_dir, exist_ok=True)\n",
                "os.makedirs(result_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "lemmatized_output_path = os.path.join(result_dir, \"cusanus_lemmatized.json\")\n",
                "pos_statistics_path = os.path.join(result_dir, \"cusanus_statistics.csv\")\n",
                "word_freq_statistics_path = os.path.join(result_dir, \"cusanus_word_frequency.csv\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Latin stopwords from URL\n",
                "stopwords_url = 'https://raw.githubusercontent.com/aurelberra/stopwords/master/stopwords_latin.txt'\n",
                "response = requests.get(stopwords_url)\n",
                "response.encoding = 'utf-8'\n",
                "latin_stopwords = set(line.strip() for line in response.text.splitlines() if line.strip())\n",
                "\n",
                "# Add additional stopwords\n",
                "additional_stopwords = {\n",
                "    'ego', 'mei', 'mihi', 'me', 'tu', 'tui', 'tibi', 'te',\n",
                "    'nos', 'noster', 'nobis', 'vos', 'vester',\n",
                "    'sui', 'sibi', 'se',\n",
                "    'ab', 'ex', 'ad', 'in', 'de', 'per', 'cum', 'sub', 'pro',\n",
                "    'ante', 'post', 'supra', 'et', 'ac', 'aut', 'nec', 'sed',\n",
                "    'ut', 'si', 'atque', 'qui', 'quae', 'quod', 'quis', 'quid', 'non', 'ne'\n",
                "}\n",
                "latin_stopwords.update(additional_stopwords)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 142MB/s]                     \n",
                        "2024-11-22 23:02:07 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
                        "2024-11-22 23:02:07 INFO: Downloading default packages for language: la (Latin) ...\n",
                        "2024-11-22 23:02:07 INFO: File exists: /Users/jessie/stanza_resources/la/default.zip\n",
                        "2024-11-22 23:02:09 INFO: Finished downloading models and saved to /Users/jessie/stanza_resources\n",
                        "2024-11-22 23:02:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
                        "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 27.8MB/s]                    \n",
                        "2024-11-22 23:02:09 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
                        "2024-11-22 23:02:10 INFO: Loading these models for language: la (Latin):\n",
                        "=============================\n",
                        "| Processor | Package       |\n",
                        "-----------------------------\n",
                        "| tokenize  | ittb          |\n",
                        "| pos       | ittb_nocharlm |\n",
                        "| lemma     | ittb_nocharlm |\n",
                        "| depparse  | ittb_nocharlm |\n",
                        "=============================\n",
                        "\n",
                        "2024-11-22 23:02:10 INFO: Using device: cpu\n",
                        "2024-11-22 23:02:10 INFO: Loading: tokenize\n",
                        "2024-11-22 23:02:10 INFO: Loading: pos\n",
                        "2024-11-22 23:02:10 INFO: Loading: lemma\n",
                        "2024-11-22 23:02:10 INFO: Loading: depparse\n",
                        "2024-11-22 23:02:10 INFO: Done loading processors!\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Stanza NLP model for Latin\n",
                "stanza.download('la')  # Download Latin model\n",
                "nlp = stanza.Pipeline('la')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/jessie/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Lemma mapping completed. Number of entries: 11784\n"
                    ]
                }
            ],
            "source": [
                "# Load lemma mapping from external file\n",
                "lemma_mapping_path = os.path.join(project_root, 'data/external/lemma.xml')\n",
                "lemma_mapping = {}\n",
                "\n",
                "with open(lemma_mapping_path, 'r', encoding='utf-8') as lemma_file:\n",
                "    lemma_soup = BeautifulSoup(lemma_file, 'lxml')\n",
                "    for lemma_entry in lemma_soup.find_all('lemma'):\n",
                "        lemma_id = lemma_entry.get('id_lemma')\n",
                "        lemma_name = lemma_entry.get('name')\n",
                "\n",
                "        # Improved extraction logic for lemma name\n",
                "        if lemma_name:\n",
                "            # Step 1: Remove any leading characters like \"*\" or other notes\n",
                "            lemma_name = re.sub(r'^\\*.*?\\s', '', lemma_name).strip()\n",
                "\n",
                "            # Step 2: If there are parentheses, prioritize the content before them\n",
                "            if '(' in lemma_name:\n",
                "                lemma_value = lemma_name.split('(')[0].strip().lower()\n",
                "            else:\n",
                "                lemma_value = lemma_name.strip().lower()\n",
                "\n",
                "            # Step 3: Handle multi-word lemmas by selecting the first valid word\n",
                "            if lemma_value:\n",
                "                lemma_value_parts = lemma_value.split()\n",
                "                if lemma_value_parts:\n",
                "                    lemma_value = lemma_value_parts[0]\n",
                "\n",
                "            # Step 4: Remove grammatical category suffixes (e.g., \"cj.\", \"adv.\")\n",
                "            lemma_value = re.split(r'\\b(?:cj\\.|adv\\.|praep\\.|f\\.|m\\.|n\\.|pl\\.|sg\\.|dat\\.|acc\\.|nom\\.|gen\\.|abl\\.)\\b', lemma_value)[0].strip()\n",
                "\n",
                "            # Step 5: Remove unnecessary descriptive content like place names\n",
                "            lemma_value = re.sub(r'\\b(?:provincia|region|place|saec\\.|asia minor|africa|italia|hispania)\\b.*', '', lemma_value).strip()\n",
                "\n",
                "            # Step 6: Ensure the final lemma value is valid and has an ID\n",
                "            if lemma_id and lemma_value:\n",
                "                lemma_mapping[lemma_id] = lemma_value\n",
                "\n",
                "print(f\"Lemma mapping completed. Number of entries: {len(lemma_mapping)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess text function\n",
                "def preprocess_text(words):\n",
                "    return [word for word in words if word.lower() not in latin_stopwords]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pos_tag_text(text):\n",
                "    doc = nlp(text)\n",
                "    return [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def lemmatize_with_cusanus(words):\n",
                "    lemmatized_words = []\n",
                "    for word in words:\n",
                "        lemma_id = word.get('lemma_l', '').lower()  # 从 lemma_l 获取 lemma id\n",
                "        if lemma_id in lemma_mapping:\n",
                "            lemmatized_words.append(lemma_mapping[lemma_id])\n",
                "        else:\n",
                "            lemmatized_words.append(word.get('rend', word.get_text()).lower())  # 默认使用原始词\n",
                "    return lemmatized_words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_file_with_cusanus(file_path, preprocessed_path):\n",
                "    \"\"\"\n",
                "    使用 Cusanus lemma 处理单个文件，提取段落、清理内容、去停用词和词形还原，返回段落列表。\n",
                "    \"\"\"\n",
                "    with open(file_path, 'r', encoding='utf-8') as file:\n",
                "        soup = BeautifulSoup(file, 'lxml')\n",
                "        paragraphs = []\n",
                "\n",
                "        for fw_tag in soup.find_all('fw', {'type': 'n'}):\n",
                "            section_content = []\n",
                "            next_sibling = fw_tag.find_next_sibling()\n",
                "\n",
                "            while next_sibling and next_sibling.name != 'fw':\n",
                "                if next_sibling.name == 'p':\n",
                "                    words = []\n",
                "                    for w in next_sibling.find_all('w'):\n",
                "                        # 跳过包含德文字母或特殊符号的单词\n",
                "                        if re.search(r'[äöüß]', w.get_text()) or re.match(r'^cum\\W*\\d*$', w.get_text()):\n",
                "                            continue\n",
                "\n",
                "                        words.append(w)\n",
                "\n",
                "                    # 使用 Cusanus lemma 进行词形还原\n",
                "                    lemmatized_words = lemmatize_with_cusanus(words)\n",
                "\n",
                "                    # 去除停用词\n",
                "                    filtered_words = preprocess_text(lemmatized_words)\n",
                "                    section_content.append(' '.join(filtered_words))\n",
                "\n",
                "                next_sibling = next_sibling.find_next_sibling()\n",
                "\n",
                "            paragraphs.append({'content': ' '.join(section_content)})\n",
                "\n",
                "        # 保存到 lemmatized.txt\n",
                "        with open(preprocessed_path, 'w', encoding='utf-8') as preprocessed_file:\n",
                "            for idx, paragraph in enumerate(paragraphs, start=1):\n",
                "                preprocessed_file.write(f\"Paragraph {idx}:\\n\")\n",
                "                preprocessed_file.write(f\"{paragraph['content']}\\n\\n\")\n",
                "\n",
                "        return paragraphs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 统计信息计算\n",
                "def calculate_statistics_with_documents(paragraphs, document_id, overall_stats):\n",
                "    doc_stats = {\n",
                "        \"document_id\": document_id,\n",
                "        \"total_paragraphs\": len(paragraphs),\n",
                "        \"total_words\": 0,\n",
                "        \"total_types\": 0,\n",
                "        \"pos_distribution\": {},\n",
                "        \"lemmatized_content\": []\n",
                "    }\n",
                "    unique_words = set()\n",
                "\n",
                "    for paragraph in paragraphs:\n",
                "        words = paragraph['content'].split()\n",
                "        doc_stats[\"total_words\"] += len(words)\n",
                "        unique_words.update(words)\n",
                "\n",
                "        # POS 标注\n",
                "        pos_tags = pos_tag_text(paragraph['content'])\n",
                "        for _, pos in pos_tags:\n",
                "            doc_stats[\"pos_distribution\"][pos] = doc_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
                "            overall_stats[\"pos_distribution\"][pos] = overall_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
                "\n",
                "        # 保存段落内容\n",
                "        doc_stats[\"lemmatized_content\"].append(paragraph['content'])\n",
                "\n",
                "    # 更新文档级统计\n",
                "    doc_stats[\"total_types\"] = len(unique_words)\n",
                "    doc_stats[\"unique_words\"] = unique_words\n",
                "\n",
                "    # 更新总体统计\n",
                "    overall_stats[\"total_words\"] += doc_stats[\"total_words\"]\n",
                "    overall_stats[\"unique_words\"].update(unique_words)\n",
                "\n",
                "    # 更新词频\n",
                "    for word in unique_words:\n",
                "        overall_stats[\"word_frequencies\"][word] = overall_stats[\"word_frequencies\"].get(word, 0) + 1\n",
                "\n",
                "    return doc_stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 保存统计信息\n",
                "def save_overall_statistics(overall_stats, document_stats, result_dir):\n",
                "    statistics_json_path = os.path.join(result_dir, \"cusanus_statistics_summary.json\")\n",
                "    overall_stats[\"total_types\"] = len(overall_stats[\"unique_words\"])\n",
                "    overall_stats.pop(\"unique_words\", None)\n",
                "    data_to_save = {\n",
                "        \"overall_statistics\": overall_stats,\n",
                "        \"document_statistics\": document_stats\n",
                "    }\n",
                "    with open(statistics_json_path, 'w', encoding='utf-8') as json_file:\n",
                "        json.dump(data_to_save, json_file, ensure_ascii=False, indent=4)\n",
                "    print(f\"统计信息已保存到 {statistics_json_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 主流程\n",
                "def main_cusanus_pipeline(input_dir, preprocessed_dir, result_dir):\n",
                "    overall_stats = {\n",
                "        \"total_words\": 0,\n",
                "        \"unique_words\": set(),\n",
                "        \"pos_distribution\": {},\n",
                "        \"word_frequencies\": {}\n",
                "    }\n",
                "    document_stats = []\n",
                "\n",
                "    for file_name in tqdm(os.listdir(input_dir), desc=\"文件处理进度\"):\n",
                "        if file_name.endswith('.xml'):\n",
                "            input_path = os.path.join(input_dir, file_name)\n",
                "            preprocessed_path = os.path.join(preprocessed_dir, file_name.replace('.xml', '_lemmatized.txt'))\n",
                "            try:\n",
                "                paragraphs = process_file_with_cusanus(input_path, preprocessed_path)\n",
                "                doc_stat = calculate_statistics_with_documents(paragraphs, file_name, overall_stats)\n",
                "                document_stats.append(doc_stat)\n",
                "            except Exception as e:\n",
                "                print(f\"处理文件 {file_name} 时出错: {e}\")\n",
                "\n",
                "    save_overall_statistics(overall_stats, document_stats, result_dir)\n",
                "    print(\"所有文件处理完成并生成统计信息！\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "文件处理进度: 100%|██████████| 308/308 [22:03<00:00,  4.30s/it]\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "Object of type set is not JSON serializable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 执行主流程\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain_cusanus_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_dir\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mmain_cusanus_pipeline\u001b[0;34m(input_dir, preprocessed_dir, result_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m处理文件 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 时出错: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43msave_overall_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverall_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m所有文件处理完成并生成统计信息！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36msave_overall_statistics\u001b[0;34m(overall_stats, document_stats, result_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m data_to_save \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m: overall_stats,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m: document_stats\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(statistics_json_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_save\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m统计信息已保存到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatistics_json_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "\u001b[0;31mTypeError\u001b[0m: Object of type set is not JSON serializable"
                    ]
                }
            ],
            "source": [
                "# 执行主流程\n",
                "if __name__ == \"__main__\":\n",
                "    main_cusanus_pipeline(input_dir, preprocessed_dir, result_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 27.9MB/s]                    \n",
                        "2024-11-22 23:58:14 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
                        "2024-11-22 23:58:14 INFO: Downloading default packages for language: la (Latin) ...\n",
                        "2024-11-22 23:58:15 INFO: File exists: /Users/jessie/stanza_resources/la/default.zip\n",
                        "2024-11-22 23:58:16 INFO: Finished downloading models and saved to /Users/jessie/stanza_resources\n",
                        "2024-11-22 23:58:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
                        "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 45.0MB/s]                    \n",
                        "2024-11-22 23:58:16 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
                        "2024-11-22 23:58:16 INFO: Loading these models for language: la (Latin):\n",
                        "=============================\n",
                        "| Processor | Package       |\n",
                        "-----------------------------\n",
                        "| tokenize  | ittb          |\n",
                        "| pos       | ittb_nocharlm |\n",
                        "=============================\n",
                        "\n",
                        "2024-11-22 23:58:16 INFO: Using device: cpu\n",
                        "2024-11-22 23:58:16 INFO: Loading: tokenize\n",
                        "2024-11-22 23:58:16 INFO: Loading: pos\n",
                        "2024-11-22 23:58:17 INFO: Done loading processors!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "开始加载所有 lemmatized 文件...\n",
                        "已加载 308 个文件，共包含 4645 个段落。\n",
                        "示例文件内容: ('h180_197_b_lemmatized.txt', ['epistula dominicus debitor sum plato constantius romanos concludo praemitto paulus sum vita sum osdroena ismael sum vita sum spiritus', 'aristoteles dico ethicus vita voluptuosus vita politicus vita contemplativus omnis philosophus concors paulo voluptas osdroena fugio doceo insector passio voluptas recedo sapientia vita rationalis fio adriel maneo homo propheta dico homo honos sum intellego comparo sum iumentum similis facio sum ille vita politicus bonus sum consisto calabria facio fio volo christus matthaei doceo lex propheta vita loquor complico is dico praeceptum vita contemplativus sum verus vita spiritus noster intellectualis sum omnis contrarius delectabilis annas vita osdroena contrarior vita spiritus voluptas contemplatio osdroena vivo morior spiritus vergo ratio bartholomaeus spiritus vivo mortifico vita osdroena compositio uterque vivo humanus politicus duco vita omnis tempus suus facio primus adriel secundus divinus tertius humanus cura magister noster sum divinus vita duco charybdis sum artus strictus paucus is spiritus deus florentinus sum filius deus contemplor do filius solus video pater jedermann principium suus possideo heres christus heres coheres', 'noto octo tusculano spiritus vivo debeo omnis constantius sum ordino vivo video ars omnis constantius tendo natura membrum principalis vita habito diligens munio pateo vita omnis sum amabilis vita contemplativus sum vita rationalis spiritus homo deus sirenes communis vita possideo possum sum stabilis perpetuus omnis vita sensibilis constantius ille comparo habeo corruptibilis constantius perpetuus sum finis laetitia delectatio omnis vita merito praefero vita osdroena video corruptibilis continuus deficio sum constantius servitium incorruptibilis spiritus sum immediatus filius deus spiritus sum imitor vita deus teneo sum filius voluptas osdroena maximus sum heres regnum theophilus perdo ille successio vivo debeo lex regnum theophilus fugio debeo vita osdroena promitto vita decipio duco constantius cotidie aesculapius seneca particula corpus noster diminuo oleum consumo lucerna exstinguo consumo oleum vivo osdroena bartholomaeus intro diminuo nobilitas forma habeo gesein ternarius qui subtraho unus desino sum ternarius subtraho ratio eo bartholomaeus peccatum charybdis photianus intellego maximus basilidis aesculapius aristoteles septimus ethicus impossibilis sum talis intellego efficio adriel osdroena concupisco guido spiritus pugna continuus sum inoboedientia primus parens sum pugna irenaeus osdroena'])\n",
                        "开始统计词频和 POS 分布...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "统计文件数据: 100%|██████████| 308/308 [05:46<00:00,  1.12s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "总词数: 394248\n",
                        "独特词数: 11503\n",
                        "开始保存统计结果...\n",
                        "统计信息已保存到 JSON 文件: results/preprocessing_result/cusanus_lemma_statistics_summary.json\n",
                        "词频统计已保存到 CSV 文件: results/preprocessing_result/cusanus_lemma_word_frequencies.csv\n",
                        "POS 统计已保存到 CSV 文件: results/preprocessing_result/cusanus_lemma_pos_statistics.csv\n",
                        "所有步骤已完成！\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "import csv\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "import stanza\n",
                "\n",
                "# 设置路径\n",
                "lemmatized_dir = 'data/preprocessed/cusanus_lemma'  # 已存在的 lemmatized 文件目录\n",
                "result_dir = 'results/preprocessing_result'  # 输出结果目录\n",
                "json_path = os.path.join(result_dir, 'cusanus_lemma_statistics_summary.json')  # JSON 输出路径\n",
                "csv_word_freq_path = os.path.join(result_dir, 'cusanus_lemma_word_frequencies.csv')  # 词频 CSV 输出路径\n",
                "csv_pos_stats_path = os.path.join(result_dir, 'cusanus_lemma_pos_statistics.csv')  # POS 统计 CSV 输出路径\n",
                "\n",
                "os.makedirs(result_dir, exist_ok=True)\n",
                "\n",
                "# 初始化 Stanza 模型\n",
                "stanza.download('la')\n",
                "nlp = stanza.Pipeline('la', processors='tokenize,pos')\n",
                "\n",
                "# 步骤 1: 加载所有 lemmatized.txt 文件并去除标点符号\n",
                "def load_and_clean_lemmatized_files(directory):\n",
                "    \"\"\"\n",
                "    加载目录中的所有 lemmatized.txt 文件并清理标点符号\n",
                "    \"\"\"\n",
                "    all_paragraphs = {}\n",
                "    punctuation_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)  # 匹配所有非单词字符和非空白字符\n",
                "    for file_name in os.listdir(directory):\n",
                "        if file_name.endswith('_lemmatized.txt'):\n",
                "            file_path = os.path.join(directory, file_name)\n",
                "            with open(file_path, 'r', encoding='utf-8') as file:\n",
                "                paragraphs = []\n",
                "                for line in file:\n",
                "                    if line.startswith(\"Paragraph\"):\n",
                "                        continue  # 跳过段落标题\n",
                "                    clean_line = punctuation_pattern.sub('', line.strip())  # 去除标点符号\n",
                "                    if clean_line:  # 只保留非空行\n",
                "                        paragraphs.append(clean_line)\n",
                "                all_paragraphs[file_name] = paragraphs\n",
                "    return all_paragraphs\n",
                "\n",
                "# 步骤 2: 统计所有文件的词频和 POS 分布\n",
                "def calculate_statistics_for_all(files_data):\n",
                "    \"\"\"\n",
                "    统计所有文件的词频和 POS 分布\n",
                "    \"\"\"\n",
                "    overall_statistics = {\n",
                "        \"total_words\": 0,\n",
                "        \"unique_words\": set(),\n",
                "        \"word_frequencies\": {},\n",
                "        \"pos_distribution\": {},\n",
                "        \"document_statistics\": []\n",
                "    }\n",
                "\n",
                "    for file_name, paragraphs in tqdm(files_data.items(), desc=\"统计文件数据\"):\n",
                "        file_stats = {\n",
                "            \"document_id\": file_name,\n",
                "            \"total_words\": 0,\n",
                "            \"total_types\": 0,\n",
                "            \"pos_distribution\": {}\n",
                "        }\n",
                "        unique_words = set()\n",
                "\n",
                "        for paragraph in paragraphs:\n",
                "            words = paragraph.split()\n",
                "            file_stats[\"total_words\"] += len(words)\n",
                "            overall_statistics[\"total_words\"] += len(words)\n",
                "            unique_words.update(words)\n",
                "\n",
                "            # 更新总体词频\n",
                "            for word in words:\n",
                "                overall_statistics[\"word_frequencies\"][word] = overall_statistics[\"word_frequencies\"].get(word, 0) + 1\n",
                "\n",
                "            # POS 标注\n",
                "            pos_tags = pos_tag_text(paragraph)\n",
                "            for _, pos in pos_tags:\n",
                "                file_stats[\"pos_distribution\"][pos] = file_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
                "                overall_statistics[\"pos_distribution\"][pos] = overall_statistics[\"pos_distribution\"].get(pos, 0) + 1\n",
                "\n",
                "        file_stats[\"total_types\"] = len(unique_words)\n",
                "        file_stats[\"pos_distribution\"] = dict(file_stats[\"pos_distribution\"])\n",
                "        file_stats[\"unique_words\"] = list(unique_words)\n",
                "\n",
                "        overall_statistics[\"unique_words\"].update(unique_words)\n",
                "        overall_statistics[\"document_statistics\"].append(file_stats)\n",
                "\n",
                "    overall_statistics[\"unique_words\"] = list(overall_statistics[\"unique_words\"])\n",
                "    return overall_statistics\n",
                "\n",
                "# POS 标注函数\n",
                "def pos_tag_text(text):\n",
                "    \"\"\"\n",
                "    对段落进行 POS 标注\n",
                "    \"\"\"\n",
                "    doc = nlp(text)\n",
                "    return [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n",
                "\n",
                "# 步骤 3: 保存统计结果到文件\n",
                "def save_statistics_to_files(statistics, json_path, csv_word_freq_path, csv_pos_stats_path):\n",
                "    # 保存到 JSON 文件\n",
                "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
                "        json.dump(statistics, json_file, ensure_ascii=False, indent=4)\n",
                "    print(f\"统计信息已保存到 JSON 文件: {json_path}\")\n",
                "\n",
                "    # 保存词频到 CSV\n",
                "    with open(csv_word_freq_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
                "        writer = csv.writer(csv_file)\n",
                "        writer.writerow([\"Word\", \"Frequency\"])  # 写入表头\n",
                "        for word, freq in statistics[\"word_frequencies\"].items():\n",
                "            writer.writerow([word, freq])\n",
                "    print(f\"词频统计已保存到 CSV 文件: {csv_word_freq_path}\")\n",
                "\n",
                "    # 保存 POS 分布到 CSV\n",
                "    with open(csv_pos_stats_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
                "        writer = csv.writer(csv_file)\n",
                "        writer.writerow([\"POS Tag\", \"Frequency\"])  # 写入表头\n",
                "        for pos, freq in statistics[\"pos_distribution\"].items():\n",
                "            writer.writerow([pos, freq])\n",
                "    print(f\"POS 统计已保存到 CSV 文件: {csv_pos_stats_path}\")\n",
                "\n",
                "# 主流程\n",
                "if __name__ == \"__main__\":\n",
                "    # 步骤 1: 加载文件\n",
                "    print(\"开始加载所有 lemmatized 文件...\")\n",
                "    files_data = load_and_clean_lemmatized_files(lemmatized_dir)\n",
                "    print(f\"已加载 {len(files_data)} 个文件，共包含 {sum(len(v) for v in files_data.values())} 个段落。\")\n",
                "    print(f\"示例文件内容: {list(files_data.items())[0] if files_data else '无文件'}\")\n",
                "\n",
                "    # 询问是否继续\n",
                "    if input(\"是否继续统计词频和 POS 数据？(y/n): \").strip().lower() != 'y':\n",
                "        print(\"已退出程序。\")\n",
                "        exit()\n",
                "\n",
                "    # 步骤 2: 统计数据\n",
                "    print(\"开始统计词频和 POS 分布...\")\n",
                "    overall_statistics = calculate_statistics_for_all(files_data)\n",
                "    print(f\"总词数: {overall_statistics['total_words']}\")\n",
                "    print(f\"独特词数: {len(overall_statistics['unique_words'])}\")\n",
                "\n",
                "    # 询问是否保存\n",
                "    if input(\"是否保存统计结果到文件？(y/n): \").strip().lower() != 'y':\n",
                "        print(\"已退出程序。\")\n",
                "        exit()\n",
                "\n",
                "    # 步骤 3: 保存结果\n",
                "    print(\"开始保存统计结果...\")\n",
                "    save_statistics_to_files(overall_statistics, json_path, csv_word_freq_path, csv_pos_stats_path)\n",
                "    print(\"所有步骤已完成！\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 48.0MB/s]                    \n",
                        "2024-11-23 00:23:55 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
                        "2024-11-23 00:23:55 INFO: Downloading default packages for language: la (Latin) ...\n",
                        "2024-11-23 00:23:55 INFO: File exists: /Users/jessie/stanza_resources/la/default.zip\n",
                        "2024-11-23 00:23:56 INFO: Finished downloading models and saved to /Users/jessie/stanza_resources\n",
                        "2024-11-23 00:23:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
                        "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 37.7MB/s]                    \n",
                        "2024-11-23 00:23:57 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
                        "2024-11-23 00:23:57 INFO: Loading these models for language: la (Latin):\n",
                        "=============================\n",
                        "| Processor | Package       |\n",
                        "-----------------------------\n",
                        "| tokenize  | ittb          |\n",
                        "| pos       | ittb_nocharlm |\n",
                        "=============================\n",
                        "\n",
                        "2024-11-23 00:23:57 INFO: Using device: cpu\n",
                        "2024-11-23 00:23:57 INFO: Loading: tokenize\n",
                        "2024-11-23 00:23:57 INFO: Loading: pos\n",
                        "2024-11-23 00:23:57 INFO: Done loading processors!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "开始加载 lemmatized 文件...\n",
                        "加载完成，文件数: 308\n",
                        "示例段落:  ['epistula, dominicus, debitor, sum, plato constantius romanos concludo, praemitto, paulus sum, vita, sum, osdroena ismael sum, vita, sum, spiritus,', 'aristoteles dico, ethicus, vita, voluptuosus, vita, politicus, vita, contemplativus, omnis, philosophus, concors, paulo voluptas, osdroena fugio, doceo, insector, passio, voluptas, recedo, sapientia, vita, rationalis, fio, adriel maneo, homo, propheta, dico, homo, honos, sum, intellego, comparo, sum, iumentum, similis, facio, sum, ille, vita, politicus, bonus, sum, consisto, calabria facio, fio, volo, christus, matthaei doceo, lex, propheta, vita, loquor, complico, is, dico, praeceptum, vita, contemplativus, sum, verus, vita, spiritus, noster, intellectualis, sum, omnis, contrarius, delectabilis, annas vita, osdroena contrarior, vita, spiritus, voluptas, contemplatio, osdroena vivo, morior, spiritus, vergo, ratio, bartholomaeus spiritus, vivo, mortifico, vita, osdroena compositio, uterque, vivo, humanus, politicus, duco, vita, omnis, tempus, suus, facio, primus, adriel secundus, divinus, tertius, humanus, cura, magister, noster, sum, divinus, vita, duco, charybdis sum, artus, strictus, paucus, is, spiritus, deus, florentinus, sum, filius, deus, contemplor, do, filius, solus, video, pater, jedermann principium, suus, possideo, heres, christus, heres, coheres,', 'noto, octo tusculano spiritus, vivo, debeo, omnis, constantius sum, ordino, vivo, video, ars, omnis, constantius tendo, natura, membrum, principalis, vita, habito, diligens, munio, pateo, vita, omnis, sum, amabilis, vita, contemplativus, sum, vita, rationalis, spiritus, homo, deus, sirenes communis, vita, possideo, possum, sum, stabilis, perpetuus, omnis, vita, sensibilis, constantius ille, comparo, habeo, corruptibilis, constantius perpetuus, sum, finis, laetitia, delectatio, omnis, vita, merito praefero, vita, osdroena video, corruptibilis, continuus, deficio, sum, constantius servitium, incorruptibilis, spiritus, sum, immediatus, filius, deus, spiritus, sum, imitor, vita, deus, teneo, sum, filius, voluptas, osdroena maximus, sum, heres, regnum, theophilus perdo, ille, successio, vivo, debeo, lex, regnum, theophilus fugio, debeo, vita, osdroena promitto, vita, decipio, duco, constantius cotidie aesculapius seneca particula, corpus, noster, diminuo, oleum, consumo, lucerna, exstinguo, consumo, oleum, vivo, osdroena bartholomaeus intro, diminuo, nobilitas, forma, habeo, gesein ternarius, qui, subtraho, unus, desino, sum, ternarius, subtraho, ratio, eo, bartholomaeus peccatum, charybdis photianus, intellego, maximus, basilidis aesculapius aristoteles septimus, ethicus, impossibilis, sum, talis, intellego, efficio, adriel osdroena concupisco, guido spiritus, pugna, continuus, sum, inoboedientia, primus, parens, sum, pugna, irenaeus osdroena']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "预处理文件: 100%|██████████| 308/308 [00:00<00:00, 1956.94it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "预处理完成，文件已保存到 data/preprocessed/cusanus\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "统计文件数据: 100%|██████████| 308/308 [02:39<00:00,  1.93it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "总词数: 360434\n",
                        "独特词数: 11501\n",
                        "统计信息已保存到 JSON 文件: results/cusanus_statistics_summary.json\n",
                        "词频统计已保存到 CSV 文件: results/cusanus_word_frequencies.csv\n",
                        "POS 统计已保存到 CSV 文件: results/cusanus_pos_statistics.csv\n",
                        "所有步骤已完成！\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import re\n",
                "import json\n",
                "import csv\n",
                "from tqdm import tqdm\n",
                "import stanza\n",
                "\n",
                "# 设置路径\n",
                "lemmatized_dir = 'data/preprocessed/cusanus_lemma'\n",
                "processed_dir = 'data/preprocessed/cusanus'\n",
                "result_dir = 'results'\n",
                "os.makedirs(processed_dir, exist_ok=True)\n",
                "os.makedirs(result_dir, exist_ok=True)\n",
                "\n",
                "json_path = os.path.join(result_dir, 'cusanus_statistics_summary.json')\n",
                "csv_word_freq_path = os.path.join(result_dir, 'cusanus_word_frequencies.csv')\n",
                "csv_pos_stats_path = os.path.join(result_dir, 'cusanus_pos_statistics.csv')\n",
                "\n",
                "# 初始化 Stanza 模型\n",
                "stanza.download('la')\n",
                "nlp = stanza.Pipeline('la', processors='tokenize,pos', tokenize_pretokenized=True)\n",
                "\n",
                "# 定义要移除的标点符号和停用词\n",
                "punctuation_pattern = r'[^\\w\\s]'\n",
                "stopwords_to_remove = {'sum', 'qui'}\n",
                "\n",
                "# 加载所有 lemmatized 文件\n",
                "def load_lemmatized_files(directory):\n",
                "    all_files = {}\n",
                "    for file_name in os.listdir(directory):\n",
                "        if file_name.endswith('_lemmatized.txt'):\n",
                "            file_path = os.path.join(directory, file_name)\n",
                "            with open(file_path, 'r', encoding='utf-8') as file:\n",
                "                paragraphs = [line.strip() for line in file if line.strip() and not line.startswith(\"Paragraph\")]\n",
                "                all_files[file_name] = paragraphs\n",
                "    return all_files\n",
                "\n",
                "# 预处理单个段落\n",
                "def preprocess_paragraph(paragraph):\n",
                "    # 移除标点符号\n",
                "    paragraph = re.sub(punctuation_pattern, '', paragraph)\n",
                "    # 按空格分词并移除停用词\n",
                "    words = [word for word in paragraph.split() if word.lower() not in stopwords_to_remove]\n",
                "    return ' '.join(words)\n",
                "\n",
                "# 预处理所有文件并保存到新目录\n",
                "def preprocess_files(files, output_dir):\n",
                "    for file_name, paragraphs in tqdm(files.items(), desc=\"预处理文件\"):\n",
                "        output_path = os.path.join(output_dir, file_name)\n",
                "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
                "            for idx, paragraph in enumerate(paragraphs, start=1):\n",
                "                processed_paragraph = preprocess_paragraph(paragraph)\n",
                "                output_file.write(f\"Paragraph {idx}:\\n{processed_paragraph}\\n\\n\")\n",
                "\n",
                "# POS 标注\n",
                "def pos_tag_text(text):\n",
                "    doc = nlp(text)\n",
                "    return [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n",
                "\n",
                "# 统计文件的词频和 POS 分布\n",
                "def calculate_statistics(files):\n",
                "    overall_statistics = {\n",
                "        \"total_words\": 0,\n",
                "        \"unique_words\": set(),\n",
                "        \"word_frequencies\": {},\n",
                "        \"pos_distribution\": {},\n",
                "        \"document_statistics\": []\n",
                "    }\n",
                "\n",
                "    for file_name, paragraphs in tqdm(files.items(), desc=\"统计文件数据\"):\n",
                "        file_stats = {\n",
                "            \"document_id\": file_name,\n",
                "            \"total_words\": 0,\n",
                "            \"total_types\": 0,\n",
                "            \"pos_distribution\": {}\n",
                "        }\n",
                "        unique_words = set()\n",
                "\n",
                "        for paragraph in paragraphs:\n",
                "            words = paragraph.split()\n",
                "            file_stats[\"total_words\"] += len(words)\n",
                "            overall_statistics[\"total_words\"] += len(words)\n",
                "            unique_words.update(words)\n",
                "\n",
                "            # 更新词频统计\n",
                "            for word in words:\n",
                "                overall_statistics[\"word_frequencies\"][word] = overall_statistics[\"word_frequencies\"].get(word, 0) + 1\n",
                "\n",
                "            # POS 标注\n",
                "            pos_tags = pos_tag_text(paragraph)\n",
                "            for _, pos in pos_tags:\n",
                "                file_stats[\"pos_distribution\"][pos] = file_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
                "                overall_statistics[\"pos_distribution\"][pos] = overall_statistics[\"pos_distribution\"].get(pos, 0) + 1\n",
                "\n",
                "        file_stats[\"total_types\"] = len(unique_words)\n",
                "        file_stats[\"unique_words\"] = list(unique_words)\n",
                "\n",
                "        overall_statistics[\"unique_words\"].update(unique_words)\n",
                "        overall_statistics[\"document_statistics\"].append(file_stats)\n",
                "\n",
                "    overall_statistics[\"unique_words\"] = list(overall_statistics[\"unique_words\"])\n",
                "    return overall_statistics\n",
                "\n",
                "# 保存统计结果到文件\n",
                "def save_statistics_to_files(statistics, json_path, csv_word_freq_path, csv_pos_stats_path):\n",
                "    # 保存到 JSON 文件\n",
                "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
                "        json.dump(statistics, json_file, ensure_ascii=False, indent=4)\n",
                "    print(f\"统计信息已保存到 JSON 文件: {json_path}\")\n",
                "\n",
                "    # 保存词频到 CSV\n",
                "    with open(csv_word_freq_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
                "        writer = csv.writer(csv_file)\n",
                "        writer.writerow([\"Word\", \"Frequency\"])\n",
                "        for word, freq in statistics[\"word_frequencies\"].items():\n",
                "            writer.writerow([word, freq])\n",
                "    print(f\"词频统计已保存到 CSV 文件: {csv_word_freq_path}\")\n",
                "\n",
                "    # 保存 POS 分布到 CSV\n",
                "    with open(csv_pos_stats_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
                "        writer = csv.writer(csv_file)\n",
                "        writer.writerow([\"POS Tag\", \"Frequency\"])\n",
                "        for pos, freq in statistics[\"pos_distribution\"].items():\n",
                "            writer.writerow([pos, freq])\n",
                "    print(f\"POS 统计已保存到 CSV 文件: {csv_pos_stats_path}\")\n",
                "\n",
                "# 主流程\n",
                "if __name__ == \"__main__\":\n",
                "    print(\"开始加载 lemmatized 文件...\")\n",
                "    lemmatized_files = load_lemmatized_files(lemmatized_dir)\n",
                "\n",
                "    print(f\"加载完成，文件数: {len(lemmatized_files)}\")\n",
                "    print(\"示例段落: \", lemmatized_files[list(lemmatized_files.keys())[0]][:3])\n",
                "\n",
                "    if input(\"是否继续进行预处理？(y/n): \").strip().lower() == 'y':\n",
                "        preprocess_files(lemmatized_files, processed_dir)\n",
                "        print(f\"预处理完成，文件已保存到 {processed_dir}\")\n",
                "\n",
                "    if input(\"是否继续统计词频和 POS 数据？(y/n): \").strip().lower() == 'y':\n",
                "        processed_files = load_lemmatized_files(processed_dir)  # 加载处理后的文件\n",
                "        statistics = calculate_statistics(processed_files)\n",
                "        print(f\"总词数: {statistics['total_words']}\")\n",
                "        print(f\"独特词数: {len(statistics['unique_words'])}\")\n",
                "        save_statistics_to_files(statistics, json_path, csv_word_freq_path, csv_pos_stats_path)\n",
                "        print(\"所有步骤已完成！\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
