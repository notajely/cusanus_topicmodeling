{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import tqdm\n",
    "import json\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 BASE_DIR 已经定义为项目的根目录\n",
    "BASE_DIR = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'  # 替换为实际的路径\n",
    "\n",
    "# 加载停用词表\n",
    "with open(os.path.join(BASE_DIR, 'data/external/stopwords.pkl'), 'rb') as f:\n",
    "    latin_stopwords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成唯一实验ID\n",
    "experiment_id = f\"bertopic_experiment_{int(time.time())}\"\n",
    "experiment_dir = os.path.join(BASE_DIR, 'experiments', experiment_id)\n",
    "\n",
    "# 创建实验目录\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "\n",
    "# 获取当前 experiment_id 的日志文件路径\n",
    "log_file_path = os.path.join(experiment_dir, f\"bertopic_experiment_{experiment_id}.log\")\n",
    "\n",
    "# 配置日志记录，使每个实验的日志记录到独立的文件中\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()  # 清除现有的处理器，避免重复添加\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# 添加文件日志处理器\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# 添加控制台日志处理器\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# 确保在配置 logging 后马上打印一条信息，便于确认新的日志文件被创建\n",
    "logger.info(f\"启动实验 {experiment_id}，日志记录到 {log_file_path}\")\n",
    "print(f\"启动实验 {experiment_id}，日志记录到 {log_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置\n",
    "# 每次修改这里\n",
    "\n",
    "experiment_config = {\n",
    "    \"parameters\": {\n",
    "        \"n_gram_range\": (1, 3),  \n",
    "        \"min_topic_size\": 3,  # 将 min_topic_size 设置得更小，以便生成更多的细分主题\n",
    "        \"nr_topics\": \"auto\",  \n",
    "        \"embedding_model_name\": \"xlm-r-distilroberta-base-paraphrase-v1\",  # 嵌入模型名称\n",
    "        \"umap_params\": {\n",
    "            \"n_neighbors\": 10,\n",
    "            \"min_dist\": 0.05,\n",
    "            \"n_components\": 5,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(experiment_dir, 'config.json')\n",
    "with open(config_path, 'w') as config_file:\n",
    "    json.dump(experiment_config, config_file, indent=4)\n",
    "\n",
    "logger.info(f\"实验配置已保存到 {config_path}\")\n",
    "print(f\"实验配置已保存到 {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的 SentenceTransformer 模型\n",
    "model_path = os.path.join(BASE_DIR, 'saved_models', experiment_config[\"parameters\"][\"embedding_model_name\"])\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # 如果本地已经保存了模型，则直接加载\n",
    "    print(f\"从本地路径 {model_path} 加载嵌入模型...\")\n",
    "    logger.info(f\"从本地路径 {model_path} 加载嵌入模型...\")\n",
    "    embedding_model = SentenceTransformer(model_path)\n",
    "else:\n",
    "    # 如果本地没有模型，则从远程加载并保存\n",
    "    print(f\"加载预训练的嵌入模型 {experiment_config['parameters']['embedding_model_name']}...\")\n",
    "    logger.info(f\"加载预训练的嵌入模型 {experiment_config['parameters']['embedding_model_name']}...\")\n",
    "    embedding_model = SentenceTransformer(experiment_config['parameters']['embedding_model_name'])\n",
    "    embedding_model.save(model_path)  # 保存到本地路径\n",
    "    logger.info(f\"嵌入模型加载成功并已保存到本地 {model_path}。\")\n",
    "    print(f\"嵌入模型加载成功并已保存到本地 {model_path}。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试集数据\n",
    "testset_dir = os.path.join(BASE_DIR, 'data/testset')\n",
    "documents = []\n",
    "\n",
    "# 加载测试集文件\n",
    "print(\"加载测试集数据...\")\n",
    "logging.info(\"加载测试集数据...\")\n",
    "\n",
    "try:\n",
    "    test_files = [f for f in os.listdir(testset_dir) if f.endswith('.txt')]\n",
    "    for test_file in tqdm.tqdm(test_files, desc=\"Loading testset files\"):\n",
    "        file_path = os.path.join(testset_dir, test_file)\n",
    "        with open(file_path, 'r') as file:\n",
    "            documents.append(file.read())\n",
    "\n",
    "    logging.info(f\"加载了 {len(documents)} 个文档用于 BERTopic 实验。\")\n",
    "    print(f\"加载了 {len(documents)} 个文档用于 BERTopic 实验。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"测试集目录未找到: {testset_dir}\")\n",
    "    logging.error(f\"测试集目录未找到: {testset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vectorizer_and_model(experiment_config, stopwords, embedding_model):\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=experiment_config[\"parameters\"][\"n_gram_range\"],\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\",  # 适用于拉丁语的正则表达式\n",
    "        stop_words=stopwords  # 使用自定义的停用词表\n",
    "    )\n",
    "    \n",
    "    # 自定义的 UMAP 设置以进行降维\n",
    "    umap_params = experiment_config[\"parameters\"][\"umap_params\"]\n",
    "    custom_umap = UMAP(\n",
    "        n_neighbors=umap_params[\"n_neighbors\"],\n",
    "        min_dist=umap_params[\"min_dist\"],\n",
    "        n_components=umap_params[\"n_components\"],\n",
    "        random_state=umap_params[\"random_state\"]\n",
    "    )\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        language=\"multilingual\",  # 拉丁语适合使用多语言模型\n",
    "        min_topic_size=experiment_config[\"parameters\"][\"min_topic_size\"],\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        embedding_model=embedding_model,  # 使用提前加载的嵌入模型\n",
    "        umap_model=custom_umap  # 使用自定义的 UMAP 设置\n",
    "    )\n",
    "    return topic_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "topic_model = initialize_vectorizer_and_model(experiment_config, latin_stopwords, embedding_model)\n",
    "\n",
    "logging.info(\"BERTopic 模型初始化成功。\")\n",
    "print(\"BERTopic 模型初始化成功。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "if documents:\n",
    "    print(\"开始训练 BERTopic 模型...\")\n",
    "    logger.info(\"开始训练 BERTopic 模型...\")\n",
    "\n",
    "    # 训练模型并显示训练进度\n",
    "    try:\n",
    "        topics, probabilities = topic_model.fit_transform(tqdm.tqdm(documents, desc=\"Training BERTopic model\"))\n",
    "        logger.info(\"BERTopic 模型训练成功。\")\n",
    "        print(\"BERTopic 模型训练成功。\")\n",
    "        \n",
    "        # 尽量避免主题合并，保持较多的主题数量\n",
    "        if experiment_config[\"parameters\"].get(\"nr_topics\") != \"auto\":\n",
    "            target_topics = experiment_config[\"parameters\"][\"nr_topics\"]\n",
    "            if len(topic_model.get_topic_info()) > target_topics:\n",
    "                topic_model = topic_model.reduce_topics(documents, nr_topics=target_topics)\n",
    "                logger.info(f\"BERTopic 模型主题数量已减少到 {target_topics}。\")\n",
    "                print(f\"BERTopic 模型主题数量已减少到 {target_topics}。\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"训练 BERTopic 模型时发生错误: {e}\")\n",
    "        print(f\"训练 BERTopic 模型时发生错误: {e}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = os.path.join(experiment_dir, 'model')\n",
    "    topic_model.save(model_path)\n",
    "    logger.info(f\"BERTopic 模型已保存到 {model_path}\")\n",
    "    print(f\"BERTopic 模型已保存到 {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"生成的主题：\")\n",
    "logger.info(\"生成的主题：\")\n",
    "topics_overview = topic_model.get_topic_info()\n",
    "\n",
    "for index, row in topics_overview.iterrows():\n",
    "    topic_num = row['Topic']\n",
    "    if topic_num != -1:  # 排除噪声主题\n",
    "        topic_keywords = topic_model.get_topic(topic_num)[:10]  # 获取前十个关键词\n",
    "        keywords_str = ', '.join([word for word, _ in topic_keywords])\n",
    "        print(f\"主题 {topic_num}: {row['Name']} - 关键词: {keywords_str}\")\n",
    "        logger.info(f\"主题 {topic_num}: {row['Name']} - 关键词: {keywords_str}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
