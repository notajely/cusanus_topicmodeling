{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import stanza\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
    "os.chdir(project_root)\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/v_variant'\n",
    "preprocessed_dir = 'data/preprocessed/stanza'\n",
    "result_dir = 'results/preprocessing_result'\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词加载\n",
    "stopwords_url = 'https://raw.githubusercontent.com/aurelberra/stopwords/master/stopwords_latin.txt'\n",
    "response = requests.get(stopwords_url)\n",
    "response.encoding = 'utf-8'\n",
    "latin_stopwords = set(line.strip() for line in response.text.splitlines() if line.strip())\n",
    "\n",
    "additional_stopwords = {\n",
    "    'ego', 'mei', 'mihi', 'me', 'tu', 'tui', 'tibi', 'te',\n",
    "    'nos', 'noster', 'nobis', 'vos', 'vester',\n",
    "    'sui', 'sibi', 'se',\n",
    "    'ab', 'ex', 'ad', 'in', 'de', 'per', 'cum', 'sub', 'pro',\n",
    "    'ante', 'post', 'supra', 'et', 'ac', 'aut', 'nec', 'sed',\n",
    "    'ut', 'si', 'atque', 'qui', 'quae', 'quod', 'quis', 'quid', 'non', 'ne'\n",
    "}\n",
    "latin_stopwords.update(additional_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Stanza NLP 模型\n",
    "stanza.download('la')\n",
    "nlp = stanza.Pipeline('la', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理函数\n",
    "def preprocess_text(words):\n",
    "    return [word for word in words if word.lower() not in latin_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS 标注函数\n",
    "def pos_tag_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanza 词形还原函数\n",
    "def lemmatize_with_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    return [word.lemma for sentence in doc.sentences for word in sentence.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_with_stanza(file_path, preprocessed_path):\n",
    "    \"\"\"\n",
    "    使用 Stanza 处理单个文件：提取段落、清理内容、去停用词和词形还原，返回段落列表。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "        paragraphs = []\n",
    "\n",
    "        for fw_tag in soup.find_all('fw', {'type': 'n'}):\n",
    "            section_content = []\n",
    "            next_sibling = fw_tag.find_next_sibling()\n",
    "            while next_sibling and next_sibling.name != 'fw':\n",
    "                if next_sibling.name == 'p':\n",
    "                    words = []\n",
    "                    for w in next_sibling.find_all('w'):\n",
    "                        original_word = w.get('rend', w.get_text()).lower()\n",
    "\n",
    "                        # 跳过德语单词和特殊符号\n",
    "                        if re.search(r'[äöüß]', original_word) or re.match(r'^cum\\W*\\d*$', original_word):\n",
    "                            continue\n",
    "\n",
    "                        words.append(original_word)\n",
    "\n",
    "                    # 词形还原\n",
    "                    lemmatized_words = lemmatize_with_stanza(' '.join(words))\n",
    "\n",
    "                    # 去停用词\n",
    "                    filtered_words = preprocess_text(lemmatized_words)\n",
    "                    section_content.append(' '.join(filtered_words))\n",
    "\n",
    "                next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "            paragraphs.append({'content': ' '.join(section_content)})\n",
    "\n",
    "        # 保存到 lemmatized.txt\n",
    "        with open(preprocessed_path, 'w', encoding='utf-8') as preprocessed_file:\n",
    "            for idx, paragraph in enumerate(paragraphs, start=1):\n",
    "                preprocessed_file.write(f\"Paragraph {idx}:\\n\")\n",
    "                preprocessed_file.write(f\"{paragraph['content']}\\n\\n\")\n",
    "\n",
    "        return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计信息计算（包含词频和文档级统计）\n",
    "def calculate_statistics_with_documents(paragraphs, document_id, overall_stats):\n",
    "    \"\"\"\n",
    "    计算文档级统计信息，保留段落的词形还原内容。\n",
    "    \"\"\"\n",
    "    doc_stats = {\n",
    "        \"document_id\": document_id,\n",
    "        \"total_paragraphs\": len(paragraphs),\n",
    "        \"total_words\": 0,\n",
    "        \"total_types\": 0,\n",
    "        \"pos_distribution\": {},\n",
    "        \"lemmatized_content\": []  # 新增：存储词形还原内容\n",
    "    }\n",
    "    unique_words = set()\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph['content'].split()\n",
    "        doc_stats[\"total_words\"] += len(words)\n",
    "        unique_words.update(words)\n",
    "\n",
    "        # POS 标注并统计分布\n",
    "        pos_tags = pos_tag_text(paragraph['content'])\n",
    "        for _, pos in pos_tags:\n",
    "            doc_stats[\"pos_distribution\"][pos] = doc_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
    "            overall_stats[\"pos_distribution\"][pos] = overall_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
    "\n",
    "        # 将段落内容加入 lemmatized_content\n",
    "        doc_stats[\"lemmatized_content\"].append(paragraph['content'])\n",
    "\n",
    "    # 更新文档级统计\n",
    "    doc_stats[\"total_types\"] = len(unique_words)\n",
    "    doc_stats[\"unique_words\"] = unique_words\n",
    "\n",
    "    # 更新总体统计\n",
    "    overall_stats[\"total_words\"] += doc_stats[\"total_words\"]\n",
    "    overall_stats[\"unique_words\"].update(unique_words)\n",
    "\n",
    "    # 更新词频\n",
    "    for word in unique_words:\n",
    "        overall_stats[\"word_frequencies\"][word] = overall_stats[\"word_frequencies\"].get(word, 0) + 1\n",
    "\n",
    "    return doc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存文档统计和词频统计\n",
    "def save_document_statistics_with_totals(document_stats, result_dir):\n",
    "    csv_path = os.path.join(result_dir, \"stanza_statistics.csv\")\n",
    "    total_words = sum(doc[\"total_words\"] for doc in document_stats)\n",
    "    total_types = len(set(word for doc in document_stats for word in doc.get(\"unique_words\", [])))\n",
    "\n",
    "    with open(csv_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Document ID\", \"Total Paragraphs\", \"Total Words\", \"Total Types\", \"POS Distribution\"])\n",
    "        \n",
    "        # 写入文档级统计\n",
    "        for doc_stat in document_stats:\n",
    "            writer.writerow([\n",
    "                doc_stat[\"document_id\"],\n",
    "                doc_stat[\"total_paragraphs\"],\n",
    "                doc_stat[\"total_words\"],\n",
    "                doc_stat[\"total_types\"],\n",
    "                json.dumps(doc_stat[\"pos_distribution\"])\n",
    "            ])\n",
    "\n",
    "        # 添加总统计信息\n",
    "        writer.writerow([\n",
    "            \"ALL DOCUMENTS\",\n",
    "            sum(doc[\"total_paragraphs\"] for doc in document_stats),\n",
    "            total_words,\n",
    "            total_types,\n",
    "            \"N/A\"\n",
    "        ])\n",
    "    print(f\"文档级统计信息已保存到 {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_frequency_with_pos(overall_stats, result_dir):\n",
    "    \"\"\"\n",
    "    保存词频统计到 CSV 文件，包含 Word, Frequency, POS 列。\n",
    "    \"\"\"\n",
    "    word_freq_csv_path = os.path.join(result_dir, \"stanza_word_frequency.csv\")\n",
    "    pos_tags = overall_stats.get(\"pos_distribution\", {})\n",
    "    word_frequencies = overall_stats.get(\"word_frequencies\", {})\n",
    "    pos_for_words = {}\n",
    "    for pos, freq in pos_tags.items():\n",
    "        for word, frequency in word_frequencies.items():\n",
    "            if word not in pos_for_words:\n",
    "                pos_for_words[word] = pos\n",
    "    with open(word_freq_csv_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Word\", \"Frequency\", \"POS\"])  # 表头\n",
    "        for word, freq in word_frequencies.items():\n",
    "            pos = pos_for_words.get(word, \"UNKNOWN\")  # 默认 POS 为 UNKNOWN\n",
    "            writer.writerow([word, freq, pos])\n",
    "    print(f\"词频统计信息已保存到 {word_freq_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存统计信息 (专门针对 Stanza)\n",
    "def save_overall_statistics_for_stanza(overall_stats, document_stats, result_dir):\n",
    "    \"\"\"\n",
    "    保存总体统计信息为 JSON 文件，确保 JSON 序列化稳定，避免类型问题。\n",
    "    \"\"\"\n",
    "    statistics_json_path = os.path.join(result_dir, \"stanza_statistics_summary.json\")\n",
    "\n",
    "    # 转换 set 为 list，确保 JSON 序列化兼容\n",
    "    overall_stats[\"total_types\"] = len(overall_stats[\"unique_words\"])\n",
    "    overall_stats[\"unique_words\"] = list(overall_stats[\"unique_words\"])  # 转换为列表\n",
    "\n",
    "    data_to_save = {\n",
    "        \"overall_statistics\": overall_stats,\n",
    "        \"document_statistics\": document_stats\n",
    "    }\n",
    "\n",
    "    with open(statistics_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_to_save, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Stanza 统计信息已保存到 {statistics_json_path}\")\n",
    "\n",
    "\n",
    "# 主流程仅针对 Stanza\n",
    "def main_stanza_pipeline(input_dir, preprocessed_dir, result_dir):\n",
    "    \"\"\"\n",
    "    针对 Stanza 的主处理流程。\n",
    "    \"\"\"\n",
    "    overall_stats = {\n",
    "        \"total_words\": 0,\n",
    "        \"unique_words\": set(),\n",
    "        \"pos_distribution\": {},\n",
    "        \"word_frequencies\": {}\n",
    "    }\n",
    "    document_stats = []\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir), desc=\"文件处理进度\"):\n",
    "        if file_name.endswith('.xml'):\n",
    "            input_path = os.path.join(input_dir, file_name)\n",
    "            preprocessed_path = os.path.join(preprocessed_dir, file_name.replace('.xml', '_lemmatized.txt'))\n",
    "\n",
    "            # 跳过已处理文件\n",
    "            if os.path.exists(preprocessed_path):\n",
    "                print(f\"{preprocessed_path} 已存在，跳过预处理...\")\n",
    "            else:\n",
    "                try:\n",
    "                    paragraphs = process_file_with_stanza(input_path, preprocessed_path)\n",
    "                    if paragraphs:\n",
    "                        doc_stat = calculate_statistics_with_documents(paragraphs, file_name, overall_stats)\n",
    "                        document_stats.append(doc_stat)\n",
    "                except Exception as e:\n",
    "                    print(f\"处理文件 {file_name} 时出错: {e}\")\n",
    "\n",
    "    # 检查统计文件是否已存在\n",
    "    statistics_json_path = os.path.join(result_dir, \"stanza_statistics_summary.json\")\n",
    "    if os.path.exists(statistics_json_path):\n",
    "        print(f\"{statistics_json_path} 已存在，跳过统计...\")\n",
    "    else:\n",
    "        save_overall_statistics_for_stanza(overall_stats, document_stats, result_dir)\n",
    "    print(\"Stanza 所有文件处理完成并生成统计信息！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行主流程\n",
    "if __name__ == \"__main__\":\n",
    "    main_stanza_pipeline(input_dir, preprocessed_dir, result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 19.8MB/s]                    \n",
      "2024-11-23 00:33:00 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
      "2024-11-23 00:33:01 INFO: Downloading default packages for language: la (Latin) ...\n",
      "2024-11-23 00:33:02 INFO: File exists: /Users/jessie/stanza_resources/la/default.zip\n",
      "2024-11-23 00:33:03 INFO: Finished downloading models and saved to /Users/jessie/stanza_resources\n",
      "2024-11-23 00:33:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 26.3MB/s]                    \n",
      "2024-11-23 00:33:03 INFO: Downloaded file to /Users/jessie/stanza_resources/resources.json\n",
      "2024-11-23 00:33:03 INFO: Loading these models for language: la (Latin):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | ittb          |\n",
      "| pos       | ittb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2024-11-23 00:33:03 INFO: Using device: cpu\n",
      "2024-11-23 00:33:03 INFO: Loading: tokenize\n",
      "2024-11-23 00:33:03 INFO: Loading: pos\n",
      "2024-11-23 00:33:03 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载已处理的文件...\n",
      "加载完成，文件数: 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清理停用词: 100%|██████████| 308/308 [00:00<00:00, 2875.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用词清理完成，文件已保存到 data/preprocessed/cusanus_cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "统计文件数据: 100%|██████████| 308/308 [02:17<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总词数: 307521\n",
      "独特词数: 11433\n",
      "统计信息已保存到: results/cusanus_cleaned_statistics_summary.json\n",
      "词频已保存到: results/cusanus_cleaned_word_frequencies.csv\n",
      "POS 统计已保存到: results/cusanus_cleaned_pos_statistics.csv\n",
      "所有统计任务已完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import re\n",
    "\n",
    "# 路径设置\n",
    "processed_dir = 'data/preprocessed/cusanus'  # 已经处理的文件目录\n",
    "updated_dir = 'data/preprocessed/cusanus_cleaned'  # 停用词再次清理后的目录\n",
    "result_dir = 'results'  # 统计结果目录\n",
    "\n",
    "os.makedirs(updated_dir, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "json_path = os.path.join(result_dir, 'cusanus_cleaned_statistics_summary.json')\n",
    "csv_word_freq_path = os.path.join(result_dir, 'cusanus_cleaned_word_frequencies.csv')\n",
    "csv_pos_stats_path = os.path.join(result_dir, 'cusanus_cleaned_pos_statistics.csv')\n",
    "\n",
    "# 加载 Stanza 模型\n",
    "stanza.download('la')\n",
    "nlp = stanza.Pipeline('la', processors='tokenize,pos', tokenize_pretokenized=True)\n",
    "\n",
    "# 加载停用词列表\n",
    "stopwords_url = 'https://raw.githubusercontent.com/aurelberra/stopwords/master/stopwords_latin.txt'\n",
    "response = requests.get(stopwords_url)\n",
    "response.encoding = 'utf-8'\n",
    "latin_stopwords = set(line.strip() for line in response.text.splitlines() if line.strip())\n",
    "\n",
    "# 如果需要增加额外停用词\n",
    "additional_stopwords = {'sum', 'qui'}  # 已经处理但未移除的词\n",
    "latin_stopwords.update(additional_stopwords)\n",
    "\n",
    "# 加载已处理的文件\n",
    "def load_processed_files(directory):\n",
    "    \"\"\"\n",
    "    加载已处理的文件（每个文件段落为列表）。\n",
    "    \"\"\"\n",
    "    all_files = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('_lemmatized.txt'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                paragraphs = [line.strip() for line in file if line.strip() and not line.startswith(\"Paragraph\")]\n",
    "                all_files[file_name] = paragraphs\n",
    "    return all_files\n",
    "\n",
    "# 再次清理停用词\n",
    "def clean_stopwords(paragraph):\n",
    "    \"\"\"\n",
    "    从段落中移除停用词。\n",
    "    \"\"\"\n",
    "    words = [word for word in paragraph.split() if word.lower() not in latin_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_files(files_data, output_dir):\n",
    "    \"\"\"\n",
    "    遍历文件并清理停用词，将结果保存到新目录。\n",
    "    \"\"\"\n",
    "    for file_name, paragraphs in tqdm(files_data.items(), desc=\"清理停用词\"):\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            for idx, paragraph in enumerate(paragraphs, start=1):\n",
    "                cleaned_paragraph = clean_stopwords(paragraph)\n",
    "                output_file.write(f\"Paragraph {idx}:\\n{cleaned_paragraph}\\n\\n\")\n",
    "\n",
    "# POS 标注\n",
    "def pos_tag_text(text):\n",
    "    \"\"\"\n",
    "    对文本进行 POS 标注。\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "# 统计文件的词频和 POS 分布\n",
    "def calculate_statistics(files_data):\n",
    "    \"\"\"\n",
    "    统计文件的词频和 POS 分布。\n",
    "    \"\"\"\n",
    "    overall_statistics = {\n",
    "        \"total_words\": 0,\n",
    "        \"unique_words\": set(),\n",
    "        \"word_frequencies\": {},\n",
    "        \"pos_distribution\": {},\n",
    "        \"document_statistics\": []\n",
    "    }\n",
    "\n",
    "    for file_name, paragraphs in tqdm(files_data.items(), desc=\"统计文件数据\"):\n",
    "        file_stats = {\n",
    "            \"document_id\": file_name,\n",
    "            \"total_words\": 0,\n",
    "            \"total_types\": 0,\n",
    "            \"pos_distribution\": {}\n",
    "        }\n",
    "        unique_words = set()\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            words = paragraph.split()\n",
    "            file_stats[\"total_words\"] += len(words)\n",
    "            overall_statistics[\"total_words\"] += len(words)\n",
    "            unique_words.update(words)\n",
    "\n",
    "            # 更新词频\n",
    "            for word in words:\n",
    "                overall_statistics[\"word_frequencies\"][word] = overall_statistics[\"word_frequencies\"].get(word, 0) + 1\n",
    "\n",
    "            # POS 标注\n",
    "            pos_tags = pos_tag_text(paragraph)\n",
    "            for _, pos in pos_tags:\n",
    "                file_stats[\"pos_distribution\"][pos] = file_stats[\"pos_distribution\"].get(pos, 0) + 1\n",
    "                overall_statistics[\"pos_distribution\"][pos] = overall_statistics[\"pos_distribution\"].get(pos, 0) + 1\n",
    "\n",
    "        file_stats[\"total_types\"] = len(unique_words)\n",
    "        file_stats[\"unique_words\"] = list(unique_words)\n",
    "\n",
    "        overall_statistics[\"unique_words\"].update(unique_words)\n",
    "        overall_statistics[\"document_statistics\"].append(file_stats)\n",
    "\n",
    "    overall_statistics[\"unique_words\"] = list(overall_statistics[\"unique_words\"])\n",
    "    return overall_statistics\n",
    "\n",
    "# 保存统计结果\n",
    "def save_statistics(statistics, json_path, csv_word_freq_path, csv_pos_stats_path):\n",
    "    \"\"\"\n",
    "    保存统计结果到 JSON 和 CSV 文件。\n",
    "    \"\"\"\n",
    "    # 保存到 JSON\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(statistics, json_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"统计信息已保存到: {json_path}\")\n",
    "\n",
    "    # 保存词频到 CSV\n",
    "    with open(csv_word_freq_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Word\", \"Frequency\"])\n",
    "        for word, freq in statistics[\"word_frequencies\"].items():\n",
    "            writer.writerow([word, freq])\n",
    "    print(f\"词频已保存到: {csv_word_freq_path}\")\n",
    "\n",
    "    # 保存 POS 分布到 CSV\n",
    "    with open(csv_pos_stats_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"POS Tag\", \"Frequency\"])\n",
    "        for pos, freq in statistics[\"pos_distribution\"].items():\n",
    "            writer.writerow([pos, freq])\n",
    "    print(f\"POS 统计已保存到: {csv_pos_stats_path}\")\n",
    "\n",
    "# 主流程\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"加载已处理的文件...\")\n",
    "    processed_files = load_processed_files(processed_dir)\n",
    "    print(f\"加载完成，文件数: {len(processed_files)}\")\n",
    "\n",
    "    if input(\"是否继续清理停用词？(y/n): \").strip().lower() == 'y':\n",
    "        clean_files(processed_files, updated_dir)\n",
    "        print(f\"停用词清理完成，文件已保存到 {updated_dir}\")\n",
    "\n",
    "    if input(\"是否继续统计词频和 POS 分布？(y/n): \").strip().lower() == 'y':\n",
    "        cleaned_files = load_processed_files(updated_dir)\n",
    "        statistics = calculate_statistics(cleaned_files)\n",
    "        print(f\"总词数: {statistics['total_words']}\")\n",
    "        print(f\"独特词数: {len(statistics['unique_words'])}\")\n",
    "        save_statistics(statistics, json_path, csv_word_freq_path, csv_pos_stats_path)\n",
    "        print(\"所有统计任务已完成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cusanus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
