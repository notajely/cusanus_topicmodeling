{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from scipy.stats import entropy\n",
    "from itertools import combinations\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# 设置工作目录和路径\n",
    "BASE_DIR = \"/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\"\n",
    "os.chdir(BASE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging() -> str:\n",
    "    \"\"\"设置日志\"\"\"\n",
    "    log_dir = Path('experiments/lda/logs')\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = log_dir / f'experiment_{timestamp}.log'\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return log_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory: Path) -> List[List[str]]:\n",
    "    \"\"\"加载文档，只在文档级别处理\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for file_path in directory.glob('*.txt'):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "            \n",
    "            # 将整个文档内容合并为一个词列表\n",
    "            words = []\n",
    "            # 将所有内容合并，不按段落分割\n",
    "            text = ' '.join([p.split(':', 1)[-1].strip() \n",
    "                           for p in content.split('Paragraph') \n",
    "                           if p.strip()])\n",
    "            \n",
    "            words = text.split()\n",
    "            if words:  # 只添加非空文档\n",
    "                documents.append(words)\n",
    "                logging.info(f\"成功加载文档: {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载文档 {file_path} 时发生错误: {str(e)}\")\n",
    "    \n",
    "    logging.info(f\"总共加载了 {len(documents)} 个文档\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_cooccurrences(texts: List[List[str]], window_size: int = 10) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"计算词对在文档中的共现次数\"\"\"\n",
    "    cooccurrences = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        for i in range(len(text)):\n",
    "            # 在窗口大小内查找共现词\n",
    "            window = text[max(0, i-window_size):min(len(text), i+window_size+1)]\n",
    "            for j, word1 in enumerate(window):\n",
    "                for word2 in window[j+1:]:\n",
    "                    if word1 < word2:  # 确保词对顺序一致\n",
    "                        pair = (word1, word2)\n",
    "                    else:\n",
    "                        pair = (word2, word1)\n",
    "                    cooccurrences[pair] = cooccurrences.get(pair, 0) + 1\n",
    "                    \n",
    "    return cooccurrences\n",
    "\n",
    "def calculate_weighted_npmi(model: LdaModel, texts: List[List[str]], \n",
    "                          dictionary: Dictionary, top_n: int = 10, \n",
    "                          window_size: int = 10, eps: float = 1e-12) -> float:\n",
    "    \"\"\"计算加权NPMI分数\"\"\"\n",
    "    try:\n",
    "        # 获取词共现统计\n",
    "        cooccurrences = calculate_word_cooccurrences(texts, window_size)\n",
    "        \n",
    "        # 计算单词频率\n",
    "        word_freq = {}\n",
    "        total_windows = 0\n",
    "        for text in texts:\n",
    "            total_windows += max(1, len(text) - window_size + 1)\n",
    "            for word in text:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        weighted_npmi_sum = 0\n",
    "        weight_sum = 0\n",
    "        \n",
    "        # 对每个主题计算加权NPMI\n",
    "        for topic_id in range(model.num_topics):\n",
    "            topic_words = [word for word, _ in model.show_topic(topic_id, topn=top_n)]\n",
    "            \n",
    "            # 计算词对的NPMI\n",
    "            for i, word1 in enumerate(topic_words):\n",
    "                for word2 in topic_words[i+1:]:\n",
    "                    if word1 < word2:\n",
    "                        pair = (word1, word2)\n",
    "                    else:\n",
    "                        pair = (word2, word1)\n",
    "                    \n",
    "                    count = cooccurrences.get(pair, 0)\n",
    "                    if count > 0:\n",
    "                        # 计算联合概率和边缘概率\n",
    "                        p_xy = (count + eps) / total_windows\n",
    "                        p_x = (word_freq.get(word1, 0) + eps) / total_windows\n",
    "                        p_y = (word_freq.get(word2, 0) + eps) / total_windows\n",
    "                        \n",
    "                        # 计算PMI和NPMI\n",
    "                        pmi = np.log(p_xy / (p_x * p_y))\n",
    "                        npmi = pmi / (-np.log(p_xy))\n",
    "                        \n",
    "                        # 加权累加\n",
    "                        weighted_npmi_sum += npmi * count\n",
    "                        weight_sum += count\n",
    "        \n",
    "        # 标准化到[0,1]区间\n",
    "        if weight_sum > 0:\n",
    "            weighted_npmi = (weighted_npmi_sum / weight_sum + 1) / 2\n",
    "        else:\n",
    "            weighted_npmi = 0\n",
    "            \n",
    "        return max(0.0, min(1.0, weighted_npmi))\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"计算加权NPMI时发生错误: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_improved_diversity(model: LdaModel, beta: float = 0.5, \n",
    "                               top_n: int = 10, eps: float = 1e-12) -> float:\n",
    "    \"\"\"计算改进的主题多样性分数\"\"\"\n",
    "    try:\n",
    "        K = model.num_topics\n",
    "        total_score = 0\n",
    "        num_pairs = 0\n",
    "        \n",
    "        # 获取所有主题的词分布\n",
    "        topic_distributions = []\n",
    "        topic_words = []\n",
    "        for topic_id in range(K):\n",
    "            topic_dist = dict(model.show_topic(topic_id, topn=top_n))\n",
    "            topic_distributions.append([v for _, v in sorted(topic_dist.items())])\n",
    "            topic_words.append(set(topic_dist.keys()))\n",
    "        \n",
    "        # 计算所有主题对的多样性\n",
    "        for i, j in combinations(range(K), 2):\n",
    "            # 计算JSD\n",
    "            M = [(p1 + p2) / 2 for p1, p2 in zip(topic_distributions[i], topic_distributions[j])]\n",
    "            jsd = (entropy(topic_distributions[i], M) + entropy(topic_distributions[j], M)) / 2\n",
    "            \n",
    "            # 计算词重叠度\n",
    "            intersection = len(topic_words[i] & topic_words[j])\n",
    "            min_size = min(len(topic_words[i]), len(topic_words[j]))\n",
    "            overlap = 1 - (intersection / min_size if min_size > 0 else 0)\n",
    "            \n",
    "            # 组合得分\n",
    "            pair_score = beta * jsd + (1 - beta) * overlap\n",
    "            total_score += pair_score\n",
    "            num_pairs += 1\n",
    "        \n",
    "        # 计算平均多样性分数\n",
    "        diversity = total_score / num_pairs if num_pairs > 0 else 0\n",
    "        return max(0.0, min(1.0, diversity))\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"计算改进的多样性分数时发生错误: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_optimal_score(model: LdaModel, texts: List[List[str]], \n",
    "                          dictionary: Dictionary, alpha: float = 0.5, \n",
    "                          beta: float = 0.5, top_n: int = 10) -> float:\n",
    "    \"\"\"计算最优分数\"\"\"\n",
    "    try:\n",
    "        # 计算加权NPMI\n",
    "        weighted_npmi = calculate_weighted_npmi(model, texts, dictionary, top_n=top_n)\n",
    "        \n",
    "        # 计算改进的多样性\n",
    "        improved_diversity = calculate_improved_diversity(model, beta=beta, top_n=top_n)\n",
    "        \n",
    "        # 计算最终得分\n",
    "        optimal_score = alpha * weighted_npmi + (1 - alpha) * improved_diversity\n",
    "        return optimal_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"计算最优分数时发生错误: {str(e)}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_parameter(params: tuple, train_docs: List[List[str]], n_passes: int = 100) -> dict:\n",
    "    \"\"\"评估单个参数组合\"\"\"\n",
    "    min_freq, max_freq, num_topics, alpha, eta = params\n",
    "    \n",
    "    try:\n",
    "        # 创建词典\n",
    "        dictionary = Dictionary(train_docs)\n",
    "        dictionary.filter_extremes(no_below=min_freq, no_above=max_freq/len(train_docs))\n",
    "        \n",
    "        # 创建语料库\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in train_docs]\n",
    "        \n",
    "        # 训练LDA模型\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "            iterations=n_passes,  # 使用传入的n_passes参数\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # 计算评分（保持原有的评估方式）\n",
    "        score = calculate_optimal_score(\n",
    "            model=model,\n",
    "            texts=train_docs,\n",
    "            dictionary=dictionary,\n",
    "            alpha=0.5  # 用于NPMI和多样性的权重\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'min_freq': min_freq,\n",
    "            'max_freq': max_freq,\n",
    "            'num_topics': num_topics,\n",
    "            'alpha': alpha,\n",
    "            'eta': eta,\n",
    "            'optimal_score': score,\n",
    "            'model': model,\n",
    "            'dictionary': dictionary\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"参数组合评估失败: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_parameters_parallel(\n",
    "    grid_params: dict, \n",
    "    train_docs: List[List[str]], \n",
    "    experiment_dir: Path, \n",
    "    use_multiprocessing: bool = True,\n",
    "    n_passes: int = 100,  # 添加n_passes参数\n",
    "    **kwargs  # 添加kwargs来处理其他可能的参数\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"并行评估所有参数组合\"\"\"\n",
    "    # 获取参数范围\n",
    "    min_freqs = grid_params.get('min_freqs', [2, 3, 4])\n",
    "    max_freqs = grid_params.get('max_freqs', [200, 800, 1400, 2000])\n",
    "    num_topics = grid_params.get('num_topics', [10])\n",
    "    alpha_range = grid_params.get('alpha_range', ['symmetric', 0.1, 0.3, 0.5, 0.7])\n",
    "    eta_range = grid_params.get('eta_range', ['symmetric', 0.1, 0.3, 0.5, 0.7])\n",
    "    \n",
    "    # 生成参数组合\n",
    "    params_combinations = [\n",
    "        (min_freq, max_freq, num_topic, alpha, eta)\n",
    "        for min_freq in min_freqs\n",
    "        for max_freq in max_freqs\n",
    "        for num_topic in num_topics\n",
    "        for alpha in alpha_range\n",
    "        for eta in eta_range\n",
    "    ]\n",
    "    \n",
    "    eval_func = partial(\n",
    "        evaluate_single_parameter, \n",
    "        train_docs=train_docs,\n",
    "        n_passes=n_passes  # 传递n_passes参数\n",
    "    )\n",
    "    \n",
    "    if use_multiprocessing:\n",
    "        with Pool() as pool:\n",
    "            results = list(tqdm(\n",
    "                pool.imap(eval_func, params_combinations), \n",
    "                total=len(params_combinations)\n",
    "            ))\n",
    "    else:\n",
    "        results = list(tqdm(\n",
    "            map(eval_func, params_combinations), \n",
    "            total=len(params_combinations)\n",
    "        ))\n",
    "    \n",
    "    # 过滤掉失败的结果并转换为DataFrame\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    results_df = pd.DataFrame(valid_results)\n",
    "    \n",
    "    # 按optimal_score排序并保存\n",
    "    results_df = results_df.sort_values('optimal_score', ascending=False)\n",
    "    results_df.to_csv(experiment_dir / 'parameter_evaluation_results.csv', index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def save_evaluation_results(results_df: pd.DataFrame, model: LdaModel, \n",
    "                          prep_type: str, experiment_dir: Path) -> Dict:\n",
    "    \"\"\"保存评估结果\"\"\"\n",
    "    results_dir = experiment_dir / prep_type / 'results'\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 保存基础评估结果\n",
    "        results_df.to_csv(results_dir / 'evaluation_results.csv', index=False)\n",
    "        \n",
    "        # 保存主题词分布\n",
    "        topic_words = {}\n",
    "        for topic_id in range(model.num_topics):\n",
    "            words = [word for word, prob in model.show_topic(topic_id, topn=20)]\n",
    "            topic_words[f'Topic_{topic_id}'] = words\n",
    "            \n",
    "        with open(results_dir / 'topic_words.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(topic_words, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        # 保存模型\n",
    "        model.save(str(results_dir / 'best_model.lda'))\n",
    "        \n",
    "        # 保存详细结果\n",
    "        detailed_results = {\n",
    "            'model_params': {\n",
    "                'num_topics': model.num_topics,\n",
    "                'alpha': model.alpha,\n",
    "                'eta': model.eta,\n",
    "                'iterations': model.iterations\n",
    "            },\n",
    "            'evaluation_metrics': {\n",
    "                'optimal_score': float(results_df['optimal_score'].max()),\n",
    "                'avg_score': float(results_df['optimal_score'].mean()),\n",
    "                'std_score': float(results_df['optimal_score'].std())\n",
    "            },\n",
    "            'topic_coherence': {\n",
    "                'npmi_score': float(results_df['npmi_score'].max()),\n",
    "                'diversity_score': float(results_df['diversity_score'].max())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(results_dir / 'detailed_results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(detailed_results, f, indent=2)\n",
    "            \n",
    "        return detailed_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"保存评估结果时发生错误: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_table(results_dict: Dict, n_topics: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"创建实验结果汇总表格\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for prep_type, results_path in results_dict.items():\n",
    "        try:\n",
    "            # 加载final_results.json\n",
    "            with open(results_path, 'r', encoding='utf-8') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # 处理top 5结果\n",
    "            for idx, params in enumerate(results['top_5_params'], 1):\n",
    "                # 获取主题词（每个主题前10个词）\n",
    "                model_path = os.path.join(os.path.dirname(results_path), f'model_{idx}.lda')\n",
    "                if os.path.exists(model_path):\n",
    "                    model = LdaModel.load(model_path)\n",
    "                    # 为每个主题创建关键词列表\n",
    "                    topics_keywords = {}\n",
    "                    for topic_id in range(n_topics):\n",
    "                        words = [word for word, _ in model.show_topic(topic_id, topn=10)]\n",
    "                        topics_keywords[f'Topic_{topic_id+1}'] = ', '.join(words)\n",
    "                    \n",
    "                    # 创建基本信息\n",
    "                    entry = {\n",
    "                        'Exp. ID': f\"{prep_type[:3]}-{idx}\",\n",
    "                        'Lemmatization Method': prep_type,\n",
    "                        'Threshold': f\"{params['min_freq']}-{params['max_freq']}\",\n",
    "                        'alpha': params['alpha'],\n",
    "                        'eta': 'auto',\n",
    "                        'n_topics': n_topics,\n",
    "                        'n_passes': params['n_passes'],\n",
    "                        'optimal score': params['test_score']\n",
    "                    }\n",
    "                    \n",
    "                    # 添加每个主题的关键词\n",
    "                    entry.update(topics_keywords)\n",
    "                    summary_data.append(entry)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"处理 {prep_type} 结果时发生错误: {str(e)}\")\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # 保存为CSV\n",
    "    os.makedirs('experiments/lda/summaries', exist_ok=True)\n",
    "    summary_df.to_csv('experiments/lda/summaries/experiment_summary.csv', index=False)\n",
    "    \n",
    "    # Excel格式（带格式）\n",
    "    with pd.ExcelWriter('experiments/lda/summaries/experiment_summary.xlsx', engine='xlsxwriter') as writer:\n",
    "        summary_df.to_excel(writer, index=False, sheet_name='Summary')\n",
    "        \n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Summary']\n",
    "        \n",
    "        # 设置列宽\n",
    "        base_columns = {\n",
    "            'A': 10,  # Exp. ID\n",
    "            'B': 20,  # Lemmatization Method\n",
    "            'C': 15,  # Threshold\n",
    "            'D': 10,  # alpha\n",
    "            'E': 10,  # eta\n",
    "            'F': 10,  # n_topics\n",
    "            'G': 10,  # n_passes\n",
    "            'H': 15,  # optimal score\n",
    "        }\n",
    "        \n",
    "        # 为每个主题的关键词添加列宽\n",
    "        topic_columns = {chr(ord('I') + i): 100 for i in range(n_topics)}\n",
    "        column_widths = {**base_columns, **topic_columns}\n",
    "        \n",
    "        for col, width in column_widths.items():\n",
    "            worksheet.set_column(f'{col}:{col}', width)\n",
    "        \n",
    "        # 添加表格格式\n",
    "        header_format = workbook.add_format({\n",
    "            'bold': True,\n",
    "            'bg_color': '#D9E1F2',\n",
    "            'border': 1,\n",
    "            'text_wrap': True,\n",
    "            'align': 'center',\n",
    "            'valign': 'vcenter'\n",
    "        })\n",
    "        \n",
    "        # 应用表头格式\n",
    "        for col_num, value in enumerate(summary_df.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "        # 添加数据单元格格式\n",
    "        data_format = workbook.add_format({\n",
    "            'text_wrap': True,\n",
    "            'valign': 'top',\n",
    "            'border': 1\n",
    "        })\n",
    "        \n",
    "        # 应用数据格式\n",
    "        for row in range(1, len(summary_df) + 1):\n",
    "            for col in range(len(summary_df.columns)):\n",
    "                worksheet.write(row, col, summary_df.iloc[row-1, col], data_format)\n",
    "    \n",
    "    # Markdown格式\n",
    "    with open('experiments/lda/summaries/experiment_summary.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"运行完整实验流程\"\"\"\n",
    "    # 设置日志\n",
    "    log_file = setup_logging()\n",
    "    logging.info(\"开始LDA实验流程\")\n",
    "    \n",
    "    # 实验参数\n",
    "    grid_params = {\n",
    "        'min_freqs': [2, 3, 4, 5],\n",
    "        'max_freqs': [200, 500, 800, 1000, 1500],\n",
    "        'num_topics': [10],\n",
    "        'alpha_range': [0.3, 0.5, 0.7],\n",
    "        'eta_range': ['symmetric']\n",
    "    }\n",
    "    \n",
    "    # 其他训练参数\n",
    "    training_params = {\n",
    "        'n_passes': 100,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        experiment_dir = Path(BASE_DIR) / 'experiments/lda'\n",
    "        results_paths = {}\n",
    "        \n",
    "        # 处理两种预处理方法\n",
    "        for prep_type in ['spacy', 'cusanus']:\n",
    "            logging.info(f\"\\n{'='*50}\")\n",
    "            logging.info(f\"开始处理 {prep_type} 预处理数据\")\n",
    "            logging.info(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                # 加载训练集和测试集\n",
    "                train_dir = experiment_dir / prep_type / 'train_set'\n",
    "                test_dir = experiment_dir / prep_type / 'test_set'\n",
    "                \n",
    "                train_docs = load_documents(train_dir)\n",
    "                test_docs = load_documents(test_dir)\n",
    "                \n",
    "                logging.info(f\"加载了 {len(train_docs)} 个训练文档和 {len(test_docs)} 个测试文档\")\n",
    "                \n",
    "                # 创建当前预处理方法的实验目录\n",
    "                prep_experiment_dir = experiment_dir / prep_type\n",
    "                prep_experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # 进行参数评估\n",
    "                logging.info(\"\\n开始参数网格搜索...\")\n",
    "                try:\n",
    "                    results_df = evaluate_parameters_parallel(\n",
    "                        grid_params=grid_params,\n",
    "                        train_docs=train_docs,\n",
    "                        experiment_dir=prep_experiment_dir,\n",
    "                        use_multiprocessing=True,\n",
    "                        **training_params\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"参数评估过程中发生错误: {str(e)}\")\n",
    "                    raise\n",
    "                \n",
    "                # 创建结果目录\n",
    "                results_dir = prep_experiment_dir / 'results'\n",
    "                results_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # 保存训练结果\n",
    "                results_df.to_csv(results_dir / 'train_results.csv', index=False)\n",
    "                logging.info(f\"保存训练结果到: {results_dir / 'train_results.csv'}\")\n",
    "                \n",
    "                # 获取top 10参数进行测试\n",
    "                logging.info(\"\\n开始测试阶段...\")\n",
    "                top_10_params = results_df.nlargest(10, 'optimal_score')\n",
    "                test_results = []\n",
    "                \n",
    "                for idx, row in enumerate(top_10_params.iterrows(), 1):\n",
    "                    _, params_dict = row\n",
    "                    logging.info(f\"测试第 {idx}/10 组参数...\")\n",
    "                    \n",
    "                    model = params_dict['model']\n",
    "                    dictionary = params_dict['dictionary']\n",
    "                    \n",
    "                    # 计算测试集分数\n",
    "                    test_score = calculate_optimal_score(\n",
    "                        model=model,\n",
    "                        texts=test_docs,\n",
    "                        dictionary=dictionary,\n",
    "                        alpha=params_dict['alpha']\n",
    "                    )\n",
    "                    \n",
    "                    test_results.append({\n",
    "                        **params_dict.to_dict(),\n",
    "                        'test_score': test_score\n",
    "                    })\n",
    "                \n",
    "                # 保存测试结果\n",
    "                test_results_df = pd.DataFrame(test_results)\n",
    "                test_results_df.to_csv(results_dir / 'test_results.csv', index=False)\n",
    "                \n",
    "                # 获取最终的top 5结果\n",
    "                top_5_results = test_results_df.nlargest(5, 'test_score')\n",
    "                \n",
    "                # 保存详细结果\n",
    "                results_paths[prep_type] = results_dir / 'final_results.json'\n",
    "                with open(results_paths[prep_type], 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\n",
    "                        'top_5_params': top_5_results.to_dict('records'),\n",
    "                        'best_score': float(top_5_results['test_score'].max()),\n",
    "                        'average_score': float(top_5_results['test_score'].mean()),\n",
    "                        'std_score': float(top_5_results['test_score'].std()),\n",
    "                        'experiment_params': {\n",
    "                            **grid_params,\n",
    "                            **training_params\n",
    "                        }\n",
    "                    }, f, indent=2)\n",
    "                \n",
    "                # 保存top 5模型和词典\n",
    "                for idx, (_, params_dict) in enumerate(top_5_results.iterrows(), 1):\n",
    "                    model = params_dict['model']\n",
    "                    dictionary = params_dict['dictionary']\n",
    "                    \n",
    "                    # 保存模型\n",
    "                    model_path = results_dir / f'model_{idx}.lda'\n",
    "                    model.save(str(model_path))\n",
    "                    \n",
    "                    # 保存词典\n",
    "                    dict_path = results_dir / f'dictionary_{idx}.dict'\n",
    "                    dictionary.save(str(dict_path))\n",
    "                \n",
    "                logging.info(f\"\\n{prep_type} 处理完成\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"{prep_type} 处理过程中发生错误: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 创建汇总表格\n",
    "        logging.info(\"\\n创建实验汇总表格...\")\n",
    "        summary_df = create_summary_table(\n",
    "            results_paths, \n",
    "            n_topics=grid_params['num_topics'][0]\n",
    "        )\n",
    "        \n",
    "        logging.info(\"\\n实验完成！\")\n",
    "        logging.info(f\"详细日志已保存至: {log_file}\")\n",
    "        logging.info(f\"汇总表格已保存至: experiments/lda/summaries/\")\n",
    "        \n",
    "        # 打印最佳结果\n",
    "        for prep_type in results_paths:\n",
    "            with open(results_paths[prep_type], 'r') as f:\n",
    "                results = json.load(f)\n",
    "            logging.info(f\"\\n{prep_type} 最佳结果:\")\n",
    "            logging.info(f\"最佳分数: {results['best_score']:.4f}\")\n",
    "            logging.info(f\"平均分数: {results['average_score']:.4f}\")\n",
    "            logging.info(f\"标准差: {results.get('std_score', 'N/A')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"实验过程中发生错误: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
