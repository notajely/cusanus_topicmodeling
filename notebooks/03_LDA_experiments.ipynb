{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置基础路径和目录\n",
    "base_dir = \"/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\"\n",
    "source_dir = os.path.join(base_dir, 'experiments/lda/spacy/preprocessed')\n",
    "log_dir = os.path.join(base_dir, 'experiments/lda/cusanus/results')\n",
    "\n",
    "# 确保目录存在\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 设置日志\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, f'lda_experiment_{timestamp}.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 23:54:31,339 - INFO - Preparing corpus and dictionary...\n",
      "2024-11-24 23:54:31,345 - INFO - Found 306 text files in /Users/jessie/Documents/Projects/Cusanus_Topic_Modeling/experiments/lda/spacy/preprocessed\n",
      "2024-11-24 23:54:31,350 - INFO - Trying to load la_core_web_lg model directly...\n",
      "2024-11-24 23:54:32,221 - INFO - Processing documents and building vocabulary...\n",
      "  0%|          | 0/306 [00:00<?, ?it/s]2024-11-24 23:54:32,233 - INFO - \n",
      "First file sample (v190_209.txt):\n",
      "2024-11-24 23:54:32,233 - INFO - Paragraph 1:\n",
      "dominus iesus christus romanus epistula dominicus intellego iesus secundus ada filius deus andreo declaro aegeus lego historia lombardicus tertius folium considero epistula apostolus hora somnus surgo\n",
      "\n",
      "Paragraph 2:\n",
      "probo apostolus dilectio completio lex praeceptum dilectio proximus complico proximus subiungo plenitudo lex dilectio scio hora somnus surgo dilectio plenitudo lex scio surgo somnus attendo propior salus credo\n",
      "\n",
      "Paragraph 3:\n",
      "expono tempus credo fides recipio salus mors chr\n",
      "2024-11-24 23:54:32,239 - INFO - \n",
      "Found 9 paragraphs in first file\n",
      "2024-11-24 23:54:32,240 - INFO - First paragraph sample:\n",
      "dominus iesus christus romanus epistula dominicus intellego iesus secundus ada filius deus andreo declaro aegeus lego historia lombardicus tertius folium considero epistula apostolus hora somnus surgo\n",
      "2024-11-24 23:54:32,380 - INFO - \n",
      "First paragraph processing results:\n",
      "2024-11-24 23:54:32,380 - INFO - Original text length: 200\n",
      "2024-11-24 23:54:32,381 - INFO - Valid words found: 26\n",
      "2024-11-24 23:54:32,381 - INFO - Sample valid words: ['dominus', 'iesus', 'christus', 'romanus', 'epistula', 'dominicus', 'intellego', 'iesus', 'secundus', 'ada']\n",
      "100%|██████████| 306/306 [01:56<00:00,  2.62it/s]\n",
      "2024-11-24 23:56:28,996 - INFO - \n",
      "Processing summary:\n",
      "2024-11-24 23:56:28,999 - INFO - Total paragraphs processed: 2305\n",
      "2024-11-24 23:56:28,999 - INFO - Total unique words before filtering: 11217\n",
      "2024-11-24 23:56:29,014 - INFO - Words remaining after frequency filtering: 7048\n",
      "2024-11-24 23:56:29,036 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-11-24 23:56:29,085 - INFO - built Dictionary<7048 unique tokens: ['aegeus', 'andreo', 'declaro', 'dominicus', 'epistula']...> from 2305 documents (total 124220 corpus positions)\n",
      "2024-11-24 23:56:29,086 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<7048 unique tokens: ['aegeus', 'andreo', 'declaro', 'dominicus', 'epistula']...> from 2305 documents (total 124220 corpus positions)\", 'datetime': '2024-11-24T23:56:29.086456', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-11-24 23:56:29,189 - INFO - Final statistics:\n",
      "2024-11-24 23:56:29,190 - INFO - Total documents: 2305\n",
      "2024-11-24 23:56:29,190 - INFO - Total paragraphs: 2305\n",
      "2024-11-24 23:56:29,190 - INFO - Vocabulary size: 7048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据处理统计:\n",
      "原始段落总数: 2305\n",
      "有效文档数量: 2305\n",
      "词典大小: 7048\n",
      "\n",
      "每段平均词数: 114.42\n",
      "每段最少词数: 1\n",
      "每段最多词数: 3590\n"
     ]
    }
   ],
   "source": [
    "def prepare_corpus_and_dictionary():\n",
    "    \"\"\"准备词典和语料库，处理带段落标记的文本\"\"\"\n",
    "    logging.info(\"Preparing corpus and dictionary...\")\n",
    "    \n",
    "    # 检查目录是否存在\n",
    "    if not os.path.exists(source_dir):\n",
    "        raise ValueError(f\"源目录不存在: {source_dir}\")\n",
    "    \n",
    "    # 检查文件数量\n",
    "    txt_files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n",
    "    logging.info(f\"Found {len(txt_files)} text files in {source_dir}\")\n",
    "    \n",
    "    if not txt_files:\n",
    "        raise ValueError(f\"在 {source_dir} 中没有找到.txt文件\")\n",
    "    \n",
    "    # 1. 加载spaCy模型\n",
    "    try:\n",
    "        nlp = spacy.load('models/la_core_web_lg-0.1.0-py3-none-any.whl')\n",
    "    except:\n",
    "        logging.info(\"Trying to load la_core_web_lg model directly...\")\n",
    "        nlp = spacy.load('la_core_web_lg')\n",
    "    \n",
    "    def is_valid_word(word):\n",
    "        \"\"\"检查词是否有效\"\"\"\n",
    "        return (not bool(re.search(r'\\d', word)) and \n",
    "                not all(char in '.,;:!?\"\\'()[]{}' for char in word) and \n",
    "                len(word.strip()) > 0)\n",
    "    \n",
    "    # 3. 处理文档并构建词频统计\n",
    "    word_stats = Counter()\n",
    "    all_texts = []\n",
    "    paragraph_info = []\n",
    "    \n",
    "    logging.info(\"Processing documents and building vocabulary...\")\n",
    "    for file in tqdm(txt_files):\n",
    "        file_path = os.path.join(source_dir, file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # 打印第一个文件的内容示例\n",
    "                if file == txt_files[0]:\n",
    "                    logging.info(f\"\\nFirst file sample ({file}):\")\n",
    "                    logging.info(content[:500])\n",
    "                \n",
    "                # 按段落分割\n",
    "                paragraphs = re.split(r'Paragraph \\d:+\\n', content)[1:]\n",
    "                \n",
    "                if file == txt_files[0]:\n",
    "                    logging.info(f\"\\nFound {len(paragraphs)} paragraphs in first file\")\n",
    "                    if paragraphs:\n",
    "                        logging.info(f\"First paragraph sample:\\n{paragraphs[0][:200]}\")\n",
    "                \n",
    "                for para_idx, para in enumerate(paragraphs, 1):\n",
    "                    para = para.strip()\n",
    "                    if para:  # 忽略空段落\n",
    "                        doc = nlp(para)\n",
    "                        valid_words = [token.text.lower() for token in doc \n",
    "                                     if not token.is_stop and \n",
    "                                     not token.is_punct and \n",
    "                                     is_valid_word(token.text.lower())]\n",
    "                        \n",
    "                        if valid_words:  # 只添加非空段落\n",
    "                            all_texts.append(valid_words)\n",
    "                            word_stats.update(valid_words)\n",
    "                            paragraph_info.append({\n",
    "                                'file': file,\n",
    "                                'paragraph_idx': para_idx,\n",
    "                                'text': para,\n",
    "                                'words': valid_words\n",
    "                            })\n",
    "                            \n",
    "                            # 打印第一个文件的第一个段落的处理结果\n",
    "                            if file == txt_files[0] and para_idx == 1:\n",
    "                                logging.info(f\"\\nFirst paragraph processing results:\")\n",
    "                                logging.info(f\"Original text length: {len(para)}\")\n",
    "                                logging.info(f\"Valid words found: {len(valid_words)}\")\n",
    "                                logging.info(f\"Sample valid words: {valid_words[:10]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logging.info(f\"\\nProcessing summary:\")\n",
    "    logging.info(f\"Total paragraphs processed: {len(paragraph_info)}\")\n",
    "    logging.info(f\"Total unique words before filtering: {len(word_stats)}\")\n",
    "    \n",
    "    # 4. 筛选词频在2-200之间的词\n",
    "    valid_words = {word for word, count in word_stats.items() \n",
    "                  if 2 <= count <= 200}\n",
    "    \n",
    "    logging.info(f\"Words remaining after frequency filtering: {len(valid_words)}\")\n",
    "    \n",
    "    # 5. 基于有效词汇过滤文档\n",
    "    filtered_texts = [[word for word in text if word in valid_words] \n",
    "                     for text in all_texts]\n",
    "    \n",
    "    # 6. 创建词典和语料库\n",
    "    dictionary = corpora.Dictionary(filtered_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in filtered_texts]\n",
    "    \n",
    "    logging.info(f\"Final statistics:\")\n",
    "    logging.info(f\"Total documents: {len(corpus)}\")\n",
    "    logging.info(f\"Total paragraphs: {len(paragraph_info)}\")\n",
    "    logging.info(f\"Vocabulary size: {len(dictionary)}\")\n",
    "    \n",
    "    # 7. 保存段落信息\n",
    "    pd.DataFrame(paragraph_info).to_csv(\n",
    "        os.path.join(log_dir, f'paragraph_info_{timestamp}.csv'),\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    return dictionary, corpus, filtered_texts, paragraph_info\n",
    "\n",
    "# 运行准备过程\n",
    "dictionary, corpus, texts, paragraph_info = prepare_corpus_and_dictionary()\n",
    "\n",
    "# 打印详细统计信息\n",
    "print(\"\\n数据处理统计:\")\n",
    "print(f\"原始段落总数: {len(paragraph_info)}\")\n",
    "print(f\"有效文档数量: {len(corpus)}\")\n",
    "print(f\"词典大小: {len(dictionary)}\")\n",
    "\n",
    "if paragraph_info:\n",
    "    words_per_para = [len(para['words']) for para in paragraph_info]\n",
    "    print(f\"\\n每段平均词数: {np.mean(words_per_para):.2f}\")\n",
    "    print(f\"每段最少词数: {min(words_per_para)}\")\n",
    "    print(f\"每段最多词数: {max(words_per_para)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_npmi(npmi):\n",
    "    \"\"\"将NPMI标准化到[0,1]区间\"\"\"\n",
    "    return (npmi + 1) / 2\n",
    "\n",
    "def normalize_diversity(diversity):\n",
    "    \"\"\"确保diversity在[0,1]区间内\"\"\"\n",
    "    return max(0, min(1, diversity))\n",
    "\n",
    "def evaluate_model(model, corpus, dictionary, texts):\n",
    "    \"\"\"评估LDA模型的NPMI和多样性，处理极端值\"\"\"\n",
    "    try:\n",
    "        # 计算NPMI\n",
    "        coherence_model = models.coherencemodel.CoherenceModel(\n",
    "            model=model, \n",
    "            texts=texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "        npmi = coherence_model.get_coherence()\n",
    "        \n",
    "        # 处理NPMI的极端值和无效值\n",
    "        if np.isinf(npmi) or np.isnan(npmi):\n",
    "            npmi = -1.0  # 设置一个默认的最低值\n",
    "        \n",
    "        # 确保NPMI在[-1, 1]范围内\n",
    "        npmi = max(-1.0, min(1.0, npmi))\n",
    "        \n",
    "        # 标准化NPMI到[0,1]区间\n",
    "        npmi_normalized = (npmi + 1) / 2\n",
    "        \n",
    "        # 计算主题多样性\n",
    "        topics = model.show_topics(formatted=False)\n",
    "        unique_words = set()\n",
    "        total_words = 0\n",
    "        for topic_id, topic in topics:\n",
    "            words = [w for w, _ in topic]\n",
    "            unique_words.update(words)\n",
    "            total_words += len(words)\n",
    "        \n",
    "        # 确保diversity在[0,1]区间内\n",
    "        diversity = max(0.0, min(1.0, len(unique_words) / total_words))\n",
    "        \n",
    "        # 最终检查确保返回值都是有效的浮点数\n",
    "        if np.isinf(npmi_normalized) or np.isnan(npmi_normalized):\n",
    "            npmi_normalized = 0.0\n",
    "        if np.isinf(diversity) or np.isnan(diversity):\n",
    "            diversity = 0.0\n",
    "            \n",
    "        logging.info(f\"Evaluation metrics - NPMI: {npmi_normalized:.4f}, Diversity: {diversity:.4f}\")\n",
    "        \n",
    "        return float(npmi_normalized), float(diversity)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in evaluate_model: {str(e)}\")\n",
    "        # 发生错误时返回默认值\n",
    "        return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 23:07:11,839 - INFO - Preparing corpus and dictionary...\n",
      "2024-11-24 23:07:11,843 - INFO - Preparing corpus and dictionary...\n",
      "2024-11-24 23:07:11,846 - INFO - Found 308 text files in /Users/jessie/Documents/Projects/Cusanus_Topic_Modeling/experiments/lda/cusanus/preprocessed\n",
      "2024-11-24 23:07:11,868 - INFO - Trying to load la_core_web_lg model directly...\n",
      "2024-11-24 23:07:13,115 - INFO - Processing documents and building vocabulary...\n",
      "  0%|          | 0/308 [00:00<?, ?it/s]2024-11-24 23:07:13,127 - INFO - \n",
      "First file sample (h160_013_cleaned.txt):\n",
      "2024-11-24 23:07:13,127 - INFO - Paragraph 1\n",
      "treverensis templum dominus larissa constantinus testamentum templum element vox terraemotus grando magnus signum magnus appareo larissa mulier ioannes sol luna pes behalten corona stella apocalypsis verbum expono mater ecclesia generalis specialis principalis membrum ecclesia gloriosus virgo mare\n",
      "\n",
      "Paragraph 2\n",
      "primus generalis expositio expono templum dominus christus habitaculum divinitas treverensis larissa mundus propalo tempus nativitas templum treverensis renatus crux miles leic\n",
      "2024-11-24 23:07:13,127 - INFO - \n",
      "Found 7 paragraphs in first file\n",
      "2024-11-24 23:07:13,127 - INFO - First paragraph sample:\n",
      "treverensis templum dominus larissa constantinus testamentum templum element vox terraemotus grando magnus signum magnus appareo larissa mulier ioannes sol luna pes behalten corona stella apocalypsis \n",
      "2024-11-24 23:07:13,233 - INFO - \n",
      "First paragraph processing results:\n",
      "2024-11-24 23:07:13,233 - INFO - Original text length: 298\n",
      "2024-11-24 23:07:13,234 - INFO - Valid words found: 37\n",
      "2024-11-24 23:07:13,234 - INFO - Sample valid words: ['treverensis', 'templum', 'dominus', 'larissa', 'constantinus', 'testamentum', 'templum', 'element', 'vox', 'terraemotus']\n",
      "100%|██████████| 308/308 [02:01<00:00,  2.52it/s]\n",
      "2024-11-24 23:09:15,119 - INFO - \n",
      "Processing summary:\n",
      "2024-11-24 23:09:15,122 - INFO - Total paragraphs processed: 4645\n",
      "2024-11-24 23:09:15,123 - INFO - Total unique words before filtering: 9560\n",
      "2024-11-24 23:09:15,128 - INFO - Words remaining after frequency filtering: 6402\n",
      "2024-11-24 23:09:15,200 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-11-24 23:09:15,412 - INFO - built Dictionary<6402 unique tokens: ['apocalypsis', 'behalten', 'constantinus', 'corona', 'element']...> from 4645 documents (total 126196 corpus positions)\n",
      "2024-11-24 23:09:15,416 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<6402 unique tokens: ['apocalypsis', 'behalten', 'constantinus', 'corona', 'element']...> from 4645 documents (total 126196 corpus positions)\", 'datetime': '2024-11-24T23:09:15.415044', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-11-24 23:09:15,502 - INFO - Final statistics:\n",
      "2024-11-24 23:09:15,502 - INFO - Total documents: 4645\n",
      "2024-11-24 23:09:15,503 - INFO - Total paragraphs: 4645\n",
      "2024-11-24 23:09:15,503 - INFO - Vocabulary size: 6402\n",
      "2024-11-24 23:09:15,898 - INFO - Starting LDA experiments...\n",
      "2024-11-24 23:09:15,901 - INFO - Starting 32 experiments with 5 folds each\n",
      "2024-11-24 23:09:15,902 - INFO - Running experiment 1/32: topics=10, alpha=symmetric, eta=symmetric\n",
      "2024-11-24 23:09:15,919 - INFO - Processing fold 1/5\n",
      "2024-11-24 23:09:15,924 - INFO - using symmetric alpha at 0.1\n",
      "2024-11-24 23:09:15,924 - INFO - using symmetric eta at 0.1\n",
      "2024-11-24 23:09:15,926 - INFO - using serial LDA version on this node\n",
      "2024-11-24 23:09:15,932 - INFO - running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 3716 documents, updating model once every 2000 documents, evaluating perplexity every 3716 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2024-11-24 23:09:15,932 - WARNING - too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2024-11-24 23:09:15,932 - INFO - PROGRESS: pass 0, at document #2000/3716\n",
      "2024-11-24 23:09:16,299 - INFO - merging changes from 2000 documents into a model of 3716 documents\n",
      "2024-11-24 23:09:16,305 - INFO - topic #2 (0.100): 0.005*\"gewerden\" + 0.005*\"ioannes\" + 0.004*\"subdo\" + 0.004*\"naturalis\" + 0.004*\"stella\" + 0.004*\"sapiens\" + 0.003*\"salvo\" + 0.003*\"felicitas\" + 0.003*\"fons\" + 0.003*\"mereo\"\n",
      "2024-11-24 23:09:16,305 - INFO - topic #0 (0.100): 0.005*\"gewerden\" + 0.004*\"notitia\" + 0.004*\"color\" + 0.003*\"sentio\" + 0.003*\"tollo\" + 0.003*\"transfero\" + 0.003*\"servo\" + 0.003*\"manifesto\" + 0.003*\"formo\" + 0.003*\"ovis\"\n",
      "2024-11-24 23:09:16,306 - INFO - topic #6 (0.100): 0.011*\"wir\" + 0.007*\"got\" + 0.007*\"uns\" + 0.005*\"resurrectio\" + 0.005*\"umb\" + 0.005*\"alst\" + 0.004*\"unser\" + 0.004*\"dar\" + 0.004*\"syn\" + 0.004*\"alla\"\n",
      "2024-11-24 23:09:16,306 - INFO - topic #1 (0.100): 0.006*\"substantia\" + 0.004*\"figura\" + 0.004*\"colonia\" + 0.004*\"cogito\" + 0.004*\"uns\" + 0.004*\"wir\" + 0.003*\"immortalitas\" + 0.003*\"semino\" + 0.003*\"pereo\" + 0.003*\"adeodatus\"\n",
      "2024-11-24 23:09:16,306 - INFO - topic #4 (0.100): 0.005*\"behalten\" + 0.004*\"pes\" + 0.004*\"gewerden\" + 0.003*\"populus\" + 0.003*\"subicio\" + 0.003*\"nobilis\" + 0.003*\"iniquitas\" + 0.003*\"mysterium\" + 0.003*\"optimus\" + 0.003*\"divitiae\"\n",
      "2024-11-24 23:09:16,307 - INFO - topic diff=6.006648, rho=1.000000\n",
      "2024-11-24 23:09:16,592 - INFO - -9.301 per-word bound, 630.8 perplexity estimate based on a held-out corpus of 1716 documents with 46885 words\n",
      "2024-11-24 23:09:16,593 - INFO - PROGRESS: pass 0, at document #3716/3716\n",
      "2024-11-24 23:09:16,829 - INFO - merging changes from 1716 documents into a model of 3716 documents\n",
      "2024-11-24 23:09:16,831 - INFO - topic #9 (0.100): 0.005*\"stella\" + 0.005*\"sanctifico\" + 0.005*\"ferrum\" + 0.005*\"facies\" + 0.005*\"multitudo\" + 0.005*\"magnes\" + 0.004*\"persis\" + 0.004*\"respondeo\" + 0.003*\"elevo\" + 0.003*\"eicio\"\n",
      "2024-11-24 23:09:16,831 - INFO - topic #3 (0.100): 0.007*\"gesein\" + 0.006*\"servo\" + 0.005*\"jerusalem\" + 0.005*\"abraha\" + 0.005*\"proximus\" + 0.005*\"iustifico\" + 0.004*\"manifesto\" + 0.004*\"supremus\" + 0.004*\"consummo\" + 0.004*\"explico\"\n",
      "2024-11-24 23:09:16,831 - INFO - topic #2 (0.100): 0.007*\"consummo\" + 0.006*\"trinitas\" + 0.005*\"fons\" + 0.004*\"stella\" + 0.004*\"jeud\" + 0.004*\"sapiens\" + 0.004*\"meritum\" + 0.004*\"plenitudo\" + 0.004*\"ioannes\" + 0.004*\"gewerden\"\n",
      "2024-11-24 23:09:16,832 - INFO - topic #4 (0.100): 0.006*\"oblatio\" + 0.005*\"behalten\" + 0.004*\"pes\" + 0.004*\"graecus\" + 0.004*\"offero\" + 0.004*\"populus\" + 0.004*\"margarita\" + 0.004*\"luna\" + 0.004*\"peccator\" + 0.003*\"sacrificium\"\n",
      "2024-11-24 23:09:16,832 - INFO - topic #5 (0.100): 0.008*\"dimitto\" + 0.006*\"poena\" + 0.005*\"culpa\" + 0.005*\"nuptiae\" + 0.004*\"concupiscentia\" + 0.004*\"ministro\" + 0.004*\"serpens\" + 0.004*\"debitum\" + 0.004*\"martha\" + 0.003*\"paradisus\"\n",
      "2024-11-24 23:09:16,833 - INFO - topic diff=0.730643, rho=0.707107\n",
      "2024-11-24 23:09:16,833 - INFO - LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=6402, num_topics=10, decay=0.5, chunksize=2000> in 0.90s', 'datetime': '2024-11-24T23:09:16.833411', 'gensim': '4.3.3', 'python': '3.10.15 (main, Oct  3 2024, 02:24:49) [Clang 14.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-11-24 23:09:16,839 - INFO - using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 141\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# 2. 运行实验\u001b[39;00m\n\u001b[1;32m    140\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting LDA experiments...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m results_df, topic_words_df, paragraph_topics_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_lda_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparagraph_info\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# 3. 显示最佳结果\u001b[39;00m\n\u001b[1;32m    146\u001b[0m best_result \u001b[38;5;241m=\u001b[39m results_df\u001b[38;5;241m.\u001b[39mloc[results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_optimal_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax()]\n",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m, in \u001b[0;36mrun_lda_experiment\u001b[0;34m(dictionary, corpus, texts, paragraph_info, k)\u001b[0m\n\u001b[1;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mLdaModel(\n\u001b[1;32m     44\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mtrain_corpus,\n\u001b[1;32m     45\u001b[0m     id2word\u001b[38;5;241m=\u001b[39mdictionary,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m npmi, diversity \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m optimal_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m npmi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m diversity\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 保存fold结果\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, corpus, dictionary, texts)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 计算NPMI\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     coherence_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mcoherencemodel\u001b[38;5;241m.\u001b[39mCoherenceModel(\n\u001b[1;32m     14\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     15\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m     16\u001b[0m         dictionary\u001b[38;5;241m=\u001b[39mdictionary,\n\u001b[1;32m     17\u001b[0m         coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_npmi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m     npmi \u001b[38;5;241m=\u001b[39m \u001b[43mcoherence_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# 处理NPMI的极端值和无效值\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(npmi) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(npmi):\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/gensim/models/coherencemodel.py:614\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coherence\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    606\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     confirmed_measures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence_per_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_measures(confirmed_measures)\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/gensim/models/coherencemodel.py:574\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    572\u001b[0m     segmented_topics \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39mseg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopics)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmented_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(with_std\u001b[38;5;241m=\u001b[39mwith_std, with_support\u001b[38;5;241m=\u001b[39mwith_support)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;129;01min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/gensim/models/coherencemodel.py:546\u001b[0m, in \u001b[0;36mCoherenceModel.estimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    544\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyed_vectors\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/gensim/topic_coherence/probability_estimation.py:156\u001b[0m, in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    154\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[1;32m    155\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to estimate probabilities from sliding windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, accumulator)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/gensim/topic_coherence/text_analysis.py:437\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.accumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, window_size):\n\u001b[0;32m--> 437\u001b[0m     workers, input_q, output_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue_all_texts(input_q, texts, window_size)\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/gensim/topic_coherence/text_analysis.py:471\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.start_workers\u001b[0;34m(self, window_size)\u001b[0m\n\u001b[1;32m    469\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m PatchedWordOccurrenceAccumulator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevant_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary)\n\u001b[1;32m    470\u001b[0m     worker \u001b[38;5;241m=\u001b[39m AccumulatingWorker(input_q, output_q, accumulator, window_size)\n\u001b[0;32m--> 471\u001b[0m     \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     workers\u001b[38;5;241m.\u001b[39mappend(worker)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m workers, input_q, output_q\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_lda_experiment(dictionary, corpus, texts, paragraph_info, k=5):\n",
    "    \"\"\"运行LDA实验并保存详细结果\"\"\"\n",
    "    # 定义参数搜索空间\n",
    "    num_topics_range = [10, 15]\n",
    "    alpha_range = ['symmetric', 0.1, 0.3, 0.5]\n",
    "    eta_range = ['symmetric', 0.1, 0.3, 0.5]\n",
    "    \n",
    "    # 创建结果列表\n",
    "    experiment_results = []\n",
    "    topic_words_results = []\n",
    "    paragraph_topic_results = []\n",
    "    \n",
    "    experiment_id = 1  # 为每个实验添加唯一ID\n",
    "    \n",
    "    # 创建KFold对象\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    total_experiments = len(num_topics_range) * len(alpha_range) * len(eta_range)\n",
    "    logging.info(f\"Starting {total_experiments} experiments with {k} folds each\")\n",
    "    \n",
    "    for num_topics in num_topics_range:\n",
    "        for alpha in alpha_range:\n",
    "            for eta in eta_range:\n",
    "                logging.info(f\"Running experiment {experiment_id}/{total_experiments}: \"\n",
    "                           f\"topics={num_topics}, alpha={alpha}, eta={eta}\")\n",
    "                \n",
    "                # 存储每个fold的结果\n",
    "                fold_scores = []\n",
    "                \n",
    "                # 获取所有fold的划分\n",
    "                fold_splits = list(kf.split(corpus))\n",
    "                \n",
    "                for fold, (train_idx, val_idx) in enumerate(fold_splits):\n",
    "                    logging.info(f\"Processing fold {fold+1}/{k}\")\n",
    "                    \n",
    "                    # 准备训练和验证数据\n",
    "                    train_corpus = [corpus[i] for i in train_idx]\n",
    "                    val_corpus = [corpus[i] for i in val_idx]\n",
    "                    val_texts = [texts[i] for i in val_idx]\n",
    "                    \n",
    "                    try:\n",
    "                        # 训练模型\n",
    "                        model = models.LdaModel(\n",
    "                            corpus=train_corpus,\n",
    "                            id2word=dictionary,\n",
    "                            num_topics=num_topics,\n",
    "                            alpha=alpha,\n",
    "                            eta=eta,\n",
    "                            random_state=42\n",
    "                        )\n",
    "                        \n",
    "                        # 评估模型\n",
    "                        npmi, diversity = evaluate_model(model, val_corpus, dictionary, val_texts)\n",
    "                        optimal_score = 0.5 * npmi + 0.5 * diversity\n",
    "                        \n",
    "                        # 保存fold结果\n",
    "                        fold_scores.append({\n",
    "                            'experiment_id': experiment_id,\n",
    "                            'fold': fold,\n",
    "                            'npmi': npmi,\n",
    "                            'diversity': diversity,\n",
    "                            'optimal_score': optimal_score\n",
    "                        })\n",
    "                        \n",
    "                        # 保存主题词\n",
    "                        for topic_id, topic in model.show_topics(formatted=False):\n",
    "                            topic_words_results.append({\n",
    "                                'experiment_id': experiment_id,\n",
    "                                'fold': fold,\n",
    "                                'num_topics': num_topics,\n",
    "                                'alpha': str(alpha),\n",
    "                                'eta': str(eta),\n",
    "                                'topic_id': topic_id,\n",
    "                                'words': ', '.join([word for word, _ in topic]),\n",
    "                                'word_probs': ', '.join([f\"{prob:.4f}\" for _, prob in topic])\n",
    "                            })\n",
    "                        \n",
    "                        # 保存段落主题分配\n",
    "                        for idx, bow in enumerate(val_corpus):\n",
    "                            topic_dist = model.get_document_topics(bow)\n",
    "                            main_topic = max(topic_dist, key=lambda x: x[1]) if topic_dist else (-1, 0)\n",
    "                            paragraph_topic_results.append({\n",
    "                                'experiment_id': experiment_id,\n",
    "                                'fold': fold,\n",
    "                                'paragraph_id': val_idx[idx],\n",
    "                                'file': paragraph_info[val_idx[idx]]['file'],\n",
    "                                'paragraph_idx': paragraph_info[val_idx[idx]]['paragraph_idx'],\n",
    "                                'main_topic': main_topic[0],\n",
    "                                'topic_prob': main_topic[1],\n",
    "                                'topic_distribution': str(dict(topic_dist))\n",
    "                            })\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error in experiment {experiment_id}, fold {fold}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # 计算平均分数\n",
    "                if fold_scores:\n",
    "                    avg_scores = pd.DataFrame(fold_scores).mean()\n",
    "                    experiment_results.append({\n",
    "                        'experiment_id': experiment_id,\n",
    "                        'num_topics': num_topics,\n",
    "                        'alpha': str(alpha),\n",
    "                        'eta': str(eta),\n",
    "                        'avg_npmi': avg_scores['npmi'],\n",
    "                        'avg_diversity': avg_scores['diversity'],\n",
    "                        'avg_optimal_score': avg_scores['optimal_score'],\n",
    "                        'fold_count': len(fold_scores)\n",
    "                    })\n",
    "                \n",
    "                # 保存中间结果\n",
    "                if experiment_results:\n",
    "                    pd.DataFrame(experiment_results).to_csv(\n",
    "                        f'{log_dir}/intermediate_results_{timestamp}.csv', \n",
    "                        index=False\n",
    "                    )\n",
    "                \n",
    "                experiment_id += 1\n",
    "    \n",
    "    # 创建最终的DataFrame\n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    topic_words_df = pd.DataFrame(topic_words_results)\n",
    "    paragraph_topics_df = pd.DataFrame(paragraph_topic_results)\n",
    "    \n",
    "    # 保存所有结果\n",
    "    results_df.to_csv(f'{log_dir}/experiment_results_{timestamp}.csv', index=False)\n",
    "    topic_words_df.to_csv(f'{log_dir}/topic_words_{timestamp}.csv', index=False)\n",
    "    paragraph_topics_df.to_csv(f'{log_dir}/paragraph_topics_{timestamp}.csv', index=False)\n",
    "    \n",
    "    return results_df, topic_words_df, paragraph_topics_df\n",
    "\n",
    "# 主执行流程\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. 准备数据\n",
    "        logging.info(\"Preparing corpus and dictionary...\")\n",
    "        dictionary, corpus, texts, paragraph_info = prepare_corpus_and_dictionary()\n",
    "        \n",
    "        # 2. 运行实验\n",
    "        logging.info(\"Starting LDA experiments...\")\n",
    "        results_df, topic_words_df, paragraph_topics_df = run_lda_experiment(\n",
    "            dictionary, corpus, texts, paragraph_info\n",
    "        )\n",
    "        \n",
    "        # 3. 显示最佳结果\n",
    "        best_result = results_df.loc[results_df['avg_optimal_score'].idxmax()]\n",
    "        logging.info(\"\\nBest Model Configuration:\")\n",
    "        logging.info(f\"Experiment ID: {best_result['experiment_id']}\")\n",
    "        logging.info(f\"Number of Topics: {best_result['num_topics']}\")\n",
    "        logging.info(f\"Alpha: {best_result['alpha']}\")\n",
    "        logging.info(f\"Eta: {best_result['eta']}\")\n",
    "        logging.info(f\"Average NPMI: {best_result['avg_npmi']:.4f}\")\n",
    "        logging.info(f\"Average Diversity: {best_result['avg_diversity']:.4f}\")\n",
    "        logging.info(f\"Average Optimal Score: {best_result['avg_optimal_score']:.4f}\")\n",
    "        \n",
    "        # 4. 保存最佳结果的主题词\n",
    "        best_topics = topic_words_df[\n",
    "            topic_words_df['experiment_id'] == best_result['experiment_id']\n",
    "        ]\n",
    "        best_topics.to_csv(f'{log_dir}/best_topics_{timestamp}.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('experiments/lda/cusanus/results/paragraph_info_20241124_212811.csv')\n",
    "\n",
    "# 保存为xlsx格式\n",
    "df.to_excel('experiments/lda/cusanus/results/paragraph_info_20241124_212811.xlsx', \n",
    "            index=False, # 不包含行索引\n",
    "            engine='openpyxl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
