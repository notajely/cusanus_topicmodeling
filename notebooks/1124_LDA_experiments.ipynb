{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\n"
     ]
    }
   ],
   "source": [
    "# 设置工作目录\n",
    "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
    "os.chdir(project_root)\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "\n",
    "# 设置日志记录\n",
    "os.makedirs('results/parameter_search', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename='results/parameter_search/experiment.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法参数配置\n",
    "PARAMETERS = {\n",
    "    'cltk': {\n",
    "        'tfidf_threshold': 0.12,\n",
    "        'alpha_weight': 0.3,    # NPMI的权重\n",
    "        'lambda_weight': 0.1    # Overlap的惩罚权重\n",
    "    },\n",
    "    'cusanus': {\n",
    "        'tfidf_threshold': 0.30,\n",
    "        'alpha_weight': 0.3,\n",
    "        'lambda_weight': 0.1\n",
    "    },\n",
    "    'stanza': {\n",
    "        'tfidf_threshold': 0.19,\n",
    "        'alpha_weight': 0.3,\n",
    "        'lambda_weight': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# LDA参数搜索空间\n",
    "num_topics_list = [10]\n",
    "alpha_list = ['symmetric', 0.1, 0.3, 0.5, 0.7]\n",
    "eta_list = ['symmetric', 0.1, 0.3, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(method, data_type='train'):\n",
    "    \"\"\"加载文件列表\"\"\"\n",
    "    if data_type == 'train':\n",
    "        data_dir = f'data/preprocessed/{method}'\n",
    "    else:\n",
    "        data_dir = f'data/testset/{method}'\n",
    "    \n",
    "    files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "    logging.info(f\"从 {data_dir} 加载了 {len(files)} 个文件\")\n",
    "    return files\n",
    "\n",
    "def load_paragraphs_from_file(file_path):\n",
    "    \"\"\"从单个文件加载段落\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # 处理每个段落中的词\n",
    "        processed_paragraphs = []\n",
    "        for para in paragraphs:\n",
    "            words = []\n",
    "            for token in para.split():\n",
    "                if '/' in token:  # 如果有词性标注\n",
    "                    word = token.rsplit('/', 1)[0]  # 只保留词形\n",
    "                    words.append(word)\n",
    "                else:\n",
    "                    words.append(token)\n",
    "            if words:  # 只添加非空段落\n",
    "                processed_paragraphs.append(' '.join(words))\n",
    "        \n",
    "        return processed_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_paragraphs(method, data_type='train'):\n",
    "    \"\"\"加载所有段落\"\"\"\n",
    "    files = load_files(method, data_type)\n",
    "    all_paragraphs = []\n",
    "    paragraph_info = []\n",
    "    \n",
    "    for file_path in tqdm(files, desc=f\"加载{data_type}集文件\"):\n",
    "        paragraphs = load_paragraphs_from_file(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        for i, para in enumerate(paragraphs):\n",
    "            all_paragraphs.append(para)\n",
    "            paragraph_info.append({\n",
    "                'file': file_name,\n",
    "                'paragraph_num': i\n",
    "            })\n",
    "    \n",
    "    logging.info(f\"{method} {data_type}集:\")\n",
    "    logging.info(f\"- 总文件数: {len(files)}\")\n",
    "    logging.info(f\"- 总段落数: {len(all_paragraphs)}\")\n",
    "    if all_paragraphs:\n",
    "        logging.info(f\"- 段落样本:\\n{all_paragraphs[0]}\\n{all_paragraphs[-1]}\")\n",
    "    \n",
    "    return all_paragraphs, paragraph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paragraphs_with_tfidf(paragraphs, threshold):\n",
    "    \"\"\"使用TF-IDF过滤段落\"\"\"\n",
    "    if not paragraphs:\n",
    "        raise ValueError(\"输入的段落列表为空！\")\n",
    "    \n",
    "    logging.info(f\"TF-IDF过滤前的段落数：{len(paragraphs)}\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
    "        max_tfidf = np.max(tfidf_matrix.toarray(), axis=0)\n",
    "        \n",
    "        # 打印TF-IDF值的分布情况\n",
    "        logging.info(f\"TF-IDF值分布:\")\n",
    "        logging.info(f\"最小值: {max_tfidf.min():.4f}\")\n",
    "        logging.info(f\"最大值: {max_tfidf.max():.4f}\")\n",
    "        logging.info(f\"平均值: {max_tfidf.mean():.4f}\")\n",
    "        logging.info(f\"中位数: {np.median(max_tfidf):.4f}\")\n",
    "        \n",
    "        feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "        kept_words = set(feature_names[max_tfidf >= threshold])\n",
    "        \n",
    "        filtered_paragraphs = []\n",
    "        for para in paragraphs:\n",
    "            words = para.split()\n",
    "            filtered_words = [w for w in words if w in kept_words]\n",
    "            if filtered_words:  # 只保留非空段落\n",
    "                filtered_paragraphs.append(' '.join(filtered_words))\n",
    "        \n",
    "        logging.info(f\"过滤后的段落数：{len(filtered_paragraphs)}\")\n",
    "        logging.info(f\"保留的词汇数：{len(kept_words)}\")\n",
    "        \n",
    "        return filtered_paragraphs, kept_words\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"TF-IDF过滤失败：{str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_score(npmi, diversity, overlap, alpha_weight, lambda_weight):\n",
    "    \"\"\"计算综合得分\n",
    "    \n",
    "    Args:\n",
    "        npmi: NPMI一致性分数\n",
    "        diversity: 主题多样性分数\n",
    "        overlap: 主题重叠度\n",
    "        alpha_weight: NPMI权重\n",
    "        lambda_weight: 重叠度惩罚权重\n",
    "    \n",
    "    Returns:\n",
    "        float: 综合得分\n",
    "    \"\"\"\n",
    "    return alpha_weight * npmi + (1 - alpha_weight) * diversity - lambda_weight * overlap\n",
    "\n",
    "def compute_topic_overlap(model, num_topics, topn=10):\n",
    "    \"\"\"计算主题重叠度\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的LDA模型\n",
    "        num_topics: 主题数量\n",
    "        topn: 每个主题考虑的top词数量\n",
    "    \n",
    "    Returns:\n",
    "        float: 平均重叠度\n",
    "    \"\"\"\n",
    "    # 获取所有主题的前topn个词\n",
    "    topic_words = []\n",
    "    for i in range(num_topics):\n",
    "        top_words = [word for word, _ in model.show_topic(i, topn=topn)]\n",
    "        topic_words.append(set(top_words))\n",
    "    \n",
    "    # 计算平均重叠度\n",
    "    overlap_scores = []\n",
    "    for i in range(num_topics):\n",
    "        for j in range(i + 1, num_topics):\n",
    "            overlap = len(topic_words[i] & topic_words[j]) / topn\n",
    "            overlap_scores.append(overlap)\n",
    "    \n",
    "    return np.mean(overlap_scores) if overlap_scores else 0.0\n",
    "\n",
    "def prepare_corpus(paragraphs):\n",
    "    \"\"\"准备语料库\n",
    "    \n",
    "    Args:\n",
    "        paragraphs: 段落列表\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (gensim词典, 语料库)\n",
    "    \"\"\"\n",
    "    # 将文本分词\n",
    "    texts = [para.split() for para in paragraphs]\n",
    "    \n",
    "    # 创建词典\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # 创建语料库\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    logging.info(f\"词典大小: {len(dictionary)}\")\n",
    "    logging.info(f\"语料库大小: {len(corpus)}\")\n",
    "    \n",
    "    return dictionary, corpus\n",
    "\n",
    "def train_lda_model(corpus, dictionary, num_topics, alpha, eta):\n",
    "    \"\"\"训练LDA模型\n",
    "    \n",
    "    Args:\n",
    "        corpus: 语料库\n",
    "        dictionary: 词典\n",
    "        num_topics: 主题数量\n",
    "        alpha: 文档-主题分布的先验\n",
    "        eta: 主题-词分布的先验\n",
    "    \n",
    "    Returns:\n",
    "        LdaModel: 训练好的模型\n",
    "    \"\"\"\n",
    "    model = models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        passes=10,\n",
    "        iterations=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, val_paragraphs, dictionary, num_topics):\n",
    "    \"\"\"评估模型性能\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的LDA模型\n",
    "        val_paragraphs: 验证集段落\n",
    "        dictionary: 词典\n",
    "        num_topics: 主题数量\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (NPMI分数, 多样性分数, 重叠度分数)\n",
    "    \"\"\"\n",
    "    # 准备验证集\n",
    "    val_texts = [para.split() for para in val_paragraphs]\n",
    "    \n",
    "    # 计算NPMI\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=val_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_npmi'\n",
    "    )\n",
    "    npmi = coherence_model.get_coherence()\n",
    "    \n",
    "    # 计算多样性\n",
    "    top_words_per_topic = []\n",
    "    for topic_id in range(num_topics):\n",
    "        top_words = [word for word, prob in model.show_topic(topic_id, topn=10)]\n",
    "        top_words_per_topic.extend(top_words)\n",
    "    diversity = len(set(top_words_per_topic)) / (num_topics * 10)\n",
    "    \n",
    "    # 计算重叠度\n",
    "    overlap = compute_topic_overlap(model, num_topics)\n",
    "    \n",
    "    return npmi, diversity, overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 306/306 [00:00<00:00, 2151.23it/s]\n",
      "加载test集文件: 100%|██████████| 30/30 [00:00<00:00, 3356.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cltk方法:\n",
      "训练集段落数: 4516\n",
      "测试集段落数: 514\n",
      "训练集样本:\n",
      "Paragraph 1: remitto pecco diligo mare peccatrix peccator sordes tenebrae circumvoluta sanctus spiritus receptaculis purus caelum1 gaudium gloriosissimo triumphus proveho diligo careo operio multitudo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 308/308 [00:00<00:00, 3341.36it/s]\n",
      "加载test集文件: 100%|██████████| 30/30 [00:00<00:00, 3051.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cusanus方法:\n",
      "训练集段落数: 4645\n",
      "测试集段落数: 666\n",
      "训练集样本:\n",
      "Paragraph 1: epistula dominicus debitor plato constantius romanos concludo praemitto paulus vita osdroena ismael vita spiritus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 306/306 [00:00<00:00, 3245.24it/s]\n",
      "加载test集文件: 100%|██████████| 30/30 [00:00<00:00, 3770.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stanza方法:\n",
      "训练集段落数: 4516\n",
      "测试集段落数: 514\n",
      "训练集样本:\n",
      "Paragraph 1: remitto peccatum diligo maria peccatrix peccatum sordus tenebra circumvo sanctus spiritus receptaculum purus caelum gaudia gloriosus triumphus provecto diligo caritas operio multitudo pec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 306/306 [00:00<00:00, 7980.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cltk方法:\n",
      "过滤前段落数: 4516\n",
      "过滤后段落数: 4515\n",
      "保留词汇数: 13348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 308/308 [00:00<00:00, 6515.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cusanus方法:\n",
      "过滤前段落数: 4645\n",
      "过滤后段落数: 4645\n",
      "保留词汇数: 2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 306/306 [00:00<00:00, 7786.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stanza方法:\n",
      "过滤前段落数: 4516\n",
      "过滤后段落数: 4515\n",
      "保留词汇数: 6977\n"
     ]
    }
   ],
   "source": [
    "# 在主循环开始前添加测试代码\n",
    "for method in PARAMETERS.keys():\n",
    "    train_paragraphs, _ = load_all_paragraphs(method, 'train')\n",
    "    test_paragraphs, _ = load_all_paragraphs(method, 'test')\n",
    "    print(f\"\\n{method}方法:\")\n",
    "    print(f\"训练集段落数: {len(train_paragraphs)}\")\n",
    "    print(f\"测试集段落数: {len(test_paragraphs)}\")\n",
    "    print(f\"训练集样本:\\n{train_paragraphs[0][:200]}\")\n",
    "    \n",
    "# 对每个方法测试TF-IDF过滤\n",
    "for method in PARAMETERS.keys():\n",
    "    train_paragraphs, _ = load_all_paragraphs(method, 'train')\n",
    "    filtered_paras, kept_words = filter_paragraphs_with_tfidf(\n",
    "        train_paragraphs,\n",
    "        PARAMETERS[method]['tfidf_threshold']\n",
    "    )\n",
    "    print(f\"\\n{method}方法:\")\n",
    "    print(f\"过滤前段落数: {len(train_paragraphs)}\")\n",
    "    print(f\"过滤后段落数: {len(filtered_paras)}\")\n",
    "    print(f\"保留词汇数: {len(kept_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理 cltk 方法...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 306/306 [00:00<00:00, 3331.62it/s]\n",
      "加载test集文件: 100%|██████████| 30/30 [00:00<00:00, 3800.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理 cusanus 方法...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 308/308 [00:00<00:00, 4526.68it/s]\n",
      "加载test集文件: 100%|██████████| 30/30 [00:00<00:00, 2484.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理 stanza 方法...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载train集文件: 100%|██████████| 306/306 [00:00<00:00, 2887.15it/s]\n",
      "加载test集文件: 100%|██████████| 30/30 [00:00<00:00, 3381.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 主实验循环\n",
    "results = []\n",
    "\n",
    "# 1. 加载数据\n",
    "for method in PARAMETERS.keys():\n",
    "    print(f\"\\n处理 {method} 方法...\")\n",
    "    \n",
    "    try:\n",
    "        # 加载训练集和测试集\n",
    "        train_paragraphs, train_info = load_all_paragraphs(method, 'train')\n",
    "        test_paragraphs, test_info = load_all_paragraphs(method, 'test')\n",
    "        \n",
    "        if not train_paragraphs or not test_paragraphs:\n",
    "            logging.error(f\"{method}: 训练集或测试集为空\")\n",
    "            continue\n",
    "            \n",
    "        # 2. TF-IDF过滤\n",
    "        filtered_train_paras, kept_words = filter_paragraphs_with_tfidf(\n",
    "            train_paragraphs,\n",
    "            PARAMETERS[method]['tfidf_threshold']\n",
    "        )\n",
    "        \n",
    "        if not filtered_train_paras:\n",
    "            logging.error(f\"{method}: TF-IDF过滤后没有剩余段落\")\n",
    "            continue\n",
    "            \n",
    "        # 3. 准备语料库\n",
    "        dictionary, corpus = prepare_corpus(filtered_train_paras)\n",
    "        \n",
    "        # 4. 参数搜索\n",
    "        for num_topics in num_topics_list:\n",
    "            for alpha in alpha_list:\n",
    "                for eta in eta_list:\n",
    "                    try:\n",
    "                        # 5. 训练模型\n",
    "                        model = train_lda_model(corpus, dictionary, num_topics, alpha, eta)\n",
    "                        \n",
    "                        # 6. 评估模型\n",
    "                        npmi, diversity, overlap = evaluate_model(\n",
    "                            model, test_paragraphs, dictionary, num_topics\n",
    "                        )\n",
    "                        \n",
    "                        # 7. 计算综合得分\n",
    "                        optimal_score = compute_optimal_score(\n",
    "                            npmi, diversity, overlap,\n",
    "                            PARAMETERS[method]['alpha_weight'],\n",
    "                            PARAMETERS[method]['lambda_weight']\n",
    "                        )\n",
    "                        \n",
    "                        # 8. 保存结果\n",
    "                        result = {\n",
    "                            'method': method,\n",
    "                            'num_topics': num_topics,\n",
    "                            'alpha': str(alpha),\n",
    "                            'eta': str(eta),\n",
    "                            'npmi': round(npmi, 4),\n",
    "                            'diversity': round(diversity, 4),\n",
    "                            'overlap': round(overlap, 4),\n",
    "                            'optimal_score': round(optimal_score, 4)\n",
    "                        }\n",
    "                        results.append(result)\n",
    "                        \n",
    "                        # 定期保存中间结果\n",
    "                        if len(results) % 5 == 0:\n",
    "                            pd.DataFrame(results).to_excel(\n",
    "                                'results/parameter_search/temp_results.xlsx',\n",
    "                                index=False\n",
    "                            )\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Error with {method}, topics={num_topics}, alpha={alpha}, eta={eta}: {str(e)}\"\n",
    "                        print(error_msg)\n",
    "                        logging.error(error_msg)\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing method {method}: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        logging.error(error_msg)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "各方法的最佳参数组合：\n",
      "\n",
      "cltk方法:\n",
      "主题数量: 10\n",
      "alpha: symmetric\n",
      "eta: symmetric\n",
      "NPMI: inf\n",
      "Diversity: 0.7100\n",
      "Overlap: 0.1156\n",
      "Optimal Score: inf\n",
      "\n",
      "cusanus方法:\n",
      "主题数量: 10\n",
      "alpha: 0.7\n",
      "eta: 0.7\n",
      "NPMI: inf\n",
      "Diversity: 0.7500\n",
      "Overlap: 0.1222\n",
      "Optimal Score: inf\n",
      "\n",
      "stanza方法:\n",
      "主题数量: 10\n",
      "alpha: 0.1\n",
      "eta: 0.1\n",
      "NPMI: inf\n",
      "Diversity: 0.7300\n",
      "Overlap: 0.0978\n",
      "Optimal Score: inf\n"
     ]
    }
   ],
   "source": [
    "# 保存最终结果\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('optimal_score', ascending=False)\n",
    "results_df.to_excel('results/parameter_search/all_results.xlsx', index=False)\n",
    "\n",
    "# 分析最佳参数\n",
    "best_params = results_df.loc[results_df.groupby('method')['optimal_score'].idxmax()]\n",
    "best_params.to_excel('results/parameter_search/best_parameters.xlsx', index=False)\n",
    "\n",
    "print(\"\\n各方法的最佳参数组合：\")\n",
    "for _, row in best_params.iterrows():\n",
    "    print(f\"\\n{row['method']}方法:\")\n",
    "    print(f\"主题数量: {row['num_topics']}\")\n",
    "    print(f\"alpha: {row['alpha']}\")\n",
    "    print(f\"eta: {row['eta']}\")\n",
    "    print(f\"NPMI: {row['npmi']:.4f}\")\n",
    "    print(f\"Diversity: {row['diversity']:.4f}\")\n",
    "    print(f\"Overlap: {row['overlap']:.4f}\")\n",
    "    print(f\"Optimal Score: {row['optimal_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cusanus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
