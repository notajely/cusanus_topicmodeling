{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from bs4 import BeautifulSoup\n",
                "from tqdm import tqdm\n",
                "import csv\n",
                "from collections import Counter\n",
                "import requests\n",
                "import json\n",
                "import stanza\n",
                "import re"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set working directory to project root\n",
                "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
                "os.chdir(project_root)\n",
                "print(\"Current working directory: \", os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_dir = 'data/h_variant'\n",
                "output_dir = 'data/processed/h_variant'\n",
                "statistics_dir = 'data/statistics'\n",
                "lemmatized_output_path = 'data/processed/h_variant/h_lemmatized.json'\n",
                "pos_statistics_path = 'data/statistics/h_statistics.csv'\n",
                "word_freq_statistics_path = 'data/statistics/h_word_frequency.csv'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(output_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Latin stopwords from URL\n",
                "stopwords_url = 'https://raw.githubusercontent.com/aurelberra/stopwords/master/stopwords_latin.txt'\n",
                "response = requests.get(stopwords_url)\n",
                "response.encoding = 'utf-8'\n",
                "latin_stopwords = set(line.strip() for line in response.text.splitlines() if line.strip())\n",
                "\n",
                "# Add additional stopwords\n",
                "additional_stopwords = {\n",
                "    'ego', 'mei', 'mihi', 'me', 'tu', 'tui', 'tibi', 'te',\n",
                "    'nos', 'noster', 'nobis', 'vos', 'vester',\n",
                "    'sui', 'sibi', 'se',\n",
                "    'ab', 'ex', 'ad', 'in', 'de', 'per', 'cum', 'sub', 'pro',\n",
                "    'ante', 'post', 'supra', 'et', 'ac', 'aut', 'nec', 'sed',\n",
                "    'ut', 'si', 'atque', 'qui', 'quae', 'quod', 'quis', 'quid', 'non', 'ne'\n",
                "}\n",
                "latin_stopwords.update(additional_stopwords)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Stanza NLP model for Latin\n",
                "stanza.download('la')  # Download Latin model\n",
                "nlp = stanza.Pipeline('la')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "lemma_mapping_path = os.path.join(project_root, 'data/external/lemma.xml')\n",
                "lemma_mapping = {}\n",
                "with open(lemma_mapping_path, 'r', encoding='utf-8') as lemma_file:\n",
                "    lemma_soup = BeautifulSoup(lemma_file, 'lxml')\n",
                "    for lemma_entry in lemma_soup.find_all('lemma'):\n",
                "        lemma_id = lemma_entry.get('id_lemma')\n",
                "        lemma_value = lemma_entry.get('name').split(',')[0].strip().lower()  # 确保只获取词根\n",
                "        lemma_value = re.sub(r'[\\[\\(].*?[\\]\\)]', '', lemma_value)  # 去除方括号和括号中的内容\n",
                "        if lemma_id and lemma_value:\n",
                "            lemma_mapping[lemma_id] = lemma_value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess text function\n",
                "# Filters stopwords from the word list\n",
                "def preprocess_text(words):\n",
                "    filtered_words = [word for word in words if word.lower() not in latin_stopwords]\n",
                "    return filtered_words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_file(file_path):\n",
                "    with open(file_path, 'r', encoding='utf-8') as file:\n",
                "        soup = BeautifulSoup(file, 'lxml')\n",
                "        paragraphs = []\n",
                "\n",
                "        # 遍历所有的 `fw` 标签，以便对文档进行分段\n",
                "        for fw_tag in soup.find_all('fw', {'type': 'n'}):\n",
                "            section_content = []\n",
                "            next_sibling = fw_tag.find_next_sibling()\n",
                "\n",
                "            # 循环直到找到下一个 `fw` 标签\n",
                "            while next_sibling and next_sibling.name != 'fw':\n",
                "                if next_sibling.name == 'p':  # 处理段落\n",
                "                    words = []\n",
                "\n",
                "                    # 处理每个 `w` 标签中的单词\n",
                "                    for w in next_sibling.find_all('w'):\n",
                "                        # 获取原始词或 `rend` 属性中的值\n",
                "                        original_word = w.get('rend', w.get_text()).lower()\n",
                "\n",
                "                        # 跳过包含德文字母的单词\n",
                "                        if re.search(r'[äöüß]', original_word):\n",
                "                            continue\n",
                "\n",
                "                        # 跳过包含特殊符号或数字的 `cum` 变体\n",
                "                        if re.match(r'^cum\\W*\\d*$', original_word):\n",
                "                            continue\n",
                "\n",
                "                        # 词形还原：使用 `lemma_l` 和映射表\n",
                "                        lemma_l = w.get('lemma_l', '').lower()\n",
                "\n",
                "                        if lemma_l in lemma_mapping:\n",
                "                            # 如果 `lemma_l` 存在于映射表中，使用映射后的词根\n",
                "                            lemma = lemma_mapping[lemma_l]\n",
                "                        else:\n",
                "                            # 如果找不到映射，使用原始词\n",
                "                            lemma = original_word\n",
                "\n",
                "                        # 添加词形到单词列表\n",
                "                        words.append(lemma)\n",
                "\n",
                "                    # 去除停用词并将段落添加到 section_content\n",
                "                    filtered_words = preprocess_text(words)\n",
                "                    section_content.append(' '.join(filtered_words))\n",
                "\n",
                "                next_sibling = next_sibling.find_next_sibling()\n",
                "\n",
                "            # 将段落合并并添加到 paragraphs 列表\n",
                "            paragraphs.append({'content': ' '.join(section_content), 'topic': None})\n",
                "\n",
                "        # 返回处理后的段落\n",
                "        return paragraphs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save sections to JSON format\n",
                "def save_to_json(output_path, data):\n",
                "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
                "        json.dump(data, json_file, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "processing_summary = {\n",
                "    'total_files': 0,\n",
                "    'failed_files': 0\n",
                "}\n",
                "all_documents = []\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 去除停用词和词形还原部分\n",
                "for file_name in tqdm(os.listdir(input_dir), desc=\"处理文件\"):\n",
                "    if file_name.endswith('.xml') and file_name.startswith('h'):\n",
                "        input_path = os.path.join(input_dir, file_name)\n",
                "        processing_summary['total_files'] += 1\n",
                "\n",
                "        try:\n",
                "            # 调用处理函数\n",
                "            paragraphs = process_file(input_path)\n",
                "            \n",
                "            # 准备保存的文档信息\n",
                "            document_data = {\n",
                "                \"document_id\": file_name,\n",
                "                \"paragraphs\": paragraphs\n",
                "            }\n",
                "            all_documents.append(document_data)\n",
                "\n",
                "            # 保存处理后的文本到 .txt 文件\n",
                "            output_file_name = file_name.replace('.xml', '_processed.txt')\n",
                "            output_path = os.path.join(output_dir, output_file_name)\n",
                "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
                "                for idx, paragraph in enumerate(paragraphs, start=1):\n",
                "                    output_file.write(f\"Paragraph {idx}:\\n\")  # 添加换行符 \\n\n",
                "                    output_file.write(f\"{paragraph['content']}\\n\\n\")\n",
                "                    \n",
                "        except Exception as e:\n",
                "            print(f\"处理文件 {file_name} 时发生错误: {str(e)}\")\n",
                "            processing_summary['failed_files'] += 1\n",
                "\n",
                "# 保存所有文档的预处理结果到 JSON\n",
                "save_to_json(lemmatized_output_path, {\n",
                "    \"processing_summary\": processing_summary,\n",
                "    \"documents\": all_documents\n",
                "})\n",
                "\n",
                "print(\"去除停用词和词形还原步骤完成，已保存结果到 data/processed/h_variant/lemmatized_data.json\")\n",
                "\n",
                "# 提示用户检查处理结果\n",
                "input(\"请检查处理后的文件是否符合要求，按下 Enter 键继续进行 POS 标注和统计工作...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform POS tagging using Stanza\n",
                "def pos_tag_text(text):\n",
                "    doc = nlp(text)\n",
                "    pos_tags = [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n",
                "    return pos_tags"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Calculate statistics and save to CSV\n",
                "def calculate_statistics(paragraphs):\n",
                "    total_words = 0\n",
                "    word_freq = {}\n",
                "    pos_freq = {}\n",
                "    unique_words = set()\n",
                "\n",
                "    for paragraph in tqdm(paragraphs, desc=\"POS标注和统计词频进度\"):  # Adding progress bar for POS tagging and frequency calculation\n",
                "        words = paragraph['content'].split()\n",
                "        total_words += len(words)\n",
                "        unique_words.update(words)\n",
                "        for word in words:\n",
                "            word_freq[word] = word_freq.get(word, 0) + 1\n",
                "\n",
                "        # POS tagging for the paragraph\n",
                "        pos_tags = pos_tag_text(paragraph['content'])\n",
                "        for _, pos in pos_tags:\n",
                "            pos_freq[pos] = pos_freq.get(pos, 0) + 1\n",
                "    \n",
                "    # Calculate total types (unique words)\n",
                "    total_types = len(unique_words)\n",
                "\n",
                "    return total_words, total_types, word_freq, pos_freq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# POS 标注和统计步骤\n",
                "all_word_freq = {}\n",
                "all_pos_freq = {}\n",
                "total_words_all = 0\n",
                "total_types_all = set()\n",
                "\n",
                "for document in tqdm(all_documents, desc=\"统计文档进度\"):\n",
                "    total_paragraphs = len(document['paragraphs'])\n",
                "    total_words, total_types, word_freq, pos_freq = calculate_statistics(document['paragraphs'])\n",
                "    total_words_all += total_words\n",
                "    total_types_all.update(word_freq.keys())\n",
                "\n",
                "    # 合并词频和POS统计信息\n",
                "    for word, freq in word_freq.items():\n",
                "        all_word_freq[word] = all_word_freq.get(word, 0) + freq\n",
                "    for pos, freq in pos_freq.items():\n",
                "        all_pos_freq[pos] = all_pos_freq.get(pos, 0) + freq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(pos_statistics_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
                "    writer = csv.writer(csv_file)\n",
                "    writer.writerow([\n",
                "        'Document ID', \n",
                "        'Total Paragraphs', \n",
                "        'Total Words', \n",
                "        'Total Types',\n",
                "        'POS Frequencies'\n",
                "    ])\n",
                "    \n",
                "    # 写入整个数据库的统计信息\n",
                "    writer.writerow([\n",
                "        'ALL DOCUMENTS',\n",
                "        '',  # No specific paragraph count for all documents\n",
                "        total_words_all,\n",
                "        len(total_types_all),\n",
                "        json.dumps(all_pos_freq)  # 整个数据库的POS统计信息\n",
                "        ])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 保存每个词的词频和每个type的数量到单独的CSV文件\n",
                "with open(word_freq_statistics_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
                "    writer = csv.writer(csv_file)\n",
                "    writer.writerow(['Word', 'Frequency'])\n",
                "    for word, freq in all_word_freq.items():\n",
                "        writer.writerow([word, freq])\n",
                "    writer.writerow([])  # 空行分隔\n",
                "    writer.writerow(['POS Tag', 'Frequency'])\n",
                "    for pos, freq in all_pos_freq.items():\n",
                "        writer.writerow([pos, freq])\n",
                "\n",
                "print(\"POS 标注和统计步骤完成，结果已保存到统计文件中。\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
