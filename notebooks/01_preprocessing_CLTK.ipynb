{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import json\n",
                "import logging\n",
                "import pandas as pd\n",
                "from bs4 import BeautifulSoup\n",
                "from collections import Counter\n",
                "from tqdm import tqdm\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 更新导入语句\n",
                "from cltk import NLP\n",
                "from cltk.core.data_types import Doc, Word\n",
                "from cltk.lemmatize.processes import LemmatizationProcess  # 替换 LatinLemmatizer\n",
                "from cltk.tag.pos import POSTag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‎𐤀 CLTK version '1.3.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
                        "\n",
                        "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinSpacyProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinLexiconProcess`.\n",
                        "\n",
                        "⸖ ``LatinSpacyProcess`` using LatinCy model by Patrick Burns from https://huggingface.co/latincy . Please cite: https://arxiv.org/abs/2305.04365\n",
                        "⸖ ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
                        "⸖ ``LatinLexiconProcess`` using Lewis's *An Elementary Latin Dictionary* (1890).\n",
                        "\n",
                        "⸎ To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[cltk.alphabet.processes.LatinNormalizeProcess,\n",
                            " cltk.dependency.processes.LatinSpacyProcess,\n",
                            " cltk.embeddings.processes.LatinEmbeddingsProcess,\n",
                            " cltk.stops.processes.StopsProcess,\n",
                            " cltk.lexicon.processes.LatinLexiconProcess]"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from cltk import NLP\n",
                "nlp = NLP('lat')\n",
                "nlp.pipeline.processes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Word(index_char_start=0, index_char_stop=4, index_token=1, index_sentence=0, string='deus', pos=noun, lemma='deus', stem=None, scansion=None, xpos='noun', upos='NOUN', dependency_relation='ROOT', governor=1, features={Case: [nominative], Gender: [masculine], Number: [singular]}, category={F: [neg], N: [pos], V: [neg]}, stop=False, named_entity=None, syllables=None, phonetic_transcription=None, definition=\"deus\\n\\n ī (nom plur. dī, diī,\\n            rarely deī; gen. deōrum or deūm, poet. also\\n            divōm or divūm; dat. dīs, diīs, and\\n            later deīs), m\\n\\nDIV-, \\na god, deity\\n: deorum inmortalium numen: consilio deorum,\\n                Cs.— In ejaculations: di! T.: di\\n                boni!\\nT.: di inmortales!\\nT.: Pro di inmortales!\\nT.: per deos inmortalīs!: di magni!\\nO.: di vostram fidem!\\nT.: pro deūm fidem!\\nT.: Pro deūm atque hominum fidem!\\nT.: pro deūm inmortalium!\\nT.—In wishes, greetings, and asseverations: di bene vortant,\\n                T.: utinam ita di faxint, T.:\\n                quod di prohibeant, T.: quod di omen avertant, \\nthe gods forbid\\n: di melius duint, T.: Di meliora piis, V.: di meliora\\n                velint, O.: di meliora!\\n\\ngod forbid!\\n: di melius, O.: Di\\n                tibi omnia optata offerant, T.: Ut illum di\\n                deaeque perdant, T.: Di tibi male\\n            faciant, T.: Ita me di ament,\\n            T.: cum dis volentibus, \\nby the gods help\\n: dis volentibus, S.: si dis placet, \\nan't please the gods\\n, T.: di hominesque, i. e. \\nall the world\\n: dis hominibusque invitis, \\nin spite of everybody.—The divine power\\n: deum ire per omnīs Terras (dicunt),\\n                V.: Incaluit deo, O.—\\nA goddess\\n (poet.): ducente deo (sc. Venere), V.: Audentīs deus ipse iuvat (sc. Fortuna),\\n            O.—Of persons, \\na god, divine being\\n: te in dicendo semper putavi deum: Plato quasi deus\\n                philosphorum: deus ille magister, V.: deos\\n                quoniam propius contingis, \\nthe powers that be\\n, H.: deus sum, si hoc ita est, \\nmy fortune is divine\\n, T.\")]\n"
                    ]
                }
            ],
            "source": [
                "doc = nlp.analyze(text=\"deus\")\n",
                "print(doc.words)  # 验证是否返回正确的词性和词形信息\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-11-24 21:47:21,824 - INFO - 成功加载CLTK拉丁语模型\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Current working directory:  /Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\n",
                        "‎𐤀 CLTK version '1.3.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
                        "\n",
                        "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinSpacyProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinLexiconProcess`.\n",
                        "\n",
                        "⸖ ``LatinSpacyProcess`` using LatinCy model by Patrick Burns from https://huggingface.co/latincy . Please cite: https://arxiv.org/abs/2305.04365\n",
                        "⸖ ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
                        "⸖ ``LatinLexiconProcess`` using Lewis's *An Elementary Latin Dictionary* (1890).\n",
                        "\n",
                        "⸎ To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n"
                    ]
                }
            ],
            "source": [
                "# 设置工作目录\n",
                "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
                "os.chdir(project_root)\n",
                "print(\"Current working directory: \", os.getcwd())\n",
                "\n",
                "# 设置路径\n",
                "base_dir = \"/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\"\n",
                "input_dir = os.path.join(base_dir, \"data/v_variant\")  # XML文件所在目录\n",
                "output_dir = os.path.join(base_dir, \"experiments/lda/cltk/preprocessed\")  # 预处理后的文本\n",
                "stats_dir = os.path.join(base_dir, \"experiments/lda/cltk/preprocessed/sta\")  # 统计信息\n",
                "\n",
                "# 创建必要的目录\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "os.makedirs(stats_dir, exist_ok=True)\n",
                "\n",
                "# 配置日志\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
                "    handlers=[\n",
                "        logging.FileHandler(\"logs/preprocessing.log\"),\n",
                "        logging.StreamHandler()\n",
                "    ]\n",
                ")\n",
                "\n",
                "# 替换spacy加载为CLTK加载\n",
                "try:\n",
                "    nlp = NLP(language=\"lat\")\n",
                "    logging.info(\"成功加载CLTK拉丁语模型\")\n",
                "except:\n",
                "    logging.error(\"无法加载CLTK拉丁语模型，请确保已安装\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 加载停用词\n",
                "with open('data/external/stopwords_latin.txt', 'r', encoding='utf-8') as f:\n",
                "    LATIN_STOPS = set(f.read().splitlines())\n",
                "logging.info(f\"成功加载 {len(LATIN_STOPS)} 个自定义停用词\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-11-24 21:47:57,679 - INFO - 开始验证CLTK输出...\n",
                        "2024-11-24 21:47:57,850 - INFO - 测试词 'deus': POS=noun, Lemma=deus\n",
                        "2024-11-24 21:47:57,921 - INFO - 测试词 'est': POS=auxiliary, Lemma=sum\n",
                        "2024-11-24 21:47:57,981 - INFO - 测试词 'bonus': POS=adjective, Lemma=bonus\n",
                        "2024-11-24 21:47:58,130 - INFO - 测试词 'nomen': POS=noun, Lemma=nomen\n",
                        "2024-11-24 21:47:58,192 - INFO - 测试词 'filii': POS=noun, Lemma=filius\n",
                        "2024-11-24 21:47:58,195 - INFO - 找到 306 个XML文件\n",
                        "处理文件:   0%|                                                             | 0/306 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "处理文件:   0%|                            | 1/306 [01:02<5:18:38, 62.68s/it, 当前文件=v170_048.xml]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "处理文件:   1%|▏                           | 2/306 [01:31<3:37:46, 42.98s/it, 当前文件=v170_060.xml]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "处理文件:   1%|▏                           | 2/306 [01:39<4:11:07, 49.56s/it, 当前文件=v170_060.xml]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m     processor\u001b[38;5;241m.\u001b[39msave_statistics()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[8], line 188\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, xml_file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     paragraphs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# 减少日志输出频率\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paragraphs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
                        "Cell \u001b[0;32mIn[8], line 97\u001b[0m, in \u001b[0;36mLatinPreprocessor.process_file\u001b[0;34m(self, file_path, output_path)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m next_sibling\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     96\u001b[0m     original_word \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrend\u001b[39m\u001b[38;5;124m'\u001b[39m, w\u001b[38;5;241m.\u001b[39mget_text())\n\u001b[0;32m---> 97\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m processed:\n\u001b[1;32m     99\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(processed)\n",
                        "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mLatinPreprocessor.process_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 使用CLTK进行处理\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mwords:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/nlp.py:170\u001b[0m, in \u001b[0;36mNLP.analyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mprocesses:\n\u001b[1;32m    169\u001b[0m     a_process: Process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_process_object(process_object\u001b[38;5;241m=\u001b[39mprocess)\n\u001b[0;32m--> 170\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43ma_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/processes.py:47\u001b[0m, in \u001b[0;36mLexiconProcess.run\u001b[0;34m(self, input_doc)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlemma:\n\u001b[0;32m---> 47\u001b[0m         word\u001b[38;5;241m.\u001b[39mdefinition \u001b[38;5;241m=\u001b[39m \u001b[43mlookup_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstring:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/lat.py:78\u001b[0m, in \u001b[0;36mLatinLewisLexicon.lookup\u001b[0;34m(self, lemma)\u001b[0m\n\u001b[1;32m     75\u001b[0m lemma \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mescape(lemma\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     77\u001b[0m keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 78\u001b[0m matches: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m     key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m regex\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[0-9]?$\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     81\u001b[0m n_matches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_matches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/lat.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m lemma \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mescape(lemma\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     77\u001b[0m keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     78\u001b[0m matches: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 79\u001b[0m     key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mregex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlemma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m[0-9]?$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     81\u001b[0m n_matches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_matches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/regex/regex.py:253\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags, pos, endpos, partial, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    250\u001b[0m   concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning a match\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     pat \u001b[38;5;241m=\u001b[39m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39mmatch(string, pos, endpos, concurrent, partial, timeout)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "class LatinPreprocessor:\n",
                "    def __init__(self):\n",
                "        self.nlp = nlp\n",
                "        \n",
                "        # 添加计数器初始化\n",
                "        self.word_freq = Counter()\n",
                "        self.word_pos = {}\n",
                "        self.pos_stats = Counter()\n",
                "        self.total_words = 0\n",
                "        self.total_paragraphs = 0\n",
                "        self.stopwords = LATIN_STOPS  # 确保这个变量已定义\n",
                "        \n",
                "        # 更新CLTK的词性映射 - 使用Universal POS Tags\n",
                "        self.VALID_POS = {'NOUN', 'ADJ', 'VERB', 'PROPN'}  # 扩展有效词性集合\n",
                "        self.pos_mapping = {\n",
                "            'NOUN': 'NOUN',      # 名词\n",
                "            'VERB': 'VERB',      # 动词\n",
                "            'ADJ': 'ADJ',        # 形容词\n",
                "            'PROPN': 'PROPN',    # 专有名词\n",
                "            'ADV': 'ADV',        # 副词\n",
                "            'PRON': 'PRON',      # 代词\n",
                "            'DET': 'DET',        # 限定词\n",
                "            'ADP': 'ADP',        # 介词\n",
                "            'CCONJ': 'CONJ',     # 并列连词\n",
                "            'SCONJ': 'CONJ',     # 从属连词\n",
                "            'NUM': 'NUM',        # 数词\n",
                "            'PART': 'PART',      # 小品词\n",
                "            'INTJ': 'INTJ',      # 感叹词\n",
                "            'X': 'X'             # 其他\n",
                "        }\n",
                "        \n",
                "    def process_word(self, word):\n",
                "        \"\"\"处理单个词\"\"\"\n",
                "        try:\n",
                "            # 清理词形\n",
                "            word = word.lower().strip()\n",
                "            # 去除所有数字\n",
                "            word = re.sub(r'\\d', '', word)  # 修改这里：移除所有数字\n",
                "            # 只保留拉丁字母和长音符号\n",
                "            word = re.sub(r'[^a-zāēīōūȳĀĒĪŌŪȲ]', '', word)\n",
                "            \n",
                "            if not word or word in self.stopwords:\n",
                "                return None\n",
                "                \n",
                "            # 使用CLTK进行处理\n",
                "            doc = self.nlp.analyze(text=word)\n",
                "            \n",
                "            if not doc.words:\n",
                "                return None\n",
                "                \n",
                "            token = doc.words[0]\n",
                "            # 处理POS标签 - 转换为字符串\n",
                "            pos_tag = str(token.pos) if token.pos else 'UNKNOWN'\n",
                "            pos = self.pos_mapping.get(pos_tag, pos_tag)\n",
                "            \n",
                "            # 更新统计信息\n",
                "            self.pos_stats[pos] += 1\n",
                "            \n",
                "            if pos not in self.VALID_POS:\n",
                "                return None\n",
                "                \n",
                "            # 使用CLTK的lemmatizer\n",
                "            lemma = token.lemma if token.lemma else word\n",
                "            if lemma:\n",
                "                # 确保lemma也经过同样的清理\n",
                "                lemma = re.sub(r'\\d', '', lemma.lower())\n",
                "                lemma = re.sub(r'[^a-zāēīōūȳĀĒĪŌŪȲ]', '', lemma)\n",
                "                \n",
                "                if lemma:  # 确保清理后的lemma不为空\n",
                "                    self.word_freq[lemma] += 1\n",
                "                    self.word_pos[lemma] = pos\n",
                "                    self.total_words += 1\n",
                "                    return lemma\n",
                "                \n",
                "        except Exception as e:\n",
                "            logging.warning(f\"处理词 '{word}' 时出错: {str(e)}\")\n",
                "            return None\n",
                "            \n",
                "        return None\n",
                "\n",
                "    def process_file(self, file_path, output_path):\n",
                "        \"\"\"处理单个文件\"\"\"\n",
                "        with open(file_path, 'r', encoding='utf-8') as file:\n",
                "            # 使用xml解析器\n",
                "            soup = BeautifulSoup(file, 'xml')\n",
                "            paragraphs = []\n",
                "            \n",
                "            for fw_tag in soup.find_all('fw', {'type': 'n'}):\n",
                "                section_content = []\n",
                "                next_sibling = fw_tag.find_next_sibling()\n",
                "                \n",
                "                while next_sibling and next_sibling.name != 'fw':\n",
                "                    if next_sibling.name == 'p':\n",
                "                        words = []\n",
                "                        for w in next_sibling.find_all('w'):\n",
                "                            original_word = w.get('rend', w.get_text())\n",
                "                            processed = self.process_word(original_word)\n",
                "                            if processed:\n",
                "                                words.append(processed)\n",
                "                        \n",
                "                        if words:\n",
                "                            section_content.append(' '.join(words))\n",
                "                    \n",
                "                    next_sibling = next_sibling.find_next_sibling()\n",
                "                \n",
                "                if section_content:\n",
                "                    paragraphs.append({'content': ' '.join(section_content)})\n",
                "                    self.total_paragraphs += 1\n",
                "            \n",
                "            # 保存处理后的文本\n",
                "            with open(output_path, 'w', encoding='utf-8') as f:\n",
                "                for idx, paragraph in enumerate(paragraphs, 1):\n",
                "                    if paragraph['content'].strip():\n",
                "                        f.write(f\"Paragraph {idx}:\\n\")\n",
                "                        f.write(f\"{paragraph['content']}\\n\\n\")\n",
                "            \n",
                "            return paragraphs\n",
                "\n",
                "    def save_statistics(self):\n",
                "        \"\"\"保存统计信息\"\"\"\n",
                "        stats = {\n",
                "            'total_words': self.total_words,\n",
                "            'unique_words': len(self.word_freq),\n",
                "            'total_paragraphs': self.total_paragraphs,\n",
                "            'pos_distribution': dict(self.pos_stats)\n",
                "        }\n",
                "        \n",
                "        # 保存基本统计信息\n",
                "        stats_file = os.path.join(stats_dir, 'preprocessing_stats.json')\n",
                "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
                "            json.dump(stats, f, ensure_ascii=False, indent=2)\n",
                "        \n",
                "        # 保存词频统计\n",
                "        word_freq_df = pd.DataFrame([\n",
                "            {'word': word, 'frequency': freq, 'pos': self.word_pos.get(word, 'UNKNOWN')}\n",
                "            for word, freq in self.word_freq.most_common()\n",
                "        ])\n",
                "        word_freq_file = os.path.join(stats_dir, 'word_frequencies.csv')\n",
                "        word_freq_df.to_csv(word_freq_file, index=False, encoding='utf-8')\n",
                "        \n",
                "        logging.info(f\"统计信息已保存到 {stats_dir}\")\n",
                "        logging.info(f\"总词数: {self.total_words}\")\n",
                "        logging.info(f\"独特词数: {len(self.word_freq)}\")\n",
                "        logging.info(f\"总段落数: {self.total_paragraphs}\")\n",
                "        logging.info(\"词性分布:\")\n",
                "        for pos, count in self.pos_stats.most_common():\n",
                "            logging.info(f\"  {pos}: {count}\")\n",
                "\n",
                "    def validate_cltk_output(self):\n",
                "        \"\"\"验证CLTK输出的词性标签\"\"\"\n",
                "        test_words = [\"deus\", \"est\", \"bonus\", \"nomen\", \"filii\"]\n",
                "        for word in test_words:\n",
                "            try:\n",
                "                doc = self.nlp.analyze(text=word)\n",
                "                if doc.words:\n",
                "                    token = doc.words[0]\n",
                "                    pos_tag = str(token.pos) if token.pos else 'UNKNOWN'\n",
                "                    lemma = token.lemma if token.lemma else word\n",
                "                    logging.info(f\"测试词 '{word}': POS={pos_tag}, Lemma={lemma}\")\n",
                "            except Exception as e:\n",
                "                logging.error(f\"测试词 '{word}' 处理失败: {str(e)}\")\n",
                "\n",
                "def main():\n",
                "    # 确保已安装 lxml\n",
                "    try:\n",
                "        import lxml\n",
                "    except ImportError:\n",
                "        logging.error(\"请安装 lxml: pip install lxml\")\n",
                "        return\n",
                "\n",
                "    processor = LatinPreprocessor()\n",
                "    \n",
                "    # 首先运行验证\n",
                "    logging.info(\"开始验证CLTK输出...\")\n",
                "    processor.validate_cltk_output()\n",
                "    \n",
                "    # 获取所有XML文件\n",
                "    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n",
                "    logging.info(f\"找到 {len(xml_files)} 个XML文件\")\n",
                "    \n",
                "    # 使用 tqdm 显示进度，但减少输出频率\n",
                "    with tqdm(xml_files, desc=\"处理文件\", ncols=100, mininterval=1.0) as pbar:\n",
                "        for xml_file in pbar:\n",
                "            input_file = os.path.join(input_dir, xml_file)\n",
                "            output_file = os.path.join(output_dir, xml_file.replace('.xml', '.txt'))\n",
                "            \n",
                "            try:\n",
                "                paragraphs = processor.process_file(input_file, output_file)\n",
                "                # 减少日志输出频率\n",
                "                if len(paragraphs) > 0:\n",
                "                    logging.debug(f\"处理文件 {xml_file}: 提取了 {len(paragraphs)} 个段落\")\n",
                "            except Exception as e:\n",
                "                logging.error(f\"处理文件 {xml_file} 时出错: {str(e)}\")\n",
                "                continue\n",
                "            \n",
                "            # 更新进度条描述\n",
                "            pbar.set_postfix({'当前文件': xml_file})\n",
                "    \n",
                "    # 保存统计信息\n",
                "    processor.save_statistics()\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# 读取词频数据\n",
                "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
                "\n",
                "# 设置风格\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "# 创建图形\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
                "\n",
                "# 左图：词频分布（使用区间分组）\n",
                "freq_ranges = [(0, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
                "freq_counts = []\n",
                "for start, end in freq_ranges:\n",
                "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "    freq_counts.append(count)\n",
                "\n",
                "ax1.bar(range(len(freq_ranges)), freq_counts, \n",
                "        width=0.6,\n",
                "        color='skyblue')\n",
                "ax1.set_xticks(range(len(freq_ranges)))\n",
                "ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
                "                     for r in freq_ranges], rotation=45)\n",
                "ax1.set_title('词频区间分布', fontsize=14, pad=20)\n",
                "ax1.set_xlabel('词频区间', fontsize=12)\n",
                "ax1.set_ylabel('词数', fontsize=12)\n",
                "\n",
                "# 在柱子上添加具体数值\n",
                "for i, count in enumerate(freq_counts):\n",
                "    ax1.text(i, count, str(count), ha='center', va='bottom')\n",
                "\n",
                "# 右图：累积分布函数（CDF）\n",
                "frequencies = sorted(df['frequency'].values)\n",
                "cumulative = np.arange(1, len(frequencies) + 1) / len(frequencies)\n",
                "\n",
                "ax2.plot(frequencies, cumulative, 'b-', linewidth=2)\n",
                "ax2.set_xscale('log')\n",
                "ax2.set_title('词频累积分布', fontsize=14, pad=20)\n",
                "ax2.set_xlabel('词频 (log尺度)', fontsize=12)\n",
                "ax2.set_ylabel('累积比例', fontsize=12)\n",
                "\n",
                "# 添加关键分位数标记\n",
                "quantiles = [0.25, 0.5, 0.75, 0.9]\n",
                "for q in quantiles:\n",
                "    q_value = np.quantile(frequencies, q)\n",
                "    ax2.axhline(y=q, color='r', linestyle='--', alpha=0.3)\n",
                "    ax2.axvline(x=q_value, color='r', linestyle='--', alpha=0.3)\n",
                "    ax2.text(frequencies[-1], q, f'{int(q*100)}%', va='center')\n",
                "    ax2.text(q_value, 0, f'{int(q_value)}', ha='center', va='top', rotation=90)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# 打印详细的统计信息\n",
                "print(\"\\n词频统计信息：\")\n",
                "print(f\"总词汇量：{len(df):,} 个词\")\n",
                "print(f\"最小词频：{df['frequency'].min():,}\")\n",
                "print(f\"最大词频：{df['frequency'].max():,}\")\n",
                "print(f\"平均词频：{df['frequency'].mean():.1f}\")\n",
                "print(f\"中位词频：{df['frequency'].median():.0f}\")\n",
                "\n",
                "print(\"\\n词频分布：\")\n",
                "for start, end in freq_ranges:\n",
                "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "    total = len(df)\n",
                "    if end == float('inf'):\n",
                "        print(f\">{start}次: {count:,}词 ({count/total*100:.1f}%)\")\n",
                "    else:\n",
                "        print(f\"{start}-{end}次: {count:,}词 ({count/total*100:.1f}%)\")\n",
                "\n",
                "print(\"\\n分位数分布：\")\n",
                "for q in [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n",
                "    print(f\"{int(q*100)}%分位数: {df['frequency'].quantile(q):.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# 读取词频数据\n",
                "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
                "\n",
                "# 设置风格\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "# 创建多子图\n",
                "fig = plt.figure(figsize=(20, 15))\n",
                "gs = fig.add_gridspec(2, 2)\n",
                "\n",
                "# 1. 词频区间分布（左上）\n",
                "ax1 = fig.add_subplot(gs[0, 0])\n",
                "freq_ranges = [(0, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
                "freq_counts = []\n",
                "for start, end in freq_ranges:\n",
                "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "    freq_counts.append(count)\n",
                "\n",
                "ax1.bar(range(len(freq_ranges)), freq_counts, width=0.6, color='skyblue')\n",
                "ax1.set_xticks(range(len(freq_ranges)))\n",
                "ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
                "                     for r in freq_ranges], rotation=45)\n",
                "ax1.set_title('词频区间分布', fontsize=14)\n",
                "ax1.set_xlabel('词频区间', fontsize=12)\n",
                "ax1.set_ylabel('词数', fontsize=12)\n",
                "\n",
                "# 添加数值标签\n",
                "for i, count in enumerate(freq_counts):\n",
                "    ax1.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
                "\n",
                "# 2. 词性分布（右上）\n",
                "ax2 = fig.add_subplot(gs[0, 1])\n",
                "pos_counts = df['pos'].value_counts()\n",
                "ax2.bar(range(len(pos_counts)), pos_counts.values, color='lightgreen')\n",
                "ax2.set_xticks(range(len(pos_counts)))\n",
                "ax2.set_xticklabels(pos_counts.index, rotation=45)\n",
                "ax2.set_title('词性分布', fontsize=14)\n",
                "ax2.set_xlabel('词性', fontsize=12)\n",
                "ax2.set_ylabel('词数', fontsize=12)\n",
                "\n",
                "# 添加数值标签\n",
                "for i, count in enumerate(pos_counts.values):\n",
                "    ax2.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
                "\n",
                "# 3. 各词性的词频箱型图（左下）\n",
                "ax3 = fig.add_subplot(gs[1, 0])\n",
                "sns.boxplot(data=df, x='pos', y='frequency', showfliers=False, ax=ax3)\n",
                "ax3.set_yscale('log')\n",
                "ax3.set_title('各词性的词频分布 (不含异常值)', fontsize=14)\n",
                "ax3.set_xlabel('词性', fontsize=12)\n",
                "ax3.set_ylabel('词频 (log尺度)', fontsize=12)\n",
                "ax3.tick_params(axis='x', rotation=45)\n",
                "\n",
                "# 4. 高频词分析（右下）\n",
                "ax4 = fig.add_subplot(gs[1, 1])\n",
                "top_n = 20\n",
                "top_words = df.nlargest(top_n, 'frequency')\n",
                "sns.barplot(data=top_words, x='frequency', y='word', hue='pos', ax=ax4)\n",
                "ax4.set_title(f'前{top_n}个高频词', fontsize=14)\n",
                "ax4.set_xlabel('词频', fontsize=12)\n",
                "ax4.set_ylabel('词', fontsize=12)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# 打印详细统计信息\n",
                "print(\"\\n各词性的词频统计：\")\n",
                "pos_stats = df.groupby('pos').agg({\n",
                "    'frequency': ['count', 'min', 'max', 'mean', 'median']\n",
                "}).round(2)\n",
                "print(pos_stats)\n",
                "\n",
                "print(\"\\n各词性的高频词（前10个）：\")\n",
                "for pos in df['pos'].unique():\n",
                "    top_words = df[df['pos'] == pos].nlargest(10, 'frequency')\n",
                "    print(f\"\\n{pos}:\")\n",
                "    for _, row in top_words.iterrows():\n",
                "        print(f\"{row['word']}: {row['frequency']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "# 设置中文字体和样式\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOS\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "def analyze_filtered_vocabulary(df, min_freq=2, max_freq=1000, output_dir='results/preprocessed'):\n",
                "    \"\"\"\n",
                "    分析词汇过滤前后的统计特征\n",
                "    \n",
                "    参数:\n",
                "    df: DataFrame, 包含词频数据\n",
                "    min_freq: int, 最小词频阈值\n",
                "    max_freq: int, 最大词频阈值\n",
                "    output_dir: str, 输出目录\n",
                "    \"\"\"\n",
                "    \n",
                "    # 创建过滤后的数据框\n",
                "    df_filtered = df[(df['frequency'] >= min_freq) & (df['frequency'] <= max_freq)].copy()\n",
                "    \n",
                "    # 创建图形\n",
                "    fig = plt.figure(figsize=(20, 15))\n",
                "    gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.2])\n",
                "    \n",
                "    # 1. 词频区间分布（左上）\n",
                "    ax1 = fig.add_subplot(gs[0, 0])\n",
                "    freq_ranges = [(2, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
                "    freq_counts_before = []\n",
                "    freq_counts_after = []\n",
                "    \n",
                "    for start, end in freq_ranges:\n",
                "        count_before = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "        count_after = len(df_filtered[(df_filtered['frequency'] >= start) & \n",
                "                                    (df_filtered['frequency'] < end)])\n",
                "        freq_counts_before.append(count_before)\n",
                "        freq_counts_after.append(count_after)\n",
                "    \n",
                "    x = np.arange(len(freq_ranges))\n",
                "    width = 0.35\n",
                "    \n",
                "    ax1.bar(x - width/2, freq_counts_before, width, label='过滤前', color='skyblue')\n",
                "    ax1.bar(x + width/2, freq_counts_after, width, label='过滤后', color='lightcoral')\n",
                "    \n",
                "    ax1.set_xticks(x)\n",
                "    ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
                "                         for r in freq_ranges], rotation=45)\n",
                "    ax1.set_title('词频区间分布对比', fontsize=14, pad=20)\n",
                "    ax1.set_xlabel('词频区间', fontsize=12)\n",
                "    ax1.set_ylabel('词数', fontsize=12)\n",
                "    ax1.legend()\n",
                "    \n",
                "    # 添加数值标签\n",
                "    for i, v in enumerate(freq_counts_before):\n",
                "        ax1.text(i - width/2, v, str(v), ha='center', va='bottom')\n",
                "    for i, v in enumerate(freq_counts_after):\n",
                "        ax1.text(i + width/2, v, str(v), ha='center', va='bottom')\n",
                "    \n",
                "    # 2. 词性分布对比（右上）\n",
                "    ax2 = fig.add_subplot(gs[0, 1])\n",
                "    pos_before = df['pos'].value_counts()\n",
                "    pos_after = df_filtered['pos'].value_counts()\n",
                "    \n",
                "    x = np.arange(len(pos_before))\n",
                "    ax2.bar(x - width/2, pos_before.values, width, label='过滤前', color='skyblue')\n",
                "    ax2.bar(x + width/2, pos_after.values, width, label='过滤后', color='lightcoral')\n",
                "    \n",
                "    ax2.set_xticks(x)\n",
                "    ax2.set_xticklabels(pos_before.index, rotation=45)\n",
                "    ax2.set_title('词性分布对比', fontsize=14, pad=20)\n",
                "    ax2.set_xlabel('词性', fontsize=12)\n",
                "    ax2.set_ylabel('词数', fontsize=12)\n",
                "    ax2.legend()\n",
                "    \n",
                "    # 添加数值标签\n",
                "    for i, v in enumerate(pos_before.values):\n",
                "        ax2.text(i - width/2, v, str(v), ha='center', va='bottom')\n",
                "    for i, v in enumerate(pos_after.values):\n",
                "        ax2.text(i + width/2, v, str(v), ha='center', va='bottom')\n",
                "    \n",
                "    # 3. 被过滤的高频词（左下）\n",
                "    ax3 = fig.add_subplot(gs[1, 0])\n",
                "    high_freq_words = df[df['frequency'] > max_freq].sort_values('frequency', ascending=True).tail(15)\n",
                "    \n",
                "    colors = {'NOUN': 'skyblue', 'VERB': 'lightcoral', 'ADJ': 'lightgreen', 'PROPN': 'orange'}\n",
                "    bars = ax3.barh(range(len(high_freq_words)), high_freq_words['frequency'],\n",
                "                    color=[colors.get(pos, 'gray') for pos in high_freq_words['pos']])\n",
                "    \n",
                "    ax3.set_yticks(range(len(high_freq_words)))\n",
                "    ax3.set_yticklabels(high_freq_words['word'])\n",
                "    ax3.set_title('被过滤的高频词 (Top 15)', fontsize=14, pad=20)\n",
                "    ax3.set_xlabel('词频', fontsize=12)\n",
                "    \n",
                "    # 添加词性标签\n",
                "    for i, bar in enumerate(bars):\n",
                "        ax3.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
                "                f\" {high_freq_words.iloc[i]['pos']}\", \n",
                "                va='center')\n",
                "    \n",
                "    # 4. 过滤后词频分布（右下）\n",
                "    ax4 = fig.add_subplot(gs[1, 1])\n",
                "    sns.boxplot(data=df_filtered, x='pos', y='frequency', ax=ax4)\n",
                "    ax4.set_yscale('log')\n",
                "    ax4.set_title('过滤后各词性的词频分布', fontsize=14, pad=20)\n",
                "    ax4.set_xlabel('词性', fontsize=12)\n",
                "    ax4.set_ylabel('词频 (log尺度)', fontsize=12)\n",
                "    ax4.tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # 打印统计信息\n",
                "    print(\"\\n=== 过滤前后统计 ===\")\n",
                "    print(f\"过滤前总词数：{len(df):,}\")\n",
                "    print(f\"过滤后总词数：{len(df_filtered):,}\")\n",
                "    print(f\"过滤掉的词数：{len(df) - len(df_filtered):,}\")\n",
                "    \n",
                "    print(\"\\n=== 各词性过滤前后对比 ===\")\n",
                "    pos_compare = pd.DataFrame({\n",
                "        '过滤前': pos_before,\n",
                "        '过滤后': pos_after,\n",
                "        '减少比例': (pos_before - pos_after) / pos_before * 100\n",
                "    }).round(2)\n",
                "    print(pos_compare)\n",
                "    \n",
                "    # 保存详细统计信息\n",
                "    stats = {\n",
                "        '过滤前': {\n",
                "            '总词数': len(df),\n",
                "            '总词频': df['frequency'].sum(),\n",
                "            '各词性数量': df['pos'].value_counts().to_dict(),\n",
                "            '词频统计': {\n",
                "                '最小值': df['frequency'].min(),\n",
                "                '最大值': df['frequency'].max(),\n",
                "                '平均值': df['frequency'].mean(),\n",
                "                '中位数': df['frequency'].median(),\n",
                "                '标准差': df['frequency'].std()\n",
                "            }\n",
                "        },\n",
                "        '过滤后': {\n",
                "            '总词数': len(df_filtered),\n",
                "            '总词频': df_filtered['frequency'].sum(),\n",
                "            '各词性数量': df_filtered['pos'].value_counts().to_dict(),\n",
                "            '词频统计': {\n",
                "                '最小值': df_filtered['frequency'].min(),\n",
                "                '最大值': df_filtered['frequency'].max(),\n",
                "                '平均值': df_filtered['frequency'].mean(),\n",
                "                '中位数': df_filtered['frequency'].median(),\n",
                "                '标准差': df_filtered['frequency'].std()\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    # 保存统计结果\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    # 保存过滤后的词典\n",
                "    filtered_df.to_csv(os.path.join(output_dir, 'spacy_filtered_vocabulary.csv'), index=False)\n",
                "    \n",
                "    # 保存统计信息\n",
                "    pd.DataFrame(pos_compare).to_csv(os.path.join(output_dir, 'spacy_pos_statistics.csv'))\n",
                "    \n",
                "    # 保存高频词列表\n",
                "    high_freq_words.to_csv(os.path.join(output_dir, 'spacy_high_frequency_words.csv'), index=False)\n",
                "    \n",
                "    return df_filtered, stats\n",
                "\n",
                "# 读取原始词频数据\n",
                "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
                "\n",
                "# 运行分析\n",
                "filtered_df, stats = analyze_filtered_vocabulary(df, min_freq=2, max_freq=1000)\n",
                "\n",
                "# 打印一些额外的统计信息\n",
                "print(\"\\n=== 高频词统计 ===\")\n",
                "print(\"词频超过1000的词：\")\n",
                "for _, row in df[df['frequency'] > 1000].sort_values('frequency', ascending=False).iterrows():\n",
                "    print(f\"{row['word']} ({row['pos']}): {row['frequency']:,}\")\n",
                "\n",
                "print(\"\\n=== 词性分布统计 ===\")\n",
                "for pos in filtered_df['pos'].unique():\n",
                "    pos_stats = filtered_df[filtered_df['pos'] == pos]['frequency'].describe()\n",
                "    print(f\"\\n{pos}:\")\n",
                "    print(pos_stats.round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}