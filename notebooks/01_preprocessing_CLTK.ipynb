{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... existing imports ...\n",
    "from cltk import NLP\n",
    "from cltk.core.data_types import Doc, Word\n",
    "from cltk.lemmatize.processes import LemmatizationProcess\n",
    "from cltk.tag.pos import POSTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP\n",
    "nlp = NLP('lat')\n",
    "nlp.pipeline.processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置工作目录\n",
    "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
    "os.chdir(project_root)\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "\n",
    "# 设置路径\n",
    "base_dir = \"/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\"\n",
    "input_dir = os.path.join(base_dir, \"data/v_variant\")  # XML文件所在目录\n",
    "output_dir = os.path.join(base_dir, \"experiments/lda/cltk/preprocessed\")  # 预处理后的文本\n",
    "stats_dir = os.path.join(base_dir, \"experiments/lda/cltk/preprocessed/sta\")  # 统计信息\n",
    "\n",
    "# 创建必要的目录\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(stats_dir, exist_ok=True)\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"experiments/lda/cltk/logs/cltk_preprocessing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 NLP 管道\n",
    "try:\n",
    "    nlp = NLP(language=\"lat\", suppress_banner=True)\n",
    "    logging.info(\"成功加载 CLTK 拉丁语模型\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"无法加载 CLTK 拉丁语模型: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open('data/external/stopwords_latin.txt', 'r', encoding='utf-8') as f:\n",
    "    LATIN_STOPS = set(f.read().splitlines())\n",
    "logging.info(f\"成功加载 {len(LATIN_STOPS)} 个自定义停用词\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatinPreprocessor:\n",
    "    \"\"\"负责拉丁语文本的预处理（仅词性标注和词形还原）\"\"\"\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp  # 初始化时传入 CLTK 的 NLP 管道对象\n",
    "        self.word_freq = Counter()  # 词频统计\n",
    "        self.word_pos = {}  # 词性信息\n",
    "        self.pos_stats = Counter()  # 词性分布统计\n",
    "        self.total_words = 0  # 总词数\n",
    "        self.total_paragraphs = 0  # 总段落数\n",
    "        self.stopwords = LATIN_STOPS  # 停用词集合（需要预定义）\n",
    "        self.VALID_POS = {'NOUN', 'ADJ', 'VERB', 'PROPN'}  # 有效词性\n",
    "\n",
    "    def process_word(self, word):\n",
    "        \"\"\"处理单个词\"\"\"\n",
    "        try:\n",
    "            # 清理和预处理单词\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'\\d', '', word)  # 移除数字\n",
    "            word = re.sub(r'[^a-zāēīōūȳĀĒĪŌŪȲ]', '', word)  # 保留拉丁字母及长音符\n",
    "            \n",
    "            if not word or word in self.stopwords:  # 如果为空或为停用词，跳过\n",
    "                return None\n",
    "            \n",
    "            # 调用 CLTK 的 NLP 管道\n",
    "            doc = self.nlp.analyze(text=word)\n",
    "            if not doc.words:  # 如果没有返回词信息\n",
    "                return None\n",
    "            \n",
    "            # 提取第一个词的信息\n",
    "            token = doc.words[0]\n",
    "            pos = str(token.upos) if token.upos else 'UNKNOWN'  # 提取词性\n",
    "            \n",
    "            self.pos_stats[pos] += 1  # 更新词性统计\n",
    "            \n",
    "            if pos not in self.VALID_POS:  # 跳过无效词性\n",
    "                return None\n",
    "            \n",
    "            # 提取词形还原后的词根\n",
    "            lemma = token.lemma if token.lemma else word\n",
    "            lemma = re.sub(r'\\d', '', lemma.lower())  # 清理词根\n",
    "            lemma = re.sub(r'[^a-zāēīōūȳĀĒĪŌŪȲ]', '', lemma)\n",
    "            \n",
    "            if lemma:\n",
    "                self.word_freq[lemma] += 1  # 更新词频统计\n",
    "                self.word_pos[lemma] = pos  # 保存词性信息\n",
    "                self.total_words += 1  # 增加总词数计数\n",
    "                return lemma\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"处理词 '{word}' 时出错: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def process_file(self, file_path):\n",
    "        \"\"\"处理单个文件并返回处理后的段落\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'xml')\n",
    "            paragraphs = []\n",
    "            \n",
    "            for fw_tag in soup.find_all('fw', {'type': 'n'}):\n",
    "                section_content = []\n",
    "                next_sibling = fw_tag.find_next_sibling()\n",
    "                \n",
    "                while next_sibling and next_sibling.name != 'fw':\n",
    "                    if next_sibling.name == 'p':\n",
    "                        words = []\n",
    "                        for w in next_sibling.find_all('w'):\n",
    "                            original_word = w.get('rend', w.get_text())\n",
    "                            processed = self.process_word(original_word)\n",
    "                            if processed:\n",
    "                                words.append(processed)\n",
    "                        \n",
    "                        if words:\n",
    "                            section_content.append(' '.join(words))\n",
    "                    \n",
    "                    next_sibling = next_sibling.find_next_sibling()\n",
    "                \n",
    "                if section_content:\n",
    "                    paragraphs.append({'content': ' '.join(section_content)})\n",
    "                    self.total_paragraphs += 1\n",
    "            \n",
    "            return paragraphs\n",
    "\n",
    "    def get_statistics(self):\n",
    "        \"\"\"返回处理统计信息\"\"\"\n",
    "        return {\n",
    "            'total_words': self.total_words,\n",
    "            'unique_words': len(self.word_freq),\n",
    "            'total_paragraphs': self.total_paragraphs,\n",
    "            'pos_distribution': dict(self.pos_stats),\n",
    "            'word_frequencies': [\n",
    "                {'word': word, 'frequency': freq, 'pos': self.word_pos.get(word, 'UNKNOWN')}\n",
    "                for word, freq in self.word_freq.most_common()\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultSaver:\n",
    "    \"\"\"负责保存处理结果和统计信息\"\"\"\n",
    "    def __init__(self, output_dir, stats_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.stats_dir = stats_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(stats_dir, exist_ok=True)\n",
    "\n",
    "    def save_processed_file(self, paragraphs, output_path):\n",
    "        \"\"\"保存处理后的文本\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for idx, paragraph in enumerate(paragraphs, 1):\n",
    "                if paragraph['content'].strip():\n",
    "                    f.write(f\"Paragraph {idx}:\\n\")\n",
    "                    f.write(f\"{paragraph['content']}\\n\\n\")\n",
    "\n",
    "    def save_statistics(self, stats):\n",
    "        \"\"\"保存统计信息\"\"\"\n",
    "        # 保存基本统计信息\n",
    "        stats_file = os.path.join(self.stats_dir, 'preprocessing_stats.json')\n",
    "        basic_stats = {k: v for k, v in stats.items() if k != 'word_frequencies'}\n",
    "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(basic_stats, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # 保存词频统计\n",
    "        word_freq_df = pd.DataFrame(stats['word_frequencies'])\n",
    "        word_freq_file = os.path.join(self.stats_dir, 'word_frequencies.csv')\n",
    "        word_freq_df.to_csv(word_freq_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        # 记录日志\n",
    "        logging.info(f\"统计信息已保存到 {self.stats_dir}\")\n",
    "        logging.info(f\"总词数: {stats['total_words']}\")\n",
    "        logging.info(f\"独特词数: {stats['unique_words']}\")\n",
    "        logging.info(f\"总段落数: {stats['total_paragraphs']}\")\n",
    "        logging.info(\"词性分布:\")\n",
    "        for pos, count in stats['pos_distribution'].items():\n",
    "            logging.info(f\"  {pos}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理文件:   1%|▎                           | 3/306 [02:31<4:15:22, 50.57s/it, 当前文件=v170_074.xml]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     saver\u001b[38;5;241m.\u001b[39msave_statistics(stats)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, xml_file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# 处理文件\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     paragraphs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# 保存处理结果\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     saver\u001b[38;5;241m.\u001b[39msave_processed_file(paragraphs, output_file)\n",
      "Cell \u001b[0;32mIn[9], line 70\u001b[0m, in \u001b[0;36mLatinPreprocessor.process_file\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m next_sibling\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     69\u001b[0m     original_word \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrend\u001b[39m\u001b[38;5;124m'\u001b[39m, w\u001b[38;5;241m.\u001b[39mget_text())\n\u001b[0;32m---> 70\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m processed:\n\u001b[1;32m     72\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(processed)\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mLatinPreprocessor.process_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 调用 CLTK 的 NLP 管道\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mwords:  \u001b[38;5;66;03m# 如果没有返回词信息\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/nlp.py:170\u001b[0m, in \u001b[0;36mNLP.analyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mprocesses:\n\u001b[1;32m    169\u001b[0m     a_process: Process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_process_object(process_object\u001b[38;5;241m=\u001b[39mprocess)\n\u001b[0;32m--> 170\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43ma_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/processes.py:47\u001b[0m, in \u001b[0;36mLexiconProcess.run\u001b[0;34m(self, input_doc)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlemma:\n\u001b[0;32m---> 47\u001b[0m         word\u001b[38;5;241m.\u001b[39mdefinition \u001b[38;5;241m=\u001b[39m \u001b[43mlookup_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstring:\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/lat.py:78\u001b[0m, in \u001b[0;36mLatinLewisLexicon.lookup\u001b[0;34m(self, lemma)\u001b[0m\n\u001b[1;32m     75\u001b[0m lemma \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mescape(lemma\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     77\u001b[0m keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 78\u001b[0m matches: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m     key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m regex\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[0-9]?$\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     81\u001b[0m n_matches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_matches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/lat.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m lemma \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mescape(lemma\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     77\u001b[0m keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     78\u001b[0m matches: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 79\u001b[0m     key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mregex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlemma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m[0-9]?$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     81\u001b[0m n_matches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_matches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/regex/regex.py:253\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags, pos, endpos, partial, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    250\u001b[0m   concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning a match\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     pat \u001b[38;5;241m=\u001b[39m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39mmatch(string, pos, endpos, concurrent, partial, timeout)\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/regex/regex.py:459\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags, ignore_unused, kwargs, cache_it)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# We won't bother to cache the pattern if we're debugging.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    460\u001b[0m     cache_it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# What locale is this pattern using?\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/enum.py:989\u001b[0m, in \u001b[0;36mIntFlag.__and__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_ \u001b[38;5;241m&\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_value_)\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[1;32m    388\u001b[0m         value,\n\u001b[1;32m    389\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m    394\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/enum.py:682\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value):\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# all enum instances are actually created during class construction\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;66;03m# without calling this method; this method is called by the metaclass'\u001b[39;00m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# __call__ (i.e. Color(3) ), and by pickle\u001b[39;00m\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;66;03m# For lookups like Color(Color.RED)\u001b[39;00m\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# by-value search for a matching enum member\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# see if it's in the reverse mapping (for hashable values)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        import lxml\n",
    "    except ImportError:\n",
    "        logging.error(\"请安装 lxml: pip install lxml\")\n",
    "        return\n",
    "\n",
    "    # 初始化处理器和保存器\n",
    "    processor = LatinPreprocessor()\n",
    "    saver = ResultSaver(output_dir, stats_dir)\n",
    "    \n",
    "    # 获取所有XML文件\n",
    "    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n",
    "    logging.info(f\"找到 {len(xml_files)} 个XML文件\")\n",
    "    \n",
    "    # 处理文件\n",
    "    with tqdm(xml_files, desc=\"处理文件\", ncols=100, mininterval=1.0) as pbar:\n",
    "        for xml_file in pbar:\n",
    "            input_file = os.path.join(input_dir, xml_file)\n",
    "            output_file = os.path.join(output_dir, xml_file.replace('.xml', '.txt'))\n",
    "            \n",
    "            try:\n",
    "                # 处理文件\n",
    "                paragraphs = processor.process_file(input_file)\n",
    "                # 保存处理结果\n",
    "                saver.save_processed_file(paragraphs, output_file)\n",
    "                \n",
    "                if len(paragraphs) > 0:\n",
    "                    logging.debug(f\"处理文件 {xml_file}: 提取了 {len(paragraphs)} 个段落\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理文件 {xml_file} 时出错: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            pbar.set_postfix({'当前文件': xml_file})\n",
    "    \n",
    "    # 保存最终统计信息\n",
    "    stats = processor.get_statistics()\n",
    "    saver.save_statistics(stats)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取词频数据\n",
    "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
    "\n",
    "# 设置风格\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 创建图形\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# 左图：词频分布（使用区间分组）\n",
    "freq_ranges = [(0, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
    "freq_counts = []\n",
    "for start, end in freq_ranges:\n",
    "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
    "    freq_counts.append(count)\n",
    "\n",
    "ax1.bar(range(len(freq_ranges)), freq_counts, \n",
    "        width=0.6,\n",
    "        color='skyblue')\n",
    "ax1.set_xticks(range(len(freq_ranges)))\n",
    "ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
    "                     for r in freq_ranges], rotation=45)\n",
    "ax1.set_title('词频区间分布', fontsize=14, pad=20)\n",
    "ax1.set_xlabel('词频区间', fontsize=12)\n",
    "ax1.set_ylabel('词数', fontsize=12)\n",
    "\n",
    "# 在柱子上添加具体数值\n",
    "for i, count in enumerate(freq_counts):\n",
    "    ax1.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "# 右图：累积分布函数（CDF）\n",
    "frequencies = sorted(df['frequency'].values)\n",
    "cumulative = np.arange(1, len(frequencies) + 1) / len(frequencies)\n",
    "\n",
    "ax2.plot(frequencies, cumulative, 'b-', linewidth=2)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_title('词频累积分布', fontsize=14, pad=20)\n",
    "ax2.set_xlabel('词频 (log尺度)', fontsize=12)\n",
    "ax2.set_ylabel('累积比例', fontsize=12)\n",
    "\n",
    "# 添加关键分位数标记\n",
    "quantiles = [0.25, 0.5, 0.75, 0.9]\n",
    "for q in quantiles:\n",
    "    q_value = np.quantile(frequencies, q)\n",
    "    ax2.axhline(y=q, color='r', linestyle='--', alpha=0.3)\n",
    "    ax2.axvline(x=q_value, color='r', linestyle='--', alpha=0.3)\n",
    "    ax2.text(frequencies[-1], q, f'{int(q*100)}%', va='center')\n",
    "    ax2.text(q_value, 0, f'{int(q_value)}', ha='center', va='top', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印详细的统计信息\n",
    "print(\"\\n词频统计信息：\")\n",
    "print(f\"总词汇量：{len(df):,} 个词\")\n",
    "print(f\"最小词频：{df['frequency'].min():,}\")\n",
    "print(f\"最大词频：{df['frequency'].max():,}\")\n",
    "print(f\"平均词频：{df['frequency'].mean():.1f}\")\n",
    "print(f\"中位词频：{df['frequency'].median():.0f}\")\n",
    "\n",
    "print(\"\\n词频分布：\")\n",
    "for start, end in freq_ranges:\n",
    "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
    "    total = len(df)\n",
    "    if end == float('inf'):\n",
    "        print(f\">{start}次: {count:,}词 ({count/total*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{start}-{end}次: {count:,}词 ({count/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n分位数分布：\")\n",
    "for q in [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n",
    "    print(f\"{int(q*100)}%分位数: {df['frequency'].quantile(q):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取词频数据\n",
    "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
    "\n",
    "# 设置风格\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 创建多子图\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "# 1. 词频区间分布（左上）\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "freq_ranges = [(0, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
    "freq_counts = []\n",
    "for start, end in freq_ranges:\n",
    "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
    "    freq_counts.append(count)\n",
    "\n",
    "ax1.bar(range(len(freq_ranges)), freq_counts, width=0.6, color='skyblue')\n",
    "ax1.set_xticks(range(len(freq_ranges)))\n",
    "ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
    "                     for r in freq_ranges], rotation=45)\n",
    "ax1.set_title('词频区间分布', fontsize=14)\n",
    "ax1.set_xlabel('词频区间', fontsize=12)\n",
    "ax1.set_ylabel('词数', fontsize=12)\n",
    "\n",
    "# 添加数值标签\n",
    "for i, count in enumerate(freq_counts):\n",
    "    ax1.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# 2. 词性分布（右上）\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "pos_counts = df['pos'].value_counts()\n",
    "ax2.bar(range(len(pos_counts)), pos_counts.values, color='lightgreen')\n",
    "ax2.set_xticks(range(len(pos_counts)))\n",
    "ax2.set_xticklabels(pos_counts.index, rotation=45)\n",
    "ax2.set_title('词性分布', fontsize=14)\n",
    "ax2.set_xlabel('词性', fontsize=12)\n",
    "ax2.set_ylabel('词数', fontsize=12)\n",
    "\n",
    "# 添加数值标签\n",
    "for i, count in enumerate(pos_counts.values):\n",
    "    ax2.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# 3. 各词性的词频箱型图（左下）\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "sns.boxplot(data=df, x='pos', y='frequency', showfliers=False, ax=ax3)\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_title('各词性的词频分布 (不含异常值)', fontsize=14)\n",
    "ax3.set_xlabel('词性', fontsize=12)\n",
    "ax3.set_ylabel('词频 (log尺度)', fontsize=12)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. 高频词分析（右下）\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "top_n = 20\n",
    "top_words = df.nlargest(top_n, 'frequency')\n",
    "sns.barplot(data=top_words, x='frequency', y='word', hue='pos', ax=ax4)\n",
    "ax4.set_title(f'前{top_n}个高频词', fontsize=14)\n",
    "ax4.set_xlabel('词频', fontsize=12)\n",
    "ax4.set_ylabel('词', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印详细统计信息\n",
    "print(\"\\n各词性的词频统计：\")\n",
    "pos_stats = df.groupby('pos').agg({\n",
    "    'frequency': ['count', 'min', 'max', 'mean', 'median']\n",
    "}).round(2)\n",
    "print(pos_stats)\n",
    "\n",
    "print(\"\\n各词性的高频词（前10个）：\")\n",
    "for pos in df['pos'].unique():\n",
    "    top_words = df[df['pos'] == pos].nlargest(10, 'frequency')\n",
    "    print(f\"\\n{pos}:\")\n",
    "    for _, row in top_words.iterrows():\n",
    "        print(f\"{row['word']}: {row['frequency']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 设置中文字体和样式\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOS\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def analyze_filtered_vocabulary(df, min_freq=2, max_freq=1000, output_dir='results/preprocessed'):\n",
    "    \"\"\"\n",
    "    分析词汇过滤前后的统计特征\n",
    "    \n",
    "    参数:\n",
    "    df: DataFrame, 包含词频数据\n",
    "    min_freq: int, 最小词频阈值\n",
    "    max_freq: int, 最大词频阈值\n",
    "    output_dir: str, 输出目录\n",
    "    \"\"\"\n",
    "    \n",
    "    # 创建过滤后的数据框\n",
    "    df_filtered = df[(df['frequency'] >= min_freq) & (df['frequency'] <= max_freq)].copy()\n",
    "    \n",
    "    # 创建图形\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.2])\n",
    "    \n",
    "    # 1. 词频区间分布（左上）\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    freq_ranges = [(2, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
    "    freq_counts_before = []\n",
    "    freq_counts_after = []\n",
    "    \n",
    "    for start, end in freq_ranges:\n",
    "        count_before = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
    "        count_after = len(df_filtered[(df_filtered['frequency'] >= start) & \n",
    "                                    (df_filtered['frequency'] < end)])\n",
    "        freq_counts_before.append(count_before)\n",
    "        freq_counts_after.append(count_after)\n",
    "    \n",
    "    x = np.arange(len(freq_ranges))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, freq_counts_before, width, label='过滤前', color='skyblue')\n",
    "    ax1.bar(x + width/2, freq_counts_after, width, label='过滤后', color='lightcoral')\n",
    "    \n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
    "                         for r in freq_ranges], rotation=45)\n",
    "    ax1.set_title('词频区间分布对比', fontsize=14, pad=20)\n",
    "    ax1.set_xlabel('词频区间', fontsize=12)\n",
    "    ax1.set_ylabel('词数', fontsize=12)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, v in enumerate(freq_counts_before):\n",
    "        ax1.text(i - width/2, v, str(v), ha='center', va='bottom')\n",
    "    for i, v in enumerate(freq_counts_after):\n",
    "        ax1.text(i + width/2, v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 词性分布对比（右上）\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    pos_before = df['pos'].value_counts()\n",
    "    pos_after = df_filtered['pos'].value_counts()\n",
    "    \n",
    "    x = np.arange(len(pos_before))\n",
    "    ax2.bar(x - width/2, pos_before.values, width, label='过滤前', color='skyblue')\n",
    "    ax2.bar(x + width/2, pos_after.values, width, label='过滤后', color='lightcoral')\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(pos_before.index, rotation=45)\n",
    "    ax2.set_title('词性分布对比', fontsize=14, pad=20)\n",
    "    ax2.set_xlabel('词性', fontsize=12)\n",
    "    ax2.set_ylabel('词数', fontsize=12)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, v in enumerate(pos_before.values):\n",
    "        ax2.text(i - width/2, v, str(v), ha='center', va='bottom')\n",
    "    for i, v in enumerate(pos_after.values):\n",
    "        ax2.text(i + width/2, v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 被过滤的高频词（左下）\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    high_freq_words = df[df['frequency'] > max_freq].sort_values('frequency', ascending=True).tail(15)\n",
    "    \n",
    "    colors = {'NOUN': 'skyblue', 'VERB': 'lightcoral', 'ADJ': 'lightgreen', 'PROPN': 'orange'}\n",
    "    bars = ax3.barh(range(len(high_freq_words)), high_freq_words['frequency'],\n",
    "                    color=[colors.get(pos, 'gray') for pos in high_freq_words['pos']])\n",
    "    \n",
    "    ax3.set_yticks(range(len(high_freq_words)))\n",
    "    ax3.set_yticklabels(high_freq_words['word'])\n",
    "    ax3.set_title('被过滤的高频词 (Top 15)', fontsize=14, pad=20)\n",
    "    ax3.set_xlabel('词频', fontsize=12)\n",
    "    \n",
    "    # 添加词性标签\n",
    "    for i, bar in enumerate(bars):\n",
    "        ax3.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
    "                f\" {high_freq_words.iloc[i]['pos']}\", \n",
    "                va='center')\n",
    "    \n",
    "    # 4. 过滤后词频分布（右下）\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    sns.boxplot(data=df_filtered, x='pos', y='frequency', ax=ax4)\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.set_title('过滤后各词性的词频分布', fontsize=14, pad=20)\n",
    "    ax4.set_xlabel('词性', fontsize=12)\n",
    "    ax4.set_ylabel('词频 (log尺度)', fontsize=12)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"\\n=== 过滤前后统计 ===\")\n",
    "    print(f\"过滤前总词数：{len(df):,}\")\n",
    "    print(f\"过滤后总词数：{len(df_filtered):,}\")\n",
    "    print(f\"过滤掉的词数：{len(df) - len(df_filtered):,}\")\n",
    "    \n",
    "    print(\"\\n=== 各词性过滤前后对比 ===\")\n",
    "    pos_compare = pd.DataFrame({\n",
    "        '过滤前': pos_before,\n",
    "        '过滤后': pos_after,\n",
    "        '减少比例': (pos_before - pos_after) / pos_before * 100\n",
    "    }).round(2)\n",
    "    print(pos_compare)\n",
    "    \n",
    "    # 保存详细统计信息\n",
    "    stats = {\n",
    "        '过滤前': {\n",
    "            '总词数': len(df),\n",
    "            '总词频': df['frequency'].sum(),\n",
    "            '各词性数量': df['pos'].value_counts().to_dict(),\n",
    "            '词频统计': {\n",
    "                '最小值': df['frequency'].min(),\n",
    "                '最大值': df['frequency'].max(),\n",
    "                '平均值': df['frequency'].mean(),\n",
    "                '中位数': df['frequency'].median(),\n",
    "                '标准差': df['frequency'].std()\n",
    "            }\n",
    "        },\n",
    "        '过滤后': {\n",
    "            '总词数': len(df_filtered),\n",
    "            '总词频': df_filtered['frequency'].sum(),\n",
    "            '各词性数量': df_filtered['pos'].value_counts().to_dict(),\n",
    "            '词频统计': {\n",
    "                '最小值': df_filtered['frequency'].min(),\n",
    "                '最大值': df_filtered['frequency'].max(),\n",
    "                '平均值': df_filtered['frequency'].mean(),\n",
    "                '中位数': df_filtered['frequency'].median(),\n",
    "                '标准差': df_filtered['frequency'].std()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存统计结果\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存过滤后的词典\n",
    "    filtered_df.to_csv(os.path.join(output_dir, 'spacy_filtered_vocabulary.csv'), index=False)\n",
    "    \n",
    "    # 保存统计信息\n",
    "    pd.DataFrame(pos_compare).to_csv(os.path.join(output_dir, 'spacy_pos_statistics.csv'))\n",
    "    \n",
    "    # 保存高频词列表\n",
    "    high_freq_words.to_csv(os.path.join(output_dir, 'spacy_high_frequency_words.csv'), index=False)\n",
    "    \n",
    "    return df_filtered, stats\n",
    "\n",
    "# 读取原始词频数据\n",
    "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
    "\n",
    "# 运行分析\n",
    "filtered_df, stats = analyze_filtered_vocabulary(df, min_freq=2, max_freq=1000)\n",
    "\n",
    "# 打印一些额外的统计信息\n",
    "print(\"\\n=== 高频词统计 ===\")\n",
    "print(\"词频超过1000的词：\")\n",
    "for _, row in df[df['frequency'] > 1000].sort_values('frequency', ascending=False).iterrows():\n",
    "    print(f\"{row['word']} ({row['pos']}): {row['frequency']:,}\")\n",
    "\n",
    "print(\"\\n=== 词性分布统计 ===\")\n",
    "for pos in filtered_df['pos'].unique():\n",
    "    pos_stats = filtered_df[filtered_df['pos'] == pos]['frequency'].describe()\n",
    "    print(f\"\\n{pos}:\")\n",
    "    print(pos_stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
