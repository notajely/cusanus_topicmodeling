{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import json\n",
                "import logging\n",
                "import pandas as pd\n",
                "from bs4 import BeautifulSoup\n",
                "from collections import Counter\n",
                "from tqdm import tqdm\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# æ›´æ–°å¯¼å…¥è¯­å¥\n",
                "from cltk import NLP\n",
                "from cltk.core.data_types import Doc, Word\n",
                "from cltk.lemmatize.processes import LemmatizationProcess  # æ›¿æ¢ LatinLemmatizer\n",
                "from cltk.tag.pos import POSTag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â€ğ¤€ CLTK version '1.3.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
                        "\n",
                        "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinSpacyProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinLexiconProcess`.\n",
                        "\n",
                        "â¸– ``LatinSpacyProcess`` using LatinCy model by Patrick Burns from https://huggingface.co/latincy . Please cite: https://arxiv.org/abs/2305.04365\n",
                        "â¸– ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
                        "â¸– ``LatinLexiconProcess`` using Lewis's *An Elementary Latin Dictionary* (1890).\n",
                        "\n",
                        "â¸ To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[cltk.alphabet.processes.LatinNormalizeProcess,\n",
                            " cltk.dependency.processes.LatinSpacyProcess,\n",
                            " cltk.embeddings.processes.LatinEmbeddingsProcess,\n",
                            " cltk.stops.processes.StopsProcess,\n",
                            " cltk.lexicon.processes.LatinLexiconProcess]"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from cltk import NLP\n",
                "nlp = NLP('lat')\n",
                "nlp.pipeline.processes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Word(index_char_start=0, index_char_stop=4, index_token=1, index_sentence=0, string='deus', pos=noun, lemma='deus', stem=None, scansion=None, xpos='noun', upos='NOUN', dependency_relation='ROOT', governor=1, features={Case: [nominative], Gender: [masculine], Number: [singular]}, category={F: [neg], N: [pos], V: [neg]}, stop=False, named_entity=None, syllables=None, phonetic_transcription=None, definition=\"deus\\n\\n Ä« (nom plur. dÄ«, diÄ«,\\n            rarely deÄ«; gen. deÅrum or deÅ«m, poet. also\\n            divÅm or divÅ«m; dat. dÄ«s, diÄ«s, and\\n            later deÄ«s), m\\n\\nDIV-, \\na god, deity\\n: deorum inmortalium numen: consilio deorum,\\n                Cs.â€” In ejaculations: di! T.: di\\n                boni!\\nT.: di inmortales!\\nT.: Pro di inmortales!\\nT.: per deos inmortalÄ«s!: di magni!\\nO.: di vostram fidem!\\nT.: pro deÅ«m fidem!\\nT.: Pro deÅ«m atque hominum fidem!\\nT.: pro deÅ«m inmortalium!\\nT.â€”In wishes, greetings, and asseverations: di bene vortant,\\n                T.: utinam ita di faxint, T.:\\n                quod di prohibeant, T.: quod di omen avertant, \\nthe gods forbid\\n: di melius duint, T.: Di meliora piis, V.: di meliora\\n                velint, O.: di meliora!\\n\\ngod forbid!\\n: di melius, O.: Di\\n                tibi omnia optata offerant, T.: Ut illum di\\n                deaeque perdant, T.: Di tibi male\\n            faciant, T.: Ita me di ament,\\n            T.: cum dis volentibus, \\nby the gods help\\n: dis volentibus, S.: si dis placet, \\nan't please the gods\\n, T.: di hominesque, i. e. \\nall the world\\n: dis hominibusque invitis, \\nin spite of everybody.â€”The divine power\\n: deum ire per omnÄ«s Terras (dicunt),\\n                V.: Incaluit deo, O.â€”\\nA goddess\\n (poet.): ducente deo (sc. Venere), V.: AudentÄ«s deus ipse iuvat (sc. Fortuna),\\n            O.â€”Of persons, \\na god, divine being\\n: te in dicendo semper putavi deum: Plato quasi deus\\n                philosphorum: deus ille magister, V.: deos\\n                quoniam propius contingis, \\nthe powers that be\\n, H.: deus sum, si hoc ita est, \\nmy fortune is divine\\n, T.\")]\n"
                    ]
                }
            ],
            "source": [
                "doc = nlp.analyze(text=\"deus\")\n",
                "print(doc.words)  # éªŒè¯æ˜¯å¦è¿”å›æ­£ç¡®çš„è¯æ€§å’Œè¯å½¢ä¿¡æ¯\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-11-24 21:47:21,824 - INFO - æˆåŠŸåŠ è½½CLTKæ‹‰ä¸è¯­æ¨¡å‹\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Current working directory:  /Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\n",
                        "â€ğ¤€ CLTK version '1.3.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
                        "\n",
                        "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinSpacyProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinLexiconProcess`.\n",
                        "\n",
                        "â¸– ``LatinSpacyProcess`` using LatinCy model by Patrick Burns from https://huggingface.co/latincy . Please cite: https://arxiv.org/abs/2305.04365\n",
                        "â¸– ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
                        "â¸– ``LatinLexiconProcess`` using Lewis's *An Elementary Latin Dictionary* (1890).\n",
                        "\n",
                        "â¸ To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n"
                    ]
                }
            ],
            "source": [
                "# è®¾ç½®å·¥ä½œç›®å½•\n",
                "project_root = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
                "os.chdir(project_root)\n",
                "print(\"Current working directory: \", os.getcwd())\n",
                "\n",
                "# è®¾ç½®è·¯å¾„\n",
                "base_dir = \"/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling\"\n",
                "input_dir = os.path.join(base_dir, \"data/v_variant\")  # XMLæ–‡ä»¶æ‰€åœ¨ç›®å½•\n",
                "output_dir = os.path.join(base_dir, \"experiments/lda/cltk/preprocessed\")  # é¢„å¤„ç†åçš„æ–‡æœ¬\n",
                "stats_dir = os.path.join(base_dir, \"experiments/lda/cltk/preprocessed/sta\")  # ç»Ÿè®¡ä¿¡æ¯\n",
                "\n",
                "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "os.makedirs(stats_dir, exist_ok=True)\n",
                "\n",
                "# é…ç½®æ—¥å¿—\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
                "    handlers=[\n",
                "        logging.FileHandler(\"logs/preprocessing.log\"),\n",
                "        logging.StreamHandler()\n",
                "    ]\n",
                ")\n",
                "\n",
                "# æ›¿æ¢spacyåŠ è½½ä¸ºCLTKåŠ è½½\n",
                "try:\n",
                "    nlp = NLP(language=\"lat\")\n",
                "    logging.info(\"æˆåŠŸåŠ è½½CLTKæ‹‰ä¸è¯­æ¨¡å‹\")\n",
                "except:\n",
                "    logging.error(\"æ— æ³•åŠ è½½CLTKæ‹‰ä¸è¯­æ¨¡å‹ï¼Œè¯·ç¡®ä¿å·²å®‰è£…\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# åŠ è½½åœç”¨è¯\n",
                "with open('data/external/stopwords_latin.txt', 'r', encoding='utf-8') as f:\n",
                "    LATIN_STOPS = set(f.read().splitlines())\n",
                "logging.info(f\"æˆåŠŸåŠ è½½ {len(LATIN_STOPS)} ä¸ªè‡ªå®šä¹‰åœç”¨è¯\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-11-24 21:47:57,679 - INFO - å¼€å§‹éªŒè¯CLTKè¾“å‡º...\n",
                        "2024-11-24 21:47:57,850 - INFO - æµ‹è¯•è¯ 'deus': POS=noun, Lemma=deus\n",
                        "2024-11-24 21:47:57,921 - INFO - æµ‹è¯•è¯ 'est': POS=auxiliary, Lemma=sum\n",
                        "2024-11-24 21:47:57,981 - INFO - æµ‹è¯•è¯ 'bonus': POS=adjective, Lemma=bonus\n",
                        "2024-11-24 21:47:58,130 - INFO - æµ‹è¯•è¯ 'nomen': POS=noun, Lemma=nomen\n",
                        "2024-11-24 21:47:58,192 - INFO - æµ‹è¯•è¯ 'filii': POS=noun, Lemma=filius\n",
                        "2024-11-24 21:47:58,195 - INFO - æ‰¾åˆ° 306 ä¸ªXMLæ–‡ä»¶\n",
                        "å¤„ç†æ–‡ä»¶:   0%|                                                             | 0/306 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "å¤„ç†æ–‡ä»¶:   0%|                            | 1/306 [01:02<5:18:38, 62.68s/it, å½“å‰æ–‡ä»¶=v170_048.xml]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'FutPerf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "å¤„ç†æ–‡ä»¶:   1%|â–                           | 2/306 [01:31<3:37:46, 42.98s/it, å½“å‰æ–‡ä»¶=v170_060.xml]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n",
                        "Unrecognized value 'Perf' for UD feature 'Tense'.\n",
                        "If you believe this is not an error in the dependency parser, please raise an issue at <https://github.com/cltk/cltk/issues> and include a short text to reproduce the error.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "å¤„ç†æ–‡ä»¶:   1%|â–                           | 2/306 [01:39<4:11:07, 49.56s/it, å½“å‰æ–‡ä»¶=v170_060.xml]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m     processor\u001b[38;5;241m.\u001b[39msave_statistics()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[8], line 188\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, xml_file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     paragraphs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# å‡å°‘æ—¥å¿—è¾“å‡ºé¢‘ç‡\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paragraphs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
                        "Cell \u001b[0;32mIn[8], line 97\u001b[0m, in \u001b[0;36mLatinPreprocessor.process_file\u001b[0;34m(self, file_path, output_path)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m next_sibling\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     96\u001b[0m     original_word \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrend\u001b[39m\u001b[38;5;124m'\u001b[39m, w\u001b[38;5;241m.\u001b[39mget_text())\n\u001b[0;32m---> 97\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m processed:\n\u001b[1;32m     99\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(processed)\n",
                        "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mLatinPreprocessor.process_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# ä½¿ç”¨CLTKè¿›è¡Œå¤„ç†\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mwords:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/nlp.py:170\u001b[0m, in \u001b[0;36mNLP.analyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mprocesses:\n\u001b[1;32m    169\u001b[0m     a_process: Process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_process_object(process_object\u001b[38;5;241m=\u001b[39mprocess)\n\u001b[0;32m--> 170\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43ma_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/processes.py:47\u001b[0m, in \u001b[0;36mLexiconProcess.run\u001b[0;34m(self, input_doc)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlemma:\n\u001b[0;32m---> 47\u001b[0m         word\u001b[38;5;241m.\u001b[39mdefinition \u001b[38;5;241m=\u001b[39m \u001b[43mlookup_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstring:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/lat.py:78\u001b[0m, in \u001b[0;36mLatinLewisLexicon.lookup\u001b[0;34m(self, lemma)\u001b[0m\n\u001b[1;32m     75\u001b[0m lemma \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mescape(lemma\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     77\u001b[0m keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 78\u001b[0m matches: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m     key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m regex\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[0-9]?$\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     81\u001b[0m n_matches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_matches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/cltk/lexicon/lat.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m lemma \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mescape(lemma\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     77\u001b[0m keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     78\u001b[0m matches: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 79\u001b[0m     key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mregex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlemma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m[0-9]?$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     81\u001b[0m n_matches: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_matches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/cusanus_env/lib/python3.10/site-packages/regex/regex.py:253\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags, pos, endpos, partial, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    250\u001b[0m   concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning a match\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     pat \u001b[38;5;241m=\u001b[39m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39mmatch(string, pos, endpos, concurrent, partial, timeout)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "class LatinPreprocessor:\n",
                "    def __init__(self):\n",
                "        self.nlp = nlp\n",
                "        \n",
                "        # æ·»åŠ è®¡æ•°å™¨åˆå§‹åŒ–\n",
                "        self.word_freq = Counter()\n",
                "        self.word_pos = {}\n",
                "        self.pos_stats = Counter()\n",
                "        self.total_words = 0\n",
                "        self.total_paragraphs = 0\n",
                "        self.stopwords = LATIN_STOPS  # ç¡®ä¿è¿™ä¸ªå˜é‡å·²å®šä¹‰\n",
                "        \n",
                "        # æ›´æ–°CLTKçš„è¯æ€§æ˜ å°„ - ä½¿ç”¨Universal POS Tags\n",
                "        self.VALID_POS = {'NOUN', 'ADJ', 'VERB', 'PROPN'}  # æ‰©å±•æœ‰æ•ˆè¯æ€§é›†åˆ\n",
                "        self.pos_mapping = {\n",
                "            'NOUN': 'NOUN',      # åè¯\n",
                "            'VERB': 'VERB',      # åŠ¨è¯\n",
                "            'ADJ': 'ADJ',        # å½¢å®¹è¯\n",
                "            'PROPN': 'PROPN',    # ä¸“æœ‰åè¯\n",
                "            'ADV': 'ADV',        # å‰¯è¯\n",
                "            'PRON': 'PRON',      # ä»£è¯\n",
                "            'DET': 'DET',        # é™å®šè¯\n",
                "            'ADP': 'ADP',        # ä»‹è¯\n",
                "            'CCONJ': 'CONJ',     # å¹¶åˆ—è¿è¯\n",
                "            'SCONJ': 'CONJ',     # ä»å±è¿è¯\n",
                "            'NUM': 'NUM',        # æ•°è¯\n",
                "            'PART': 'PART',      # å°å“è¯\n",
                "            'INTJ': 'INTJ',      # æ„Ÿå¹è¯\n",
                "            'X': 'X'             # å…¶ä»–\n",
                "        }\n",
                "        \n",
                "    def process_word(self, word):\n",
                "        \"\"\"å¤„ç†å•ä¸ªè¯\"\"\"\n",
                "        try:\n",
                "            # æ¸…ç†è¯å½¢\n",
                "            word = word.lower().strip()\n",
                "            # å»é™¤æ‰€æœ‰æ•°å­—\n",
                "            word = re.sub(r'\\d', '', word)  # ä¿®æ”¹è¿™é‡Œï¼šç§»é™¤æ‰€æœ‰æ•°å­—\n",
                "            # åªä¿ç•™æ‹‰ä¸å­—æ¯å’Œé•¿éŸ³ç¬¦å·\n",
                "            word = re.sub(r'[^a-zÄÄ“Ä«ÅÅ«È³Ä€Ä’ÄªÅŒÅªÈ²]', '', word)\n",
                "            \n",
                "            if not word or word in self.stopwords:\n",
                "                return None\n",
                "                \n",
                "            # ä½¿ç”¨CLTKè¿›è¡Œå¤„ç†\n",
                "            doc = self.nlp.analyze(text=word)\n",
                "            \n",
                "            if not doc.words:\n",
                "                return None\n",
                "                \n",
                "            token = doc.words[0]\n",
                "            # å¤„ç†POSæ ‡ç­¾ - è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
                "            pos_tag = str(token.pos) if token.pos else 'UNKNOWN'\n",
                "            pos = self.pos_mapping.get(pos_tag, pos_tag)\n",
                "            \n",
                "            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯\n",
                "            self.pos_stats[pos] += 1\n",
                "            \n",
                "            if pos not in self.VALID_POS:\n",
                "                return None\n",
                "                \n",
                "            # ä½¿ç”¨CLTKçš„lemmatizer\n",
                "            lemma = token.lemma if token.lemma else word\n",
                "            if lemma:\n",
                "                # ç¡®ä¿lemmaä¹Ÿç»è¿‡åŒæ ·çš„æ¸…ç†\n",
                "                lemma = re.sub(r'\\d', '', lemma.lower())\n",
                "                lemma = re.sub(r'[^a-zÄÄ“Ä«ÅÅ«È³Ä€Ä’ÄªÅŒÅªÈ²]', '', lemma)\n",
                "                \n",
                "                if lemma:  # ç¡®ä¿æ¸…ç†åçš„lemmaä¸ä¸ºç©º\n",
                "                    self.word_freq[lemma] += 1\n",
                "                    self.word_pos[lemma] = pos\n",
                "                    self.total_words += 1\n",
                "                    return lemma\n",
                "                \n",
                "        except Exception as e:\n",
                "            logging.warning(f\"å¤„ç†è¯ '{word}' æ—¶å‡ºé”™: {str(e)}\")\n",
                "            return None\n",
                "            \n",
                "        return None\n",
                "\n",
                "    def process_file(self, file_path, output_path):\n",
                "        \"\"\"å¤„ç†å•ä¸ªæ–‡ä»¶\"\"\"\n",
                "        with open(file_path, 'r', encoding='utf-8') as file:\n",
                "            # ä½¿ç”¨xmlè§£æå™¨\n",
                "            soup = BeautifulSoup(file, 'xml')\n",
                "            paragraphs = []\n",
                "            \n",
                "            for fw_tag in soup.find_all('fw', {'type': 'n'}):\n",
                "                section_content = []\n",
                "                next_sibling = fw_tag.find_next_sibling()\n",
                "                \n",
                "                while next_sibling and next_sibling.name != 'fw':\n",
                "                    if next_sibling.name == 'p':\n",
                "                        words = []\n",
                "                        for w in next_sibling.find_all('w'):\n",
                "                            original_word = w.get('rend', w.get_text())\n",
                "                            processed = self.process_word(original_word)\n",
                "                            if processed:\n",
                "                                words.append(processed)\n",
                "                        \n",
                "                        if words:\n",
                "                            section_content.append(' '.join(words))\n",
                "                    \n",
                "                    next_sibling = next_sibling.find_next_sibling()\n",
                "                \n",
                "                if section_content:\n",
                "                    paragraphs.append({'content': ' '.join(section_content)})\n",
                "                    self.total_paragraphs += 1\n",
                "            \n",
                "            # ä¿å­˜å¤„ç†åçš„æ–‡æœ¬\n",
                "            with open(output_path, 'w', encoding='utf-8') as f:\n",
                "                for idx, paragraph in enumerate(paragraphs, 1):\n",
                "                    if paragraph['content'].strip():\n",
                "                        f.write(f\"Paragraph {idx}:\\n\")\n",
                "                        f.write(f\"{paragraph['content']}\\n\\n\")\n",
                "            \n",
                "            return paragraphs\n",
                "\n",
                "    def save_statistics(self):\n",
                "        \"\"\"ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
                "        stats = {\n",
                "            'total_words': self.total_words,\n",
                "            'unique_words': len(self.word_freq),\n",
                "            'total_paragraphs': self.total_paragraphs,\n",
                "            'pos_distribution': dict(self.pos_stats)\n",
                "        }\n",
                "        \n",
                "        # ä¿å­˜åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
                "        stats_file = os.path.join(stats_dir, 'preprocessing_stats.json')\n",
                "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
                "            json.dump(stats, f, ensure_ascii=False, indent=2)\n",
                "        \n",
                "        # ä¿å­˜è¯é¢‘ç»Ÿè®¡\n",
                "        word_freq_df = pd.DataFrame([\n",
                "            {'word': word, 'frequency': freq, 'pos': self.word_pos.get(word, 'UNKNOWN')}\n",
                "            for word, freq in self.word_freq.most_common()\n",
                "        ])\n",
                "        word_freq_file = os.path.join(stats_dir, 'word_frequencies.csv')\n",
                "        word_freq_df.to_csv(word_freq_file, index=False, encoding='utf-8')\n",
                "        \n",
                "        logging.info(f\"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ° {stats_dir}\")\n",
                "        logging.info(f\"æ€»è¯æ•°: {self.total_words}\")\n",
                "        logging.info(f\"ç‹¬ç‰¹è¯æ•°: {len(self.word_freq)}\")\n",
                "        logging.info(f\"æ€»æ®µè½æ•°: {self.total_paragraphs}\")\n",
                "        logging.info(\"è¯æ€§åˆ†å¸ƒ:\")\n",
                "        for pos, count in self.pos_stats.most_common():\n",
                "            logging.info(f\"  {pos}: {count}\")\n",
                "\n",
                "    def validate_cltk_output(self):\n",
                "        \"\"\"éªŒè¯CLTKè¾“å‡ºçš„è¯æ€§æ ‡ç­¾\"\"\"\n",
                "        test_words = [\"deus\", \"est\", \"bonus\", \"nomen\", \"filii\"]\n",
                "        for word in test_words:\n",
                "            try:\n",
                "                doc = self.nlp.analyze(text=word)\n",
                "                if doc.words:\n",
                "                    token = doc.words[0]\n",
                "                    pos_tag = str(token.pos) if token.pos else 'UNKNOWN'\n",
                "                    lemma = token.lemma if token.lemma else word\n",
                "                    logging.info(f\"æµ‹è¯•è¯ '{word}': POS={pos_tag}, Lemma={lemma}\")\n",
                "            except Exception as e:\n",
                "                logging.error(f\"æµ‹è¯•è¯ '{word}' å¤„ç†å¤±è´¥: {str(e)}\")\n",
                "\n",
                "def main():\n",
                "    # ç¡®ä¿å·²å®‰è£… lxml\n",
                "    try:\n",
                "        import lxml\n",
                "    except ImportError:\n",
                "        logging.error(\"è¯·å®‰è£… lxml: pip install lxml\")\n",
                "        return\n",
                "\n",
                "    processor = LatinPreprocessor()\n",
                "    \n",
                "    # é¦–å…ˆè¿è¡ŒéªŒè¯\n",
                "    logging.info(\"å¼€å§‹éªŒè¯CLTKè¾“å‡º...\")\n",
                "    processor.validate_cltk_output()\n",
                "    \n",
                "    # è·å–æ‰€æœ‰XMLæ–‡ä»¶\n",
                "    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n",
                "    logging.info(f\"æ‰¾åˆ° {len(xml_files)} ä¸ªXMLæ–‡ä»¶\")\n",
                "    \n",
                "    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼Œä½†å‡å°‘è¾“å‡ºé¢‘ç‡\n",
                "    with tqdm(xml_files, desc=\"å¤„ç†æ–‡ä»¶\", ncols=100, mininterval=1.0) as pbar:\n",
                "        for xml_file in pbar:\n",
                "            input_file = os.path.join(input_dir, xml_file)\n",
                "            output_file = os.path.join(output_dir, xml_file.replace('.xml', '.txt'))\n",
                "            \n",
                "            try:\n",
                "                paragraphs = processor.process_file(input_file, output_file)\n",
                "                # å‡å°‘æ—¥å¿—è¾“å‡ºé¢‘ç‡\n",
                "                if len(paragraphs) > 0:\n",
                "                    logging.debug(f\"å¤„ç†æ–‡ä»¶ {xml_file}: æå–äº† {len(paragraphs)} ä¸ªæ®µè½\")\n",
                "            except Exception as e:\n",
                "                logging.error(f\"å¤„ç†æ–‡ä»¶ {xml_file} æ—¶å‡ºé”™: {str(e)}\")\n",
                "                continue\n",
                "            \n",
                "            # æ›´æ–°è¿›åº¦æ¡æè¿°\n",
                "            pbar.set_postfix({'å½“å‰æ–‡ä»¶': xml_file})\n",
                "    \n",
                "    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\n",
                "    processor.save_statistics()\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# è¯»å–è¯é¢‘æ•°æ®\n",
                "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
                "\n",
                "# è®¾ç½®é£æ ¼\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "# åˆ›å»ºå›¾å½¢\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
                "\n",
                "# å·¦å›¾ï¼šè¯é¢‘åˆ†å¸ƒï¼ˆä½¿ç”¨åŒºé—´åˆ†ç»„ï¼‰\n",
                "freq_ranges = [(0, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
                "freq_counts = []\n",
                "for start, end in freq_ranges:\n",
                "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "    freq_counts.append(count)\n",
                "\n",
                "ax1.bar(range(len(freq_ranges)), freq_counts, \n",
                "        width=0.6,\n",
                "        color='skyblue')\n",
                "ax1.set_xticks(range(len(freq_ranges)))\n",
                "ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
                "                     for r in freq_ranges], rotation=45)\n",
                "ax1.set_title('è¯é¢‘åŒºé—´åˆ†å¸ƒ', fontsize=14, pad=20)\n",
                "ax1.set_xlabel('è¯é¢‘åŒºé—´', fontsize=12)\n",
                "ax1.set_ylabel('è¯æ•°', fontsize=12)\n",
                "\n",
                "# åœ¨æŸ±å­ä¸Šæ·»åŠ å…·ä½“æ•°å€¼\n",
                "for i, count in enumerate(freq_counts):\n",
                "    ax1.text(i, count, str(count), ha='center', va='bottom')\n",
                "\n",
                "# å³å›¾ï¼šç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰\n",
                "frequencies = sorted(df['frequency'].values)\n",
                "cumulative = np.arange(1, len(frequencies) + 1) / len(frequencies)\n",
                "\n",
                "ax2.plot(frequencies, cumulative, 'b-', linewidth=2)\n",
                "ax2.set_xscale('log')\n",
                "ax2.set_title('è¯é¢‘ç´¯ç§¯åˆ†å¸ƒ', fontsize=14, pad=20)\n",
                "ax2.set_xlabel('è¯é¢‘ (logå°ºåº¦)', fontsize=12)\n",
                "ax2.set_ylabel('ç´¯ç§¯æ¯”ä¾‹', fontsize=12)\n",
                "\n",
                "# æ·»åŠ å…³é”®åˆ†ä½æ•°æ ‡è®°\n",
                "quantiles = [0.25, 0.5, 0.75, 0.9]\n",
                "for q in quantiles:\n",
                "    q_value = np.quantile(frequencies, q)\n",
                "    ax2.axhline(y=q, color='r', linestyle='--', alpha=0.3)\n",
                "    ax2.axvline(x=q_value, color='r', linestyle='--', alpha=0.3)\n",
                "    ax2.text(frequencies[-1], q, f'{int(q*100)}%', va='center')\n",
                "    ax2.text(q_value, 0, f'{int(q_value)}', ha='center', va='top', rotation=90)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# æ‰“å°è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯\n",
                "print(\"\\nè¯é¢‘ç»Ÿè®¡ä¿¡æ¯ï¼š\")\n",
                "print(f\"æ€»è¯æ±‡é‡ï¼š{len(df):,} ä¸ªè¯\")\n",
                "print(f\"æœ€å°è¯é¢‘ï¼š{df['frequency'].min():,}\")\n",
                "print(f\"æœ€å¤§è¯é¢‘ï¼š{df['frequency'].max():,}\")\n",
                "print(f\"å¹³å‡è¯é¢‘ï¼š{df['frequency'].mean():.1f}\")\n",
                "print(f\"ä¸­ä½è¯é¢‘ï¼š{df['frequency'].median():.0f}\")\n",
                "\n",
                "print(\"\\nè¯é¢‘åˆ†å¸ƒï¼š\")\n",
                "for start, end in freq_ranges:\n",
                "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "    total = len(df)\n",
                "    if end == float('inf'):\n",
                "        print(f\">{start}æ¬¡: {count:,}è¯ ({count/total*100:.1f}%)\")\n",
                "    else:\n",
                "        print(f\"{start}-{end}æ¬¡: {count:,}è¯ ({count/total*100:.1f}%)\")\n",
                "\n",
                "print(\"\\nåˆ†ä½æ•°åˆ†å¸ƒï¼š\")\n",
                "for q in [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n",
                "    print(f\"{int(q*100)}%åˆ†ä½æ•°: {df['frequency'].quantile(q):.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# è¯»å–è¯é¢‘æ•°æ®\n",
                "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
                "\n",
                "# è®¾ç½®é£æ ¼\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "# åˆ›å»ºå¤šå­å›¾\n",
                "fig = plt.figure(figsize=(20, 15))\n",
                "gs = fig.add_gridspec(2, 2)\n",
                "\n",
                "# 1. è¯é¢‘åŒºé—´åˆ†å¸ƒï¼ˆå·¦ä¸Šï¼‰\n",
                "ax1 = fig.add_subplot(gs[0, 0])\n",
                "freq_ranges = [(0, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
                "freq_counts = []\n",
                "for start, end in freq_ranges:\n",
                "    count = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "    freq_counts.append(count)\n",
                "\n",
                "ax1.bar(range(len(freq_ranges)), freq_counts, width=0.6, color='skyblue')\n",
                "ax1.set_xticks(range(len(freq_ranges)))\n",
                "ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
                "                     for r in freq_ranges], rotation=45)\n",
                "ax1.set_title('è¯é¢‘åŒºé—´åˆ†å¸ƒ', fontsize=14)\n",
                "ax1.set_xlabel('è¯é¢‘åŒºé—´', fontsize=12)\n",
                "ax1.set_ylabel('è¯æ•°', fontsize=12)\n",
                "\n",
                "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
                "for i, count in enumerate(freq_counts):\n",
                "    ax1.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
                "\n",
                "# 2. è¯æ€§åˆ†å¸ƒï¼ˆå³ä¸Šï¼‰\n",
                "ax2 = fig.add_subplot(gs[0, 1])\n",
                "pos_counts = df['pos'].value_counts()\n",
                "ax2.bar(range(len(pos_counts)), pos_counts.values, color='lightgreen')\n",
                "ax2.set_xticks(range(len(pos_counts)))\n",
                "ax2.set_xticklabels(pos_counts.index, rotation=45)\n",
                "ax2.set_title('è¯æ€§åˆ†å¸ƒ', fontsize=14)\n",
                "ax2.set_xlabel('è¯æ€§', fontsize=12)\n",
                "ax2.set_ylabel('è¯æ•°', fontsize=12)\n",
                "\n",
                "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
                "for i, count in enumerate(pos_counts.values):\n",
                "    ax2.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
                "\n",
                "# 3. å„è¯æ€§çš„è¯é¢‘ç®±å‹å›¾ï¼ˆå·¦ä¸‹ï¼‰\n",
                "ax3 = fig.add_subplot(gs[1, 0])\n",
                "sns.boxplot(data=df, x='pos', y='frequency', showfliers=False, ax=ax3)\n",
                "ax3.set_yscale('log')\n",
                "ax3.set_title('å„è¯æ€§çš„è¯é¢‘åˆ†å¸ƒ (ä¸å«å¼‚å¸¸å€¼)', fontsize=14)\n",
                "ax3.set_xlabel('è¯æ€§', fontsize=12)\n",
                "ax3.set_ylabel('è¯é¢‘ (logå°ºåº¦)', fontsize=12)\n",
                "ax3.tick_params(axis='x', rotation=45)\n",
                "\n",
                "# 4. é«˜é¢‘è¯åˆ†æï¼ˆå³ä¸‹ï¼‰\n",
                "ax4 = fig.add_subplot(gs[1, 1])\n",
                "top_n = 20\n",
                "top_words = df.nlargest(top_n, 'frequency')\n",
                "sns.barplot(data=top_words, x='frequency', y='word', hue='pos', ax=ax4)\n",
                "ax4.set_title(f'å‰{top_n}ä¸ªé«˜é¢‘è¯', fontsize=14)\n",
                "ax4.set_xlabel('è¯é¢‘', fontsize=12)\n",
                "ax4.set_ylabel('è¯', fontsize=12)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n",
                "print(\"\\nå„è¯æ€§çš„è¯é¢‘ç»Ÿè®¡ï¼š\")\n",
                "pos_stats = df.groupby('pos').agg({\n",
                "    'frequency': ['count', 'min', 'max', 'mean', 'median']\n",
                "}).round(2)\n",
                "print(pos_stats)\n",
                "\n",
                "print(\"\\nå„è¯æ€§çš„é«˜é¢‘è¯ï¼ˆå‰10ä¸ªï¼‰ï¼š\")\n",
                "for pos in df['pos'].unique():\n",
                "    top_words = df[df['pos'] == pos].nlargest(10, 'frequency')\n",
                "    print(f\"\\n{pos}:\")\n",
                "    for _, row in top_words.iterrows():\n",
                "        print(f\"{row['word']}: {row['frequency']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOS\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "def analyze_filtered_vocabulary(df, min_freq=2, max_freq=1000, output_dir='results/preprocessed'):\n",
                "    \"\"\"\n",
                "    åˆ†æè¯æ±‡è¿‡æ»¤å‰åçš„ç»Ÿè®¡ç‰¹å¾\n",
                "    \n",
                "    å‚æ•°:\n",
                "    df: DataFrame, åŒ…å«è¯é¢‘æ•°æ®\n",
                "    min_freq: int, æœ€å°è¯é¢‘é˜ˆå€¼\n",
                "    max_freq: int, æœ€å¤§è¯é¢‘é˜ˆå€¼\n",
                "    output_dir: str, è¾“å‡ºç›®å½•\n",
                "    \"\"\"\n",
                "    \n",
                "    # åˆ›å»ºè¿‡æ»¤åçš„æ•°æ®æ¡†\n",
                "    df_filtered = df[(df['frequency'] >= min_freq) & (df['frequency'] <= max_freq)].copy()\n",
                "    \n",
                "    # åˆ›å»ºå›¾å½¢\n",
                "    fig = plt.figure(figsize=(20, 15))\n",
                "    gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.2])\n",
                "    \n",
                "    # 1. è¯é¢‘åŒºé—´åˆ†å¸ƒï¼ˆå·¦ä¸Šï¼‰\n",
                "    ax1 = fig.add_subplot(gs[0, 0])\n",
                "    freq_ranges = [(2, 10), (10, 50), (50, 100), (100, 500), (500, 1000), (1000, float('inf'))]\n",
                "    freq_counts_before = []\n",
                "    freq_counts_after = []\n",
                "    \n",
                "    for start, end in freq_ranges:\n",
                "        count_before = len(df[(df['frequency'] >= start) & (df['frequency'] < end)])\n",
                "        count_after = len(df_filtered[(df_filtered['frequency'] >= start) & \n",
                "                                    (df_filtered['frequency'] < end)])\n",
                "        freq_counts_before.append(count_before)\n",
                "        freq_counts_after.append(count_after)\n",
                "    \n",
                "    x = np.arange(len(freq_ranges))\n",
                "    width = 0.35\n",
                "    \n",
                "    ax1.bar(x - width/2, freq_counts_before, width, label='è¿‡æ»¤å‰', color='skyblue')\n",
                "    ax1.bar(x + width/2, freq_counts_after, width, label='è¿‡æ»¤å', color='lightcoral')\n",
                "    \n",
                "    ax1.set_xticks(x)\n",
                "    ax1.set_xticklabels([f'{r[0]}-{r[1]}' if r[1] != float('inf') else f'>{r[0]}' \n",
                "                         for r in freq_ranges], rotation=45)\n",
                "    ax1.set_title('è¯é¢‘åŒºé—´åˆ†å¸ƒå¯¹æ¯”', fontsize=14, pad=20)\n",
                "    ax1.set_xlabel('è¯é¢‘åŒºé—´', fontsize=12)\n",
                "    ax1.set_ylabel('è¯æ•°', fontsize=12)\n",
                "    ax1.legend()\n",
                "    \n",
                "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
                "    for i, v in enumerate(freq_counts_before):\n",
                "        ax1.text(i - width/2, v, str(v), ha='center', va='bottom')\n",
                "    for i, v in enumerate(freq_counts_after):\n",
                "        ax1.text(i + width/2, v, str(v), ha='center', va='bottom')\n",
                "    \n",
                "    # 2. è¯æ€§åˆ†å¸ƒå¯¹æ¯”ï¼ˆå³ä¸Šï¼‰\n",
                "    ax2 = fig.add_subplot(gs[0, 1])\n",
                "    pos_before = df['pos'].value_counts()\n",
                "    pos_after = df_filtered['pos'].value_counts()\n",
                "    \n",
                "    x = np.arange(len(pos_before))\n",
                "    ax2.bar(x - width/2, pos_before.values, width, label='è¿‡æ»¤å‰', color='skyblue')\n",
                "    ax2.bar(x + width/2, pos_after.values, width, label='è¿‡æ»¤å', color='lightcoral')\n",
                "    \n",
                "    ax2.set_xticks(x)\n",
                "    ax2.set_xticklabels(pos_before.index, rotation=45)\n",
                "    ax2.set_title('è¯æ€§åˆ†å¸ƒå¯¹æ¯”', fontsize=14, pad=20)\n",
                "    ax2.set_xlabel('è¯æ€§', fontsize=12)\n",
                "    ax2.set_ylabel('è¯æ•°', fontsize=12)\n",
                "    ax2.legend()\n",
                "    \n",
                "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
                "    for i, v in enumerate(pos_before.values):\n",
                "        ax2.text(i - width/2, v, str(v), ha='center', va='bottom')\n",
                "    for i, v in enumerate(pos_after.values):\n",
                "        ax2.text(i + width/2, v, str(v), ha='center', va='bottom')\n",
                "    \n",
                "    # 3. è¢«è¿‡æ»¤çš„é«˜é¢‘è¯ï¼ˆå·¦ä¸‹ï¼‰\n",
                "    ax3 = fig.add_subplot(gs[1, 0])\n",
                "    high_freq_words = df[df['frequency'] > max_freq].sort_values('frequency', ascending=True).tail(15)\n",
                "    \n",
                "    colors = {'NOUN': 'skyblue', 'VERB': 'lightcoral', 'ADJ': 'lightgreen', 'PROPN': 'orange'}\n",
                "    bars = ax3.barh(range(len(high_freq_words)), high_freq_words['frequency'],\n",
                "                    color=[colors.get(pos, 'gray') for pos in high_freq_words['pos']])\n",
                "    \n",
                "    ax3.set_yticks(range(len(high_freq_words)))\n",
                "    ax3.set_yticklabels(high_freq_words['word'])\n",
                "    ax3.set_title('è¢«è¿‡æ»¤çš„é«˜é¢‘è¯ (Top 15)', fontsize=14, pad=20)\n",
                "    ax3.set_xlabel('è¯é¢‘', fontsize=12)\n",
                "    \n",
                "    # æ·»åŠ è¯æ€§æ ‡ç­¾\n",
                "    for i, bar in enumerate(bars):\n",
                "        ax3.text(bar.get_width(), bar.get_y() + bar.get_height()/2,\n",
                "                f\" {high_freq_words.iloc[i]['pos']}\", \n",
                "                va='center')\n",
                "    \n",
                "    # 4. è¿‡æ»¤åè¯é¢‘åˆ†å¸ƒï¼ˆå³ä¸‹ï¼‰\n",
                "    ax4 = fig.add_subplot(gs[1, 1])\n",
                "    sns.boxplot(data=df_filtered, x='pos', y='frequency', ax=ax4)\n",
                "    ax4.set_yscale('log')\n",
                "    ax4.set_title('è¿‡æ»¤åå„è¯æ€§çš„è¯é¢‘åˆ†å¸ƒ', fontsize=14, pad=20)\n",
                "    ax4.set_xlabel('è¯æ€§', fontsize=12)\n",
                "    ax4.set_ylabel('è¯é¢‘ (logå°ºåº¦)', fontsize=12)\n",
                "    ax4.tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
                "    print(\"\\n=== è¿‡æ»¤å‰åç»Ÿè®¡ ===\")\n",
                "    print(f\"è¿‡æ»¤å‰æ€»è¯æ•°ï¼š{len(df):,}\")\n",
                "    print(f\"è¿‡æ»¤åæ€»è¯æ•°ï¼š{len(df_filtered):,}\")\n",
                "    print(f\"è¿‡æ»¤æ‰çš„è¯æ•°ï¼š{len(df) - len(df_filtered):,}\")\n",
                "    \n",
                "    print(\"\\n=== å„è¯æ€§è¿‡æ»¤å‰åå¯¹æ¯” ===\")\n",
                "    pos_compare = pd.DataFrame({\n",
                "        'è¿‡æ»¤å‰': pos_before,\n",
                "        'è¿‡æ»¤å': pos_after,\n",
                "        'å‡å°‘æ¯”ä¾‹': (pos_before - pos_after) / pos_before * 100\n",
                "    }).round(2)\n",
                "    print(pos_compare)\n",
                "    \n",
                "    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n",
                "    stats = {\n",
                "        'è¿‡æ»¤å‰': {\n",
                "            'æ€»è¯æ•°': len(df),\n",
                "            'æ€»è¯é¢‘': df['frequency'].sum(),\n",
                "            'å„è¯æ€§æ•°é‡': df['pos'].value_counts().to_dict(),\n",
                "            'è¯é¢‘ç»Ÿè®¡': {\n",
                "                'æœ€å°å€¼': df['frequency'].min(),\n",
                "                'æœ€å¤§å€¼': df['frequency'].max(),\n",
                "                'å¹³å‡å€¼': df['frequency'].mean(),\n",
                "                'ä¸­ä½æ•°': df['frequency'].median(),\n",
                "                'æ ‡å‡†å·®': df['frequency'].std()\n",
                "            }\n",
                "        },\n",
                "        'è¿‡æ»¤å': {\n",
                "            'æ€»è¯æ•°': len(df_filtered),\n",
                "            'æ€»è¯é¢‘': df_filtered['frequency'].sum(),\n",
                "            'å„è¯æ€§æ•°é‡': df_filtered['pos'].value_counts().to_dict(),\n",
                "            'è¯é¢‘ç»Ÿè®¡': {\n",
                "                'æœ€å°å€¼': df_filtered['frequency'].min(),\n",
                "                'æœ€å¤§å€¼': df_filtered['frequency'].max(),\n",
                "                'å¹³å‡å€¼': df_filtered['frequency'].mean(),\n",
                "                'ä¸­ä½æ•°': df_filtered['frequency'].median(),\n",
                "                'æ ‡å‡†å·®': df_filtered['frequency'].std()\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    # ä¿å­˜ç»Ÿè®¡ç»“æœ\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    # ä¿å­˜è¿‡æ»¤åçš„è¯å…¸\n",
                "    filtered_df.to_csv(os.path.join(output_dir, 'spacy_filtered_vocabulary.csv'), index=False)\n",
                "    \n",
                "    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\n",
                "    pd.DataFrame(pos_compare).to_csv(os.path.join(output_dir, 'spacy_pos_statistics.csv'))\n",
                "    \n",
                "    # ä¿å­˜é«˜é¢‘è¯åˆ—è¡¨\n",
                "    high_freq_words.to_csv(os.path.join(output_dir, 'spacy_high_frequency_words.csv'), index=False)\n",
                "    \n",
                "    return df_filtered, stats\n",
                "\n",
                "# è¯»å–åŸå§‹è¯é¢‘æ•°æ®\n",
                "df = pd.read_csv('results/preprocessed/spacy_word_frequencies.csv')\n",
                "\n",
                "# è¿è¡Œåˆ†æ\n",
                "filtered_df, stats = analyze_filtered_vocabulary(df, min_freq=2, max_freq=1000)\n",
                "\n",
                "# æ‰“å°ä¸€äº›é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯\n",
                "print(\"\\n=== é«˜é¢‘è¯ç»Ÿè®¡ ===\")\n",
                "print(\"è¯é¢‘è¶…è¿‡1000çš„è¯ï¼š\")\n",
                "for _, row in df[df['frequency'] > 1000].sort_values('frequency', ascending=False).iterrows():\n",
                "    print(f\"{row['word']} ({row['pos']}): {row['frequency']:,}\")\n",
                "\n",
                "print(\"\\n=== è¯æ€§åˆ†å¸ƒç»Ÿè®¡ ===\")\n",
                "for pos in filtered_df['pos'].unique():\n",
                "    pos_stats = filtered_df[filtered_df['pos'] == pos]['frequency'].describe()\n",
                "    print(f\"\\n{pos}:\")\n",
                "    print(pos_stats.round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}