{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 MPS 是否可用\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend on Apple Silicon.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS backend not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础路径设置\n",
    "BASE_DIR = '/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling'\n",
    "\n",
    "# 生成唯一实验ID\n",
    "experiment_id = f\"latinbert_{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "experiment_dir = os.path.join(BASE_DIR, 'experiments', 'latinbert', experiment_id)\n",
    "\n",
    "# 创建实验目录\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "\n",
    "# 获取日志文件路径\n",
    "log_file_path = os.path.join(experiment_dir, f\"{experiment_id}.log\")\n",
    "\n",
    "# 配置日志记录\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(f\"启动实验 {experiment_id}，日志记录到 {log_file_path}\")\n",
    "print(f\"启动实验 {experiment_id}，日志记录到 {log_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置\n",
    "experiment_config = {\n",
    "    \"parameters\": {\n",
    "        \"n_gram_range\": (1, 2),  # 捕捉更多的上下文短语\n",
    "        \"min_topic_size\": 2,  # 增大最小主题大小以减少噪声\n",
    "        \"nr_topics\": \"auto\",  # 自动确定主题数量\n",
    "        \"umap_params\": {\n",
    "            \"n_neighbors\": 10,  # 增加邻居数使得降维更平滑\n",
    "            \"min_dist\": 0.1,  # 增大最小距离使主题更分离\n",
    "            \"n_components\": 2,  # 增大维度以保留更多特征信息\n",
    "            \"random_state\": 42  # 确保实验可重复\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(experiment_dir, 'config.json')\n",
    "with open(config_path, 'w') as config_file:\n",
    "    json.dump(experiment_config, config_file, indent=4)\n",
    "\n",
    "logger.info(f\"实验配置已保存到 {config_path}\")\n",
    "print(f\"实验配置已保存到 {config_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载本地 Latin BERT 模型\n",
    "logger.info(\"加载 Latin-BERT 嵌入模型...\")\n",
    "model_path = \"/Users/jessie/Documents/Projects/Cusanus_Topic_Modeling/latin-bert/models/latin_bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义嵌入生成函数\n",
    "def get_latin_bert_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=256).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载段落级别的测试集数据\n",
    "testset_path = os.path.join(BASE_DIR, 'data/testset_paragraphs_level.json')\n",
    "with open(testset_path, 'r', encoding='utf-8') as json_file:\n",
    "    testset_data = json.load(json_file)\n",
    "\n",
    "# 提取段落文本和对应的 document_id, paragraph_num\n",
    "documents = []\n",
    "paragraph_contents = []\n",
    "document_ids = []\n",
    "paragraph_nums = []\n",
    "for document in testset_data[\"documents\"]:\n",
    "    for paragraph_num, paragraph in enumerate(document[\"paragraphs\"], start=1):\n",
    "        if \"content\" in paragraph:\n",
    "            documents.append(paragraph[\"content\"])\n",
    "            paragraph_contents.append(paragraph[\"content\"])\n",
    "            document_ids.append(document[\"document_id\"])\n",
    "            paragraph_nums.append(paragraph_num)\n",
    "\n",
    "logger.info(f\"从测试集加载了 {len(documents)} 个段落\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入生成\n",
    "document_embeddings = get_latin_bert_embeddings(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 CountVectorizer 和 UMAP\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=experiment_config[\"parameters\"][\"n_gram_range\"],\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "custom_umap = UMAP(\n",
    "    n_neighbors=experiment_config[\"parameters\"][\"umap_params\"][\"n_neighbors\"],\n",
    "    min_dist=experiment_config[\"parameters\"][\"umap_params\"][\"min_dist\"],\n",
    "    n_components=experiment_config[\"parameters\"][\"umap_params\"][\"n_components\"],\n",
    "    random_state=experiment_config[\"parameters\"][\"umap_params\"][\"random_state\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 CountVectorizer 和 UMAP\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=experiment_config[\"parameters\"][\"n_gram_range\"],\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "custom_umap = UMAP(\n",
    "    n_neighbors=experiment_config[\"parameters\"][\"umap_params\"][\"n_neighbors\"],\n",
    "    min_dist=experiment_config[\"parameters\"][\"umap_params\"][\"min_dist\"],\n",
    "    n_components=experiment_config[\"parameters\"][\"umap_params\"][\"n_components\"],\n",
    "    random_state=experiment_config[\"parameters\"][\"umap_params\"][\"random_state\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 BERTopic 模型，不设置 embedding_model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None,  # 不使用默认的嵌入模型\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    umap_model=custom_umap,\n",
    "    min_topic_size=experiment_config[\"parameters\"][\"min_topic_size\"],\n",
    "    nr_topics=experiment_config[\"parameters\"][\"nr_topics\"],\n",
    "    language=None  # 禁用语言特定的嵌入\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用手动生成的嵌入进行主题模型训练\n",
    "if documents:\n",
    "    print(\"Starting BERTopic model training...\")\n",
    "    logger.info(\"开始训练 BERTopic 模型...\")\n",
    "    \n",
    "    # 将生成的 numpy 格式的自定义嵌入传入 fit_transform\n",
    "    topics, probabilities = topic_model.fit_transform(documents, embeddings=document_embeddings)\n",
    "    logger.info(\"BERTopic 模型训练成功。\")\n",
    "    print(\"BERTopic model training completed.\")\n",
    "    \n",
    "    # 可视化和保存结果\n",
    "    topics_info = topic_model.get_topic_info()\n",
    "    for topic_num in topics_info['Topic'][:10]:  # 输出前 10 个主题\n",
    "        if topic_num != -1:\n",
    "            words_weights = topic_model.get_topic(topic_num)\n",
    "            words_str = ', '.join([word for word, _ in words_weights])\n",
    "            print(f\"主题 {topic_num}: {words_str}\")\n",
    "            logger.info(f\"主题 {topic_num}: {words_str}\")\n",
    "\n",
    "    # 保存文档的主题分配结果\n",
    "    document_topic_data = []\n",
    "    for doc_idx, (topic, prob) in enumerate(zip(topics, probabilities)):\n",
    "        document_id = document_ids[doc_idx]\n",
    "        paragraph_num = paragraph_nums[doc_idx]\n",
    "        document_topic_data.append([document_id, paragraph_num, topic, prob])\n",
    "\n",
    "    df_document_topics = pd.DataFrame(document_topic_data, columns=[\"Document\", \"Paragraph\", \"Assigned Topic\", \"Probability\"])\n",
    "    document_topics_csv_path = os.path.join(experiment_dir, 'bertopic_document_topic_distribution.csv')\n",
    "    df_document_topics.to_csv(document_topics_csv_path, index=False)\n",
    "    logger.info(f\"每个文档的主题分配结果已保存至 {document_topics_csv_path}。\")\n",
    "\n",
    "    # 保存文档的主题分配结果到 JSON 文件\n",
    "    document_topic_distributions = []\n",
    "    for doc_idx, (topic, prob) in enumerate(zip(topics, probabilities)):\n",
    "        document_id = document_ids[doc_idx]\n",
    "        paragraph_num = paragraph_nums[doc_idx]\n",
    "        document_topic_distributions.append({\n",
    "            \"Document\": document_id,\n",
    "            \"Paragraph\": int(paragraph_num),\n",
    "            \"Content\": paragraph_contents[doc_idx],\n",
    "            \"Topic\": topic,\n",
    "            \"Topic Keywords\": [word for word, _ in topic_model.get_topic(topic)],\n",
    "            \"Probability\": float(prob)\n",
    "        })\n",
    "\n",
    "    json_output_path = os.path.join(experiment_dir, 'document_topic_distributions.json')\n",
    "    with open(json_output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(document_topic_distributions, json_file, ensure_ascii=False, indent=4)\n",
    "    logger.info(f\"文档的主题分配情况已保存至 {json_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存可视化图表为 HTML 文件\n",
    "try:\n",
    "    topics_fig_path = os.path.join(experiment_dir, 'bertopic_topics.html')\n",
    "    fig = topic_model.visualize_topics()\n",
    "    fig.write_html(topics_fig_path)\n",
    "    logger.info(f\"主题可视化图表已保存至 {topics_fig_path}\")\n",
    "\n",
    "    # 生成并保存层次聚类图表\n",
    "    fig_hierarchy = topic_model.visualize_hierarchy()\n",
    "    fig_hierarchy.show()\n",
    "    hierarchy_fig_path = os.path.join(experiment_dir, 'bertopic_hierarchy.html')\n",
    "    fig_hierarchy.write_html(hierarchy_fig_path)\n",
    "    logger.info(f\"层次聚类图表已保存至 {hierarchy_fig_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"保存可视化图表时发生错误: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "logger.info(\"开始评估 BERTopic 模型...\")\n",
    "\n",
    "# 提取每个主题的前10个关键词\n",
    "bertopic_topics = []\n",
    "for topic_num in range(len(topic_model.get_topics())):\n",
    "    topic = topic_model.get_topic(topic_num)\n",
    "    if topic:  # 确保 topic 不是布尔值\n",
    "        bertopic_topics.append([word for word, _ in topic])\n",
    "\n",
    "# 使用 gensim.corpora.Dictionary 创建词典\n",
    "texts = [doc.split() for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# 计算主题一致性（NPMI）\n",
    "logger.info(\"计算主题一致性 (NPMI)...\")\n",
    "coherence_model_npmi = CoherenceModel(topics=bertopic_topics, texts=texts, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score_npmi = coherence_model_npmi.get_coherence()\n",
    "logger.info(f\"平均主题一致性 (NPMI): {coherence_score_npmi}\")\n",
    "print(f\"平均主题一致性 (NPMI): {coherence_score_npmi}\")\n",
    "\n",
    "# 计算主题多样性\n",
    "unique_words = set()\n",
    "total_words = 0\n",
    "for topic in bertopic_topics:\n",
    "    unique_words.update(topic)\n",
    "    total_words += len(topic)\n",
    "topic_diversity = len(unique_words) / total_words\n",
    "logger.info(f\"主题多样性: {topic_diversity}\")\n",
    "print(f\"主题多样性: {topic_diversity}\")\n",
    "\n",
    "# 计算 WEPS 评分\n",
    "logger.info(\"计算 WEPS 评分...\")\n",
    "\n",
    "# 定义嵌入获取函数：根据主题中的词汇获取其嵌入向量\n",
    "def get_average_embedding_for_topic(topic_words, tokenizer, model, device):\n",
    "    tokens = tokenizer(topic_words, padding=True, truncation=True, return_tensors=\"pt\", max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# 获取每个主题的平均嵌入向量\n",
    "topic_embeddings = []\n",
    "for topic in bertopic_topics:  # 每个主题的词汇\n",
    "    topic_embedding = get_average_embedding_for_topic(topic, tokenizer, model, device)\n",
    "    topic_embeddings.append(topic_embedding)\n",
    "\n",
    "# 计算不同主题之间的余弦相似度\n",
    "topic_embeddings = np.concatenate(topic_embeddings)  # 合并所有主题的嵌入\n",
    "similarity_matrix = cosine_similarity(topic_embeddings)\n",
    "\n",
    "# 计算 WEPS（主题之间的相似度）\n",
    "weps = np.mean(similarity_matrix)  # 或者根据需求选择合适的聚合方式\n",
    "logger.info(f\"WEPS: {weps}\")\n",
    "print(f\"WEPS: {weps}\")\n",
    "\n",
    "# 保存评估结果\n",
    "evaluation_results_path = os.path.join(experiment_dir, 'bertopic_evaluation_results.txt')\n",
    "with open(evaluation_results_path, 'w') as eval_file:\n",
    "    eval_file.write(f\"平均主题一致性 (NPMI): {coherence_score_npmi}\\n\")\n",
    "    eval_file.write(f\"主题多样性: {topic_diversity}\\n\")\n",
    "    eval_file.write(f\"WEPS: {weps}\\n\")\n",
    "\n",
    "logger.info(f\"BERTopic 模型评估结果已保存至 {evaluation_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存每次生成的主题和评估结果到一个文件\n",
    "results_file_path = os.path.join(experiment_dir, 'bertopic_results.txt')\n",
    "with open(results_file_path, 'w', encoding='utf-8') as f:\n",
    "    # 保存主题词\n",
    "    f.write(\"BERTopic 生成的主题：\\n\")\n",
    "    for idx, topic in enumerate(bertopic_topics):\n",
    "        topic_str = f\"Topic {idx}: {', '.join(topic)}\"\n",
    "        f.write(topic_str + '\\n')\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # 保存评估结果\n",
    "    f.write(f\"平均主题一致性 (NPMI): {coherence_score_npmi}\\n\")\n",
    "    f.write(f\"主题多样性: {topic_diversity}\\n\")\n",
    "\n",
    "logger.info(f\"生成的主题和评估结果已保存至 {results_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latinbert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
